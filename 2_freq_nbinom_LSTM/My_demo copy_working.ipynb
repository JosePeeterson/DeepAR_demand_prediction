{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_forecasting.data.examples import generate_ar_data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import NegativeBinomialDistributionLoss, DeepAR\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('1_f_nbinom_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "4399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josepeeterson.er\\AppData\\Local\\Temp\\ipykernel_2140\\357397288.py:8: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  data['_week_of_year'] = data[\"date\"].dt.weekofyear#.astype(str)\n"
     ]
    }
   ],
   "source": [
    "data[\"date\"] = pd.Timestamp(\"2021-08-24\") + pd.to_timedelta(data.time_idx, \"H\")\n",
    "\n",
    "\n",
    "data['_hour_of_day'] = data[\"date\"].dt.hour#.astype(str)\n",
    "data['_day_of_week'] = data[\"date\"].dt.dayofweek#.astype(str)\n",
    "data['_day_of_month'] = data[\"date\"].dt.day#.astype(str)\n",
    "data['_day_of_year'] = data[\"date\"].dt.dayofyear#.astype(str)\n",
    "data['_week_of_year'] = data[\"date\"].dt.weekofyear#.astype(str)\n",
    "data['_month_of_year'] = data[\"date\"].dt.month#.astype(str)\n",
    "data['_year'] = data[\"date\"].dt.year#.astype(str)\n",
    "\n",
    "data['value'] = data['value'].astype(float)\n",
    "print(type(data['value'][0])) \n",
    "print(len(data.iloc[0:-620]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 60\n",
    "max_prediction_length = 20\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data.iloc[0:-620],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    # categorical_encoders={\"series\": NaNLabelEncoder(add_nan=True).fit(data.series), \"_hour_of_day\": NaNLabelEncoder(add_nan=True).fit(data._hour_of_day), \\\n",
    "    #    \"_day_of_week\": NaNLabelEncoder(add_nan=True).fit(data._day_of_week), \"_day_of_month\" : NaNLabelEncoder(add_nan=True).fit(data._day_of_month), \"_day_of_year\" : NaNLabelEncoder(add_nan=True).fit(data._day_of_year), \\\n",
    "    #     \"_week_of_year\": NaNLabelEncoder(add_nan=True).fit(data._week_of_year), \"_month_of_year\": NaNLabelEncoder(add_nan=True).fit(data._month_of_year) ,\"_year\": NaNLabelEncoder(add_nan=True).fit(data._year)},\n",
    "    group_ids=[\"series\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    time_varying_known_reals=[\"_hour_of_day\",\"_day_of_week\",\"_day_of_month\",\"_day_of_year\",\"_week_of_year\",\"_month_of_year\",\"_year\" ],\n",
    "    add_relative_time_idx=False,\n",
    "    randomize_length=None,\n",
    "    scalers={},\n",
    "    target_normalizer=TorchNormalizer(method=\"identity\",center=False,transformation=None )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#training.data['target'][0][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    data.iloc[-620:-420],\n",
    "    # predict=True,\n",
    "    stop_randomization=True,\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for x,y in train_dataloader:\n",
    "#     print('xen',x[\"encoder_target\"])\n",
    "#     print('xen_cont',x['encoder_cont'])\n",
    "#     print('xde',x[\"decoder_target\"])\n",
    "#     print('xde_cont',x[\"decoder_cont\"])\n",
    "#     print('y',y[0])\n",
    "#     i+=1\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "training.save(\"training.pkl\")\n",
    "validation.save(\"validation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=0,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    limit_val_batches=3,\n",
    "    # fast_dev_run=True,\n",
    "    # logger=logger,\n",
    "    # profiler=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.1,\n",
    "    hidden_size=32,\n",
    "    dropout=0.1,\n",
    "    loss=NegativeBinomialDistributionLoss(),\n",
    "    log_interval=10,\n",
    "    log_val_interval=3,\n",
    "    # reduce_on_plateau_patience=3,\n",
    ")\n",
    "print(f\"Number of parameters in network: {deepar.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find optimal learning rate\n",
    "# deepar.hparams.log_interval = -1\n",
    "# deepar.hparams.log_val_interval = -1\n",
    "# trainer.limit_train_batches = 1.0\n",
    "# res = trainer.tuner.lr_find(\n",
    "#     deepar, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5, max_lr=1e2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                   | Type                             | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | loss                   | NegativeBinomialDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList                       | 0     \n",
      "2 | embeddings             | MultiEmbedding                   | 0     \n",
      "3 | rnn                    | LSTM                             | 13.8 K\n",
      "4 | distribution_projector | Linear                           | 66    \n",
      "----------------------------------------------------------------------------\n",
      "13.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.9 K    Total params\n",
      "0.056     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josepeeterson.er\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josepeeterson.er\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\josepeeterson.er\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:428: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 33/33 [00:03<00:00,  9.04it/s, loss=0.725, v_num=14, train_loss_step=0.648, val_loss=0.760, train_loss_epoch=0.729]\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "trainer.fit(\n",
    "    deepar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict: 100%|██████████| 61/61 [00:05<00:00, 12.01 batches/s]\n"
     ]
    }
   ],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "\n",
    "print(actuals.shape)\n",
    "\n",
    "predictions = deepar.predict(data=val_dataloader,mode='prediction',return_index=True,num_workers=8,show_progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "#print(f\"Mean absolute error of model: {(actuals - torch.tensor(predictions)).abs().mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  2.],\n",
      "        [ 0.,  0.,  8.,  ...,  0.,  2.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0., 10.,  ...,  0., 14.,  0.],\n",
      "        [ 0., 10.,  0.,  ..., 14.,  0.,  0.],\n",
      "        [10.,  0.,  0.,  ...,  0.,  0.,  0.]])\n",
      "tensor([[9.1000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7200e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.2500e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.0100e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3600e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e-02, 0.0000e+00, 0.0000e+00, 9.2700e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.2900e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7600e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 9.6200e+00, 1.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 9.2700e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.9800e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.4300e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9300e+00, 0.0000e+00,\n",
      "         1.0000e-02, 0.0000e+00, 9.6900e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0100e+01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.3400e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0800e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3800e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.4100e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2600e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.3500e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5300e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.8000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.2400e+00, 1.0000e-02, 0.0000e+00, 0.0000e+00, 9.1900e+00, 1.0000e-02,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7100e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.4400e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6500e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3800e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e-02, 9.2700e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.6500e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.5400e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3300e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.8600e+00, 2.0000e-02, 0.0000e+00, 2.0000e-02,\n",
      "         8.6400e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.3200e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6800e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5400e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.1700e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7900e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.9200e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3500e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.7300e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.9000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.1300e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6400e+00, 1.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 8.6500e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2700e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2400e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 8.6900e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(actuals)\n",
    "print((predictions[0][0:10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0213e7566dd6df184afb4bbdab7d267fb988f5f901680f5ca1af43b2a6441d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
