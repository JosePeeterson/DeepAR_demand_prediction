{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.99s/trial, best loss: 1.9210727277316206]\n",
      "100%|██████████| 2/2 [00:05<00:00,  5.12s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 3/3 [13:10<00:00, 790.51s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 4/4 [00:03<00:00,  3.96s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 5/5 [00:04<00:00,  4.59s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 6/6 [00:04<00:00,  4.49s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 7/7 [38:40<00:00, 2320.08s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 8/8 [00:08<00:00,  8.13s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 9/9 [00:12<00:00, 12.80s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 10/10 [00:05<00:00,  5.68s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 11/11 [00:45<00:00, 45.47s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 12/12 [00:08<00:00,  8.57s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 13/13 [00:05<00:00,  5.93s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 14/14 [00:05<00:00,  5.35s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 15/15 [00:05<00:00,  5.05s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 16/16 [00:13<00:00, 13.02s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 17/17 [00:05<00:00,  5.18s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 18/18 [08:09<00:00, 489.78s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 19/19 [00:06<00:00,  6.46s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.99s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 21/21 [00:08<00:00,  8.11s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 22/22 [00:05<00:00,  5.07s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 23/23 [00:08<00:00,  8.11s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 24/24 [00:04<00:00,  4.80s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 25/25 [00:05<00:00,  5.98s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 26/26 [00:05<00:00,  5.75s/trial, best loss: 0.921907494661748]\n",
      "100%|██████████| 27/27 [00:05<00:00,  5.84s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.93s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 29/29 [00:05<00:00,  5.30s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 30/30 [00:10<00:00, 10.72s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 31/31 [00:08<00:00,  8.71s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 32/32 [00:06<00:00,  6.11s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 33/33 [00:20<00:00, 20.41s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 34/34 [00:05<00:00,  5.85s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 35/35 [00:09<00:00,  9.39s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 36/36 [00:09<00:00,  9.61s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 37/37 [00:09<00:00,  9.24s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 38/38 [00:05<00:00,  5.77s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 39/39 [20:04<00:00, 1204.67s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 40/40 [03:56<00:00, 237.00s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 41/41 [00:07<00:00,  7.39s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 42/42 [00:16<00:00, 16.51s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 43/43 [00:29<00:00, 29.74s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 44/44 [00:07<00:00,  7.62s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 45/45 [00:05<00:00,  5.08s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 46/46 [00:05<00:00,  5.47s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 47/47 [00:04<00:00,  4.96s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 48/48 [00:25<00:00, 25.73s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 49/49 [00:10<00:00, 10.27s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 50/50 [00:11<00:00, 11.32s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 51/51 [00:07<00:00,  7.01s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 52/52 [00:05<00:00,  5.22s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 53/53 [00:06<00:00,  6.64s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 54/54 [00:05<00:00,  5.50s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 55/55 [00:05<00:00,  5.77s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 56/56 [00:05<00:00,  5.36s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 57/57 [01:43<00:00, 103.84s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 58/58 [00:05<00:00,  5.99s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 59/59 [00:06<00:00,  6.56s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 60/60 [00:05<00:00,  5.39s/trial, best loss: 0.9128862780640536]\n",
      "100%|██████████| 61/61 [00:05<00:00,  5.59s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 62/62 [00:05<00:00,  5.11s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 63/63 [00:04<00:00,  4.82s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 64/64 [00:07<00:00,  7.23s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 65/65 [00:19<00:00, 19.02s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 66/66 [00:05<00:00,  5.34s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 67/67 [00:05<00:00,  5.21s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 68/68 [00:05<00:00,  5.05s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 69/69 [00:05<00:00,  5.00s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 70/70 [00:05<00:00,  5.13s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 71/71 [00:06<00:00,  6.39s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 72/72 [00:04<00:00,  4.78s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 73/73 [00:04<00:00,  4.67s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 74/74 [00:08<00:00,  8.01s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 75/75 [00:05<00:00,  5.90s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 76/76 [00:05<00:00,  5.48s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 77/77 [00:06<00:00,  6.33s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 78/78 [00:05<00:00,  5.17s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 79/79 [00:05<00:00,  5.72s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 80/80 [00:06<00:00,  6.50s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 81/81 [00:05<00:00,  5.65s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 82/82 [20:05<00:00, 1205.00s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 83/83 [00:05<00:00,  5.41s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 84/84 [00:05<00:00,  5.59s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 85/85 [00:05<00:00,  5.62s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 86/86 [02:37<00:00, 157.46s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 87/87 [00:06<00:00,  6.23s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 88/88 [00:08<00:00,  8.20s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 89/89 [00:05<00:00,  5.07s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 90/90 [00:05<00:00,  5.54s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 91/91 [00:06<00:00,  6.41s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 92/92 [06:45<00:00, 405.82s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 93/93 [00:05<00:00,  5.61s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 94/94 [00:15<00:00, 15.13s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 95/95 [00:05<00:00,  5.59s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 96/96 [00:05<00:00,  5.67s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 97/97 [00:10<00:00, 10.93s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 98/98 [00:05<00:00,  5.68s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 99/99 [00:05<00:00,  5.51s/trial, best loss: 0.9127018568613278]\n",
      "100%|██████████| 100/100 [00:05<00:00,  5.22s/trial, best loss: 0.9127018568613278]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josepeeterson.er\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.115\n",
      "{'learner': AdaBoostRegressor(learning_rate=0.0006592792294329333, loss='square',\n",
      "                  n_estimators=11, random_state=4), 'preprocs': (Normalizer(norm='max'),), 'ex_preprocs': ()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josepeeterson.er\\Miniconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but Normalizer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tweedie\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import pygraphviz\n",
    "#warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from xgboost import plot_tree\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, xgboost_regression\n",
    "from hpsklearn import any_regressor\n",
    "from hpsklearn import any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "\n",
    "class train_validate_n_test(object):\n",
    "\n",
    "    def __init__(self) -> None:    \n",
    "        dataset_filename = \"xgboost_feat_train_ds_stn_130.csv\"\n",
    "        self.stn = dataset_filename.split(\"_stn_\")[1].split(\".\")[0]\n",
    "        self.train_set, self.val_set, self.test_set,self.large_train_set = self.train_val_test_split(dataset_filename)\n",
    "\n",
    "\n",
    "    def onehotencode_cat_var(self,ds_df):\n",
    "        ds_df = ds_df.astype({\"block_id_\"+self.stn:str,\"ts_of_day_\"+self.stn:str,\"hr_of_day_\"+self.stn:str,\"day_of_wk_\"+self.stn:str,\"day_of_mn_\"+self.stn:str,\"wk_of_mon_\"+self.stn:str })\n",
    "        ds_df = pd.get_dummies(ds_df, prefix_sep=\"_\",columns =[\"block_id_\"+self.stn,\"ts_of_day_\"+self.stn,\"hr_of_day_\"+self.stn,\"day_of_wk_\"+self.stn,\"day_of_mn_\"+self.stn,\"wk_of_mon_\"+self.stn],\n",
    "        drop_first=True)\n",
    "        #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n",
    "\n",
    "        # ratio of postive to negative classes\n",
    "        num_pos_instances = np.array(ds_df['target_'+self.stn].value_counts('Class'))[0] # most frequent class\n",
    "        num_neg_instances = np.array(ds_df['target_'+self.stn].value_counts('Class'))[-2] # second least frequent class\n",
    "        self.scale_pos_weight = int(num_pos_instances / num_neg_instances)\n",
    "        return ds_df\n",
    "\n",
    "\n",
    "    def train_val_test_split(self,dataset_filename):\n",
    "        ds_df = pd.read_csv(dataset_filename)\n",
    "        tot_len = len(ds_df) #10050\n",
    "\n",
    "        ds_df = self.onehotencode_cat_var(ds_df)\n",
    "\n",
    "        # use 60% as training set\n",
    "        train_idx_end = int(tot_len*0.6)\n",
    "        train_Set = ds_df.loc[:train_idx_end]\n",
    "        train_Set = train_Set.drop(['Unnamed: 0', 'dt_ts'],axis=1)\n",
    "\n",
    "        # ensure that val set has 3 weeks for data for 3 walk-forward validation sets\n",
    "        # 3*7*24*6 = 3024 time slots\n",
    "        val_idx_end = train_idx_end+1+3024\n",
    "        val_Set = ds_df.loc[train_idx_end+1:val_idx_end]\n",
    "        val_Set = val_Set.drop(['Unnamed: 0', 'dt_ts'],axis=1)\n",
    "        \n",
    "        # Use remaining for test set\n",
    "        test_Set = ds_df.loc[val_idx_end+1:]\n",
    "        test_Set = test_Set.drop(['Unnamed: 0', 'dt_ts'],axis=1)\n",
    "\n",
    "        # for hpsklearn\n",
    "        large_train_set = ds_df.loc[:val_idx_end+1]\n",
    "        large_train_set = large_train_set.drop(['Unnamed: 0', 'dt_ts'],axis=1)\n",
    "        return train_Set, val_Set, test_Set,large_train_set    \n",
    "\n",
    "\n",
    "    def walk_forward_val_n_train_set(self,part):\n",
    "        # split validation set into 3 parts\n",
    "        part_idx = int(len(self.val_set)/3)\n",
    "        self.val_set1 = self.val_set.iloc[:part_idx]\n",
    "        self.val_set2 = self.val_set.iloc[part_idx+1:part_idx*2]\n",
    "        self.val_set3 = self.val_set.iloc[part_idx*2+1:]\n",
    "\n",
    "\n",
    "        if (part==1):\n",
    "            # provide first part of validation set\n",
    "            train = self.train_set.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_train = pd.DataFrame(self.train_set[\"target_\"+self.stn])\n",
    "            dtrain1 = xgb.DMatrix(train, label=label_train)\n",
    "            val = self.val_set1.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_val = pd.DataFrame(self.val_set1[\"target_\"+self.stn])\n",
    "            dval1 = xgb.DMatrix(val, label=label_val)\n",
    "            # print('\\nset1\\n')\n",
    "            # print('\\n',train, label_train)\n",
    "            # print('\\n',val, label_val,'\\n')     \n",
    "            return dtrain1, dval1\n",
    "        \n",
    "        elif (part==2):\n",
    "            # append validation part to train set and provide 2nd part of val. set\n",
    "            self.train2 = pd.concat([self.train_set,self.val_set1], axis=0)\n",
    "            new_train = self.train2.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_train = pd.DataFrame(self.train2[\"target_\"+self.stn])\n",
    "            dtrain2 = xgb.DMatrix(new_train, label=label_train)\n",
    "            val2 = self.val_set2.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_val = pd.DataFrame(self.val_set2[\"target_\"+self.stn])\n",
    "            dval2 = xgb.DMatrix(val2, label=label_val)        \n",
    "            return dtrain2,dval2\n",
    "        \n",
    "        else:\n",
    "            # append 1st and 2nd validation part to train set and provide 3rd part of val. set\n",
    "            self.train3 = pd.concat([self.train2,self.val_set2], axis=0)\n",
    "            new_train = self.train3.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_train = pd.DataFrame(self.train3[\"target_\"+self.stn])\n",
    "            dtrain3 = xgb.DMatrix(new_train, label=label_train)\n",
    "            val3 = self.val_set3.drop(\"target_\"+self.stn, axis=1)\n",
    "            label_val = pd.DataFrame(self.val_set3[\"target_\"+self.stn])\n",
    "            dval3 = xgb.DMatrix(val3, label=label_val)    \n",
    "            return dtrain3,dval3\n",
    "\n",
    "\n",
    "\n",
    "    def xgb_train_validate(self,num_round,max_depth, eta, subsample, colsample_bytree, objective, e_s_r, eval_met,tvr):\n",
    "\n",
    "        # Hyperparameters and algorithm parameters are described here\n",
    "\n",
    "        params = {\"max_depth\": max_depth,\n",
    "                \"eta\": eta,\n",
    "                \"objective\": objective,\n",
    "                \"subsample\" : subsample,\n",
    "                \"colsample_bytree\":colsample_bytree,\n",
    "                'eval_metric':eval_met, ## try using AUC as well.. \n",
    "                'tweedie_variance_power': tvr,\n",
    "                'gamma': 0.05,\n",
    "                'reg_alpha': 0, \n",
    "                'reg_lambda':1,\n",
    "                'min_child_weight':1,\n",
    "                'scale_pos_weight': self.scale_pos_weight # important for imbalanced dataset, num_neg_instces/num_pos_instces\n",
    "\n",
    "                }\n",
    "\n",
    "        dtrain1, dval1 = self.walk_forward_val_n_train_set(part=1)\n",
    "        watchlist1  = [(dtrain1,'train1_tweedie_loss'), (dval1, 'val1_tweedie_loss')]\n",
    "        evals_result1 = {}\n",
    "        model1 = xgb.train(params=params, dtrain=dtrain1, num_boost_round=num_round, evals=watchlist1, evals_result=evals_result1,  early_stopping_rounds=e_s_r )\n",
    "        #val1_error = evals_result1['val1_tweedie_loss']['tweedie-nloglik@'+str(tvr)]\n",
    "\n",
    "        dtrain2, dval2 = self.walk_forward_val_n_train_set(part=2)\n",
    "        watchlist2  = [(dtrain2,'train2_tweedie_loss'), (dval2, 'val2_tweedie_loss')]\n",
    "        evals_result2 = {}\n",
    "        model2 = xgb.train(params=params, dtrain=dtrain2, num_boost_round=num_round, evals=watchlist2, evals_result=evals_result2, early_stopping_rounds=e_s_r,xgb_model=model1 )\n",
    "        #val2_error = evals_result2['val2_tweedie_loss']['tweedie-nloglik@'+str(tvr)]\n",
    "\n",
    "        dtrain3, dval3 = self.walk_forward_val_n_train_set(part=3)\n",
    "        watchlist3  = [(dtrain3,'train3_tweedie_loss'), (dval3, 'val3_tweedie_loss')]\n",
    "        evals_result3 = {}\n",
    "        self.model3 = xgb.train(params=params, dtrain=dtrain3, num_boost_round=num_round, evals=watchlist3, evals_result=evals_result3, early_stopping_rounds=e_s_r,xgb_model=model2 )\n",
    "        #val3_error = evals_result3['val3_tweedie_loss']['tweedie-nloglik@'+str(tvr)]\n",
    "\n",
    "        print('\\n')\n",
    "        # print(val1_error)\n",
    "        # print(val2_error)\n",
    "        # print(val3_error)\n",
    "\n",
    "        # val_avg_error = np.mean(val1_error,val2_error,val3_error)\n",
    "\n",
    "        return #val_avg_error\n",
    "    \n",
    "    def plot_target_distribution(self,):\n",
    "        col_list = self.train3.columns\n",
    "        self.train3.loc[:][col_list[-1]].hist()\n",
    "        plt.title('Histogram of target distribution')\n",
    "        plt.xlabel('demand value (class)')\n",
    "        plt.ylabel('count')\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    def display_tweedie_plot(self,):\n",
    "        tvs = tweedie.tweedie(mu=1, p=1.1, phi=1.5).rvs(6000)\n",
    "        sn.set(rc={'figure.figsize':(10,5)})\n",
    "        # plt.figure(figsize=(25,10))\n",
    "        sn.distplot(tvs)\n",
    "        plt.title('Parametric tweedie distribution')\n",
    "        plt.xlabel('demand value (class)')\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    def evaluate_predictions(self,):\n",
    "        testx = self.test_set.drop('target_'+self.stn,axis=1)\n",
    "        label_test = pd.DataFrame(self.test_set['target_'+self.stn])\n",
    "        dtest = xgb.DMatrix(testx,label=label_test)\n",
    "        preds = self.model3.predict(dtest)\n",
    "        preds = np.rint(preds)\n",
    "        label_test = label_test['target_'+self.stn].to_numpy()\n",
    "        print('preds', len(preds))\n",
    "        print('label_test', len(label_test))\n",
    "    \n",
    "        plt.plot(preds,'*')\n",
    "        plt.ylabel('Demand value (prediction)')\n",
    "        plt.xlabel('Time')     \n",
    "        plt.show()\n",
    "        plt.plot(label_test,'*')\n",
    "        plt.ylabel('Demand value (Ground-truth)')\n",
    "        plt.xlabel('Time')     \n",
    "        plt.show()\n",
    "        # plt.plot(label_test[40:60],'*')\n",
    "        # plt.title('zoomed true label test')\n",
    "        # plt.ylabel('Demand value')\n",
    "        # plt.xlabel('Time')     \n",
    "        # plt.show()\n",
    "        # plt.plot(preds[40:60],'*')\n",
    "        # plt.title('zoomed pred label')\n",
    "        # plt.ylabel('Demand value')\n",
    "        # plt.xlabel('Time')     \n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        #\"use confusion matrix, ROC, F1 scores to evaluate\"\n",
    "        cm = confusion_matrix(label_test,preds)\n",
    "        max_classes = max(len(np.unique(label_test)),len(np.unique(preds)))\n",
    "        df_cm = pd.DataFrame(cm, index = [i for i in range(max_classes)],\n",
    "                        columns = [i for i in range(max_classes)])\n",
    "        plt.figure(figsize = (10,7))      \n",
    "        s = sn.heatmap(df_cm, annot=True, )\n",
    "        s.set(xlabel='Predicted-Label', ylabel='True-Label')\n",
    "\n",
    "        return testx,label_test, preds\n",
    "\n",
    "\n",
    "    def identify_tweedie_variance_param():\n",
    "        #TODO:#\"fit on data\"\n",
    "        # follow this tutorial https://notebook.community/thequackdaddy/tweedie/example/tweedie_demo\n",
    "        # GLM.estimate_tweedie_power()\n",
    "        # #Training model\n",
    "        # tweedie_model = sm.GLM(y_train, X_train, exposure = df_train.exposure, family=sm.families.Tweedie(link=None,var_power=1.5,eql=True))\n",
    "        # tweedie_result = tweedie_model.fit()\n",
    "        # #Using the initial model output to decide the optimum index parameter \"p\"\n",
    "        # GLM.estimate_tweedie_power(training_result, method='brentq', low=1.01, high=5.0)\n",
    "        # tweedie_model.estimate_tweedie_power(tweedie_result.mu, method='brentq', low=1.01, high=5.0)\n",
    "        return\n",
    "\n",
    "\n",
    "    def make_predictions(self,):\n",
    "        #TODO:#for new predictions after tuning model\n",
    "\n",
    "        return\n",
    "    \n",
    "\n",
    "    def visualize_tree(self,):\n",
    "        #TODO:#\"draw out a few trees to interpret results\"        \n",
    "        #xgb.plot_tree(self.train3)\n",
    "        plot_tree(self.model3)\n",
    "\n",
    "        return\n",
    "\n",
    "    def hyperopt(self,):\n",
    "        # define search\n",
    "        model = HyperoptEstimator(regressor=any_regressor('reg'), preprocessing=any_preprocessing('pre'), algo=tpe.suggest, max_evals=100, trial_timeout=1200)\n",
    "        # perform the search\n",
    "        new_train = self.large_train_set.drop(\"target_\"+self.stn, axis=1)\n",
    "        label_train = pd.DataFrame(self.large_train_set[\"target_\"+self.stn])\n",
    "        model.fit(new_train, label_train)\n",
    "        # summarize performance\n",
    "        new_test = self.test_set.drop(\"target_\"+self.stn, axis=1)\n",
    "        label_test = pd.DataFrame(self.test_set[\"target_\"+self.stn])\n",
    "        mae = model.score(new_test, label_test)\n",
    "        print(\"MAE: %.3f\" % mae)\n",
    "        # summarize the best model\n",
    "        print(model.best_model())\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\BLUESG_Demand_data\\Data-preprocessing_data_generation\")\n",
    "    t_v_t = train_validate_n_test()\n",
    "    t_v_t.hyperopt()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
