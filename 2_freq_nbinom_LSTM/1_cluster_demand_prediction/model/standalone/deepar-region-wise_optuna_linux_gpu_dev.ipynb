{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'windows\\nos.chdir(\"c:/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR-pytorch/My_model/2_freq_nbinom_LSTM/1_cluster_demand_prediction\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\"\"\"Windows\n",
    "os.chdir(\"c:/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR-pytorch/My_model/2_freq_nbinom_LSTM\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Linux \"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM\")\n",
    "sys.path.append(os.path.abspath(os.path.join(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM\")))\n",
    "\n",
    "\n",
    "#from ctypes import FormatError\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os,sys\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM')))\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data\\weather_data')))\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\2_freq_nbinom_LSTM\\1_cluster_demand_prediction\\data\\demand_data')))\n",
    "\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, RMSE\n",
    "from torchmetrics import R2Score, SymmetricMeanAbsolutePercentageError, MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "import os,sys\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from scipy.signal import find_peaks\n",
    "import operator\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "from pytorch_forecasting import Baseline\n",
    "import random\n",
    "from pytorch_forecasting import DeepAR,NegativeBinomialDistributionLoss\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import plotly\n",
    "from deepar_RegionWise_LinuxGpu_prediction_dev import train_and_forecast\n",
    "\n",
    "\"\"\"\n",
    "Set Random seed\n",
    "\"\"\"\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "## additional seeding to ensure reproduciblility.\n",
    "pl.seed_everything(0)\n",
    "\n",
    "\"\"\"windows\n",
    "os.chdir(\"c:/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR-pytorch/My_model/2_freq_nbinom_LSTM/1_cluster_demand_prediction\")\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set inputs here\n",
    "(hyperparameters grid search)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "######### MISCELLANEOUS ###################\n",
    "region =\"Tampines\"\n",
    "cov_lag_len= 0 #we can use forecasted values, even for inflow\n",
    "Target = 'target'\n",
    "encoder_length = 18\n",
    "######### MISCELLANEOUS ###################\n",
    "\n",
    "\n",
    "######### Network Architecture ###################\n",
    "p = 10 # patience no. of epochs\n",
    "\n",
    "Loss=NegativeBinomialDistributionLoss()\n",
    "\n",
    "######### Network Architecture ###################\n",
    "\n",
    "\n",
    "######### Training Routine ###################\n",
    "fdv_steps = 10 # fast_dev_run\n",
    "######### Training Routine ###################\n",
    "\n",
    "\n",
    "############## Inputs for 2) Persistance model ( seasonal naive forecast ) #######################\n",
    "season_len = 168 # length of season\n",
    "num_past_seas = 6 # number of past seasons to use in averaging\n",
    "#seas_pred_strt_idx = 2035 # seasonal naive forecast start index, in hours use the df dataframe\n",
    "############## Inputs for 2) Persistance model ( seasonal naive forecast ) #######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linux\"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/demand_data/standalone/region_level/train_val_test_data\")\n",
    "\n",
    "\"\"\"\n",
    "Import pre-processed Data\n",
    "\n",
    "response and target are the same thing\n",
    "\"\"\"\n",
    "all_clstr_train_dem_data = pd.read_csv(region+'_all_clstr_train_dem_data.csv')\n",
    "all_clstr_full_train_dem_data = pd.read_csv(region+'_all_clstr_full_train_dem_data.csv')\n",
    "\n",
    "all_clstr_val_dem_data = pd.read_csv(region+'_all_clstr_val_dem_data.csv')\n",
    "all_clstr_test_dem_data = pd.read_csv(region+'_all_clstr_test_dem_data.csv')\n",
    "\n",
    "train_data = all_clstr_train_dem_data\n",
    "full_train_data = all_clstr_full_train_dem_data\n",
    "val_data = all_clstr_val_dem_data\n",
    "test_data = all_clstr_test_dem_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>datetime</th>\n",
       "      <th>group</th>\n",
       "      <th>target</th>\n",
       "      <th>inflow</th>\n",
       "      <th>inflow_lag168</th>\n",
       "      <th>inflow_lag336</th>\n",
       "      <th>outflow_lag168</th>\n",
       "      <th>outflow_lag336</th>\n",
       "      <th>inflow_diff1_per1</th>\n",
       "      <th>...</th>\n",
       "      <th>inflow_diff2_per1__partial_autocorrelation__lag_2</th>\n",
       "      <th>inflow_diff2_per1__fft_coefficient__attr_\"real\"__coeff_1</th>\n",
       "      <th>inflow_diff2_per1__skewness</th>\n",
       "      <th>inflow_diff2_per1__kurtosis</th>\n",
       "      <th>inflow_diff2_per1__longest_strike_below_mean</th>\n",
       "      <th>inflow_diff2_per1__partial_autocorrelation__lag_1</th>\n",
       "      <th>inflow_diff2_per1__autocorrelation__lag_1</th>\n",
       "      <th>inflow_diff2_per1__mean</th>\n",
       "      <th>inflow_diff2_per1__sum_values</th>\n",
       "      <th>inflow_diff2_per1__fft_coefficient__attr_\"real\"__coeff_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>336</td>\n",
       "      <td>2021-10-08 00:00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530444</td>\n",
       "      <td>0.106416</td>\n",
       "      <td>-0.548952</td>\n",
       "      <td>0.841539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337</td>\n",
       "      <td>2021-10-08 01:00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.548952</td>\n",
       "      <td>0.841539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>338</td>\n",
       "      <td>2021-10-08 02:00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.841539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>-0.644841</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>339</td>\n",
       "      <td>2021-10-08 03:00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340</td>\n",
       "      <td>2021-10-08 04:00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530444</td>\n",
       "      <td>-1.427051</td>\n",
       "      <td>-1.293234</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>-0.479167</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13987</th>\n",
       "      <td>1603</td>\n",
       "      <td>2021-11-29 19:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231270</td>\n",
       "      <td>0.711465</td>\n",
       "      <td>-1.083028</td>\n",
       "      <td>0.790053</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.695849</td>\n",
       "      <td>-0.695849</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>1604</td>\n",
       "      <td>2021-11-29 20:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319976</td>\n",
       "      <td>-0.303356</td>\n",
       "      <td>-0.928873</td>\n",
       "      <td>0.279489</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.710227</td>\n",
       "      <td>-0.710227</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>1605</td>\n",
       "      <td>2021-11-29 21:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174149</td>\n",
       "      <td>-1.267700</td>\n",
       "      <td>-0.914687</td>\n",
       "      <td>0.385837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.644163</td>\n",
       "      <td>-0.644163</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990</th>\n",
       "      <td>1606</td>\n",
       "      <td>2021-11-29 22:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173670</td>\n",
       "      <td>-0.183806</td>\n",
       "      <td>-0.836740</td>\n",
       "      <td>0.581467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.636574</td>\n",
       "      <td>-0.636574</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>1607</td>\n",
       "      <td>2021-11-29 23:00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275198</td>\n",
       "      <td>-0.088363</td>\n",
       "      <td>-0.836740</td>\n",
       "      <td>0.581467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.636574</td>\n",
       "      <td>-0.636574</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13992 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_idx             datetime  group  target  inflow  inflow_lag168  \\\n",
       "0           336  2021-10-08 00:00:00    160     0.0     0.0            2.0   \n",
       "1           337  2021-10-08 01:00:00    160     0.0     0.0            0.0   \n",
       "2           338  2021-10-08 02:00:00    160     0.0     0.0            0.0   \n",
       "3           339  2021-10-08 03:00:00    160     0.0     1.0            0.0   \n",
       "4           340  2021-10-08 04:00:00    160     0.0     0.0            0.0   \n",
       "...         ...                  ...    ...     ...     ...            ...   \n",
       "13987      1603  2021-11-29 19:00:00    170     1.0     1.0            1.0   \n",
       "13988      1604  2021-11-29 20:00:00    170     0.0     1.0            0.0   \n",
       "13989      1605  2021-11-29 21:00:00    170     0.0     1.0            1.0   \n",
       "13990      1606  2021-11-29 22:00:00    170     0.0     0.0            2.0   \n",
       "13991      1607  2021-11-29 23:00:00    170     1.0     0.0            0.0   \n",
       "\n",
       "       inflow_lag336  outflow_lag168  outflow_lag336  inflow_diff1_per1  ...  \\\n",
       "0                0.0             0.0             0.0                0.0  ...   \n",
       "1                0.0             0.0             0.0                0.0  ...   \n",
       "2                0.0             0.0             0.0                0.0  ...   \n",
       "3                0.0             0.0             0.0                1.0  ...   \n",
       "4                0.0             0.0             0.0               -1.0  ...   \n",
       "...              ...             ...             ...                ...  ...   \n",
       "13987            0.0             0.0             2.0                1.0  ...   \n",
       "13988            1.0             1.0             0.0                0.0  ...   \n",
       "13989            2.0             0.0             0.0                0.0  ...   \n",
       "13990            0.0             0.0             0.0               -1.0  ...   \n",
       "13991            1.0             1.0             1.0                0.0  ...   \n",
       "\n",
       "       inflow_diff2_per1__partial_autocorrelation__lag_2  \\\n",
       "0                                              -0.530444   \n",
       "1                                              -0.530444   \n",
       "2                                              -0.530444   \n",
       "3                                              -0.530444   \n",
       "4                                              -0.530444   \n",
       "...                                                  ...   \n",
       "13987                                          -0.231270   \n",
       "13988                                          -0.319976   \n",
       "13989                                          -0.174149   \n",
       "13990                                          -0.173670   \n",
       "13991                                          -0.275198   \n",
       "\n",
       "       inflow_diff2_per1__fft_coefficient__attr_\"real\"__coeff_1  \\\n",
       "0                                               0.106416          \n",
       "1                                               0.000000          \n",
       "2                                               0.000000          \n",
       "3                                               0.000000          \n",
       "4                                              -1.427051          \n",
       "...                                                  ...          \n",
       "13987                                           0.711465          \n",
       "13988                                          -0.303356          \n",
       "13989                                          -1.267700          \n",
       "13990                                          -0.183806          \n",
       "13991                                          -0.088363          \n",
       "\n",
       "       inflow_diff2_per1__skewness  inflow_diff2_per1__kurtosis  \\\n",
       "0                        -0.548952                     0.841539   \n",
       "1                        -0.548952                     0.841539   \n",
       "2                         0.000000                     0.841539   \n",
       "3                         2.000000                     4.000000   \n",
       "4                        -1.293234                     2.916667   \n",
       "...                            ...                          ...   \n",
       "13987                    -1.083028                     0.790053   \n",
       "13988                    -0.928873                     0.279489   \n",
       "13989                    -0.914687                     0.385837   \n",
       "13990                    -0.836740                     0.581467   \n",
       "13991                    -0.836740                     0.581467   \n",
       "\n",
       "       inflow_diff2_per1__longest_strike_below_mean  \\\n",
       "0                                               0.0   \n",
       "1                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               3.0   \n",
       "4                                               1.0   \n",
       "...                                             ...   \n",
       "13987                                           7.0   \n",
       "13988                                           1.0   \n",
       "13989                                           1.0   \n",
       "13990                                           1.0   \n",
       "13991                                           1.0   \n",
       "\n",
       "       inflow_diff2_per1__partial_autocorrelation__lag_1  \\\n",
       "0                                              -0.644841   \n",
       "1                                              -0.644841   \n",
       "2                                              -0.644841   \n",
       "3                                              -0.111111   \n",
       "4                                              -0.479167   \n",
       "...                                                  ...   \n",
       "13987                                          -0.695849   \n",
       "13988                                          -0.710227   \n",
       "13989                                          -0.644163   \n",
       "13990                                          -0.636574   \n",
       "13991                                          -0.636574   \n",
       "\n",
       "       inflow_diff2_per1__autocorrelation__lag_1  inflow_diff2_per1__mean  \\\n",
       "0                                      -0.644841                     0.00   \n",
       "1                                      -0.644841                     0.00   \n",
       "2                                      -0.644841                     0.00   \n",
       "3                                      -0.111111                     0.25   \n",
       "4                                      -0.479167                    -0.20   \n",
       "...                                          ...                      ...   \n",
       "13987                                  -0.695849                     0.04   \n",
       "13988                                  -0.710227                     0.00   \n",
       "13989                                  -0.644163                    -0.04   \n",
       "13990                                  -0.636574                     0.00   \n",
       "13991                                  -0.636574                     0.00   \n",
       "\n",
       "       inflow_diff2_per1__sum_values  \\\n",
       "0                                0.0   \n",
       "1                                0.0   \n",
       "2                                0.0   \n",
       "3                                1.0   \n",
       "4                               -1.0   \n",
       "...                              ...   \n",
       "13987                            1.0   \n",
       "13988                            0.0   \n",
       "13989                           -1.0   \n",
       "13990                            0.0   \n",
       "13991                            0.0   \n",
       "\n",
       "       inflow_diff2_per1__fft_coefficient__attr_\"real\"__coeff_0  \n",
       "0                                                    0.0         \n",
       "1                                                    0.0         \n",
       "2                                                    0.0         \n",
       "3                                                    1.0         \n",
       "4                                                   -1.0         \n",
       "...                                                  ...         \n",
       "13987                                                1.0         \n",
       "13988                                                0.0         \n",
       "13989                                               -1.0         \n",
       "13990                                                0.0         \n",
       "13991                                                0.0         \n",
       "\n",
       "[13992 rows x 171 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data has null values? False\n",
      "train_data has null values? False\n",
      "val_data has null values? False\n",
      "test_data has null values? False\n"
     ]
    }
   ],
   "source": [
    "#################### add date information ts ####################\n",
    "train_data[\"datetime\"] = pd.to_datetime(train_data[\"datetime\"])\n",
    "train_data['_hour_of_day'] = train_data[\"datetime\"].dt.hour.astype(str)\n",
    "train_data['_day_of_week'] = train_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "train_data['_day_of_month'] = train_data[\"datetime\"].dt.day.astype(str)\n",
    "train_data['_day_of_year'] = train_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "train_data['_week_of_year'] = train_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "train_data['_month_of_year'] = train_data[\"datetime\"].dt.month.astype(str)\n",
    "train_data['_year'] = train_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "full_train_data[\"datetime\"] = pd.to_datetime(full_train_data[\"datetime\"])\n",
    "full_train_data['_hour_of_day'] = full_train_data[\"datetime\"].dt.hour.astype(str)\n",
    "full_train_data['_day_of_week'] = full_train_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "full_train_data['_day_of_month'] = full_train_data[\"datetime\"].dt.day.astype(str)\n",
    "full_train_data['_day_of_year'] = full_train_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "full_train_data['_week_of_year'] = full_train_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "full_train_data['_month_of_year'] = full_train_data[\"datetime\"].dt.month.astype(str)\n",
    "full_train_data['_year'] = full_train_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "val_data[\"datetime\"] = pd.to_datetime(val_data[\"datetime\"])\n",
    "val_data['_hour_of_day'] = val_data[\"datetime\"].dt.hour.astype(str)\n",
    "val_data['_day_of_week'] = val_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "val_data['_day_of_month'] = val_data[\"datetime\"].dt.day.astype(str)\n",
    "val_data['_day_of_year'] = val_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "val_data['_week_of_year'] = val_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "val_data['_month_of_year'] = val_data[\"datetime\"].dt.month.astype(str)\n",
    "val_data['_year'] = val_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "test_data[\"datetime\"] = pd.to_datetime(test_data[\"datetime\"])\n",
    "test_data['_hour_of_day'] = test_data[\"datetime\"].dt.hour.astype(str)\n",
    "test_data['_day_of_week'] = test_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "test_data['_day_of_month'] = test_data[\"datetime\"].dt.day.astype(str)\n",
    "test_data['_day_of_year'] = test_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "test_data['_week_of_year'] = test_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "test_data['_month_of_year'] = test_data[\"datetime\"].dt.month.astype(str)\n",
    "test_data['_year'] = test_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "\n",
    "#print(list(train_data.columns))\n",
    "\n",
    "\"\"\"\n",
    "CHecK for null values\n",
    "\"\"\"\n",
    "\n",
    "print(\"train_data has null values?\",train_data.isnull().values.any())\n",
    "print(\"train_data has null values?\",full_train_data.isnull().values.any())\n",
    "print(\"val_data has null values?\",val_data.isnull().values.any())\n",
    "print(\"test_data has null values?\",test_data.isnull().values.any())\n",
    "\n",
    "\"\"\" \n",
    "Drop datetime column\n",
    "\"\"\"\n",
    "train_data = train_data.drop(columns=['datetime'])\n",
    "full_train_data = full_train_data.drop(columns=['datetime'])\n",
    "val_data = val_data.drop(columns=['datetime'])\n",
    "test_data = test_data.drop(columns=['datetime'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in train_data.columns[3:-7]:\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Full Training Routine \n",
    "with bayesisan hyperparmeter search\n",
    "\n",
    "Load data into TimeSeriesDataSet object\n",
    "\n",
    "for fast development run\n",
    "uncomment fast_dev_run = fdv_steps\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=p, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "\n",
    "class MetricsCallback(pl.Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)\n",
    "\n",
    "\n",
    "def objective(trial,):  \n",
    "  \n",
    "  neu = trial.suggest_int(name=\"neu\",low=600,high=800,step=25,log=False)\n",
    "  lay = trial.suggest_int(name=\"lay\",low=1,high=3,step=1,log=False)\n",
    "  bat = trial.suggest_int(name=\"bat\",low=4,high=12,step=4,log=False)\n",
    "  lr = trial.suggest_float(name=\"lr\",low=0.000001,high=0.01,log=True)\n",
    "  num_ep = trial.suggest_int(name=\"num_ep\",low=20,high=30,step=2,log=False)\n",
    "  enc_len = encoder_length\n",
    "  pred_len = 1\n",
    "  drop = trial.suggest_float(name=\"dropout\",low=0,high=0.4,step=0.1,log=False)\n",
    "\n",
    "\n",
    "  num_cols_list = list(train_data.columns[3:-7])\n",
    "\n",
    "  cat_dict = {\"_hour_of_day\": NaNLabelEncoder(add_nan=True).fit(train_data._hour_of_day), \\\n",
    "  \"_day_of_week\": NaNLabelEncoder(add_nan=True).fit(train_data._day_of_week), \"_day_of_month\" : NaNLabelEncoder(add_nan=True).fit(train_data._day_of_month), \"_day_of_year\" : NaNLabelEncoder(add_nan=True).fit(train_data._day_of_year), \\\n",
    "      \"_week_of_year\": NaNLabelEncoder(add_nan=True).fit(train_data._week_of_year), \"_month_of_year\": NaNLabelEncoder(add_nan=True).fit(train_data._month_of_year) ,\"_year\": NaNLabelEncoder(add_nan=True).fit(train_data._year) }\n",
    "  cat_list = [\"_hour_of_day\",\"_day_of_week\",\"_day_of_month\",\"_day_of_year\",\"_week_of_year\",\"_month_of_year\",\"_year\"]  \n",
    "\n",
    "  train_dataset = TimeSeriesDataSet(\n",
    "      train_data,\n",
    "      time_idx=\"time_idx\",\n",
    "      target=Target,\n",
    "      categorical_encoders=cat_dict,\n",
    "      group_ids=[\"group\"],\n",
    "      min_encoder_length=enc_len,\n",
    "      max_encoder_length=enc_len,\n",
    "      min_prediction_length=pred_len,\n",
    "      max_prediction_length=pred_len,\n",
    "      time_varying_unknown_reals=[Target],\n",
    "      time_varying_known_reals=num_cols_list,\n",
    "      time_varying_known_categoricals=cat_list,\n",
    "      add_relative_time_idx=False,\n",
    "      randomize_length=False,\n",
    "      scalers={},\n",
    "      target_normalizer=TorchNormalizer(method=\"identity\",center=False,transformation=None )\n",
    "\n",
    "  )\n",
    "\n",
    "  val_dataset = TimeSeriesDataSet.from_dataset(train_dataset,val_data, stop_randomization=True, predict=False)\n",
    "\n",
    "  train_dataloader = train_dataset.to_dataloader(train=True, batch_size=bat)\n",
    "  val_dataloader = val_dataset.to_dataloader(train=False, batch_size=bat)\n",
    "  ######### Load DATA #############\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  Machine Learning predictions START\n",
    "  1) DeepAR \n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  metrics_callback = MetricsCallback()\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "      max_epochs=num_ep,\n",
    "      gpus=-1, #-1\n",
    "      auto_lr_find=False,\n",
    "      gradient_clip_val=0.1,\n",
    "      limit_train_batches=1.0,\n",
    "      limit_val_batches=1.0,\n",
    "      logger=True,\n",
    "      val_check_interval=1.0,\n",
    "      callbacks=[lr_logger,metrics_callback]\n",
    "  )\n",
    "\n",
    "  #print(f\"training routing:\\n \\n {trainer}\")\n",
    "  deepar = DeepAR.from_dataset(\n",
    "      train_dataset,\n",
    "      learning_rate=lr,\n",
    "      hidden_size=neu,\n",
    "      rnn_layers=lay,\n",
    "      dropout=drop,\n",
    "      loss=Loss,\n",
    "      log_interval=20,\n",
    "      log_val_interval=6,\n",
    "      log_gradient_flow=False,\n",
    "      # reduce_on_plateau_patience=3,\n",
    "  )\n",
    "\n",
    "\n",
    "  torch.set_num_threads(10)\n",
    "  trainer.fit(\n",
    "      deepar,\n",
    "      train_dataloaders=train_dataloader,\n",
    "      val_dataloaders=val_dataloader,\n",
    "  )\n",
    "\n",
    "\n",
    "  metrics_list = [ metrics[\"val_RMSE\"].item()  for metrics in  metrics_callback.metrics[1:]]\n",
    "  min_val_rmse_epoch = np.argmin(metrics_list)\n",
    "  min_val_rmse = np.min(metrics_list)\n",
    "\n",
    "\n",
    "  trial.report(min_val_rmse, min_val_rmse_epoch)\n",
    "\n",
    "  # Handle pruning based on the intermediate value.\n",
    "  if trial.should_prune():\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "  return min_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-24 19:48:56,286]\u001b[0m A new study created in memory with name: no-name-0ec6b6b5-c1ce-4645-aa88-0929ad1876a7\u001b[0m\n",
      "/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                             | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | loss                   | NegativeBinomialDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList                       | 0     \n",
      "2 | embeddings             | MultiEmbedding                   | 1.5 K \n",
      "3 | rnn                    | LSTM                             | 2.7 M \n",
      "4 | distribution_projector | Linear                           | 1.5 K \n",
      "----------------------------------------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.986    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  36%|███▋      | 809/2228 [00:06<00:12, 117.34it/s, loss=0.881, v_num=24, train_loss_step=1.100, val_loss=0.794, train_loss_epoch=0.767] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-24 19:50:09,843]\u001b[0m Trial 0 finished with value: 0.6001598834991455 and parameters: {'neu': 725, 'lay': 1, 'bat': 8, 'lr': 0.0006013113076735698, 'num_ep': 28, 'dropout': 0.2}. Best is trial 0 with value: 0.6001598834991455.\u001b[0m\n",
      "/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "\u001b[33m[W 2023-04-24 19:50:10,283]\u001b[0m Trial 1 failed with parameters: {'neu': 700, 'lay': 1, 'bat': 4, 'lr': 0.0010510624240288073, 'num_ep': 22, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_17861/4236089926.py\", line 94, in objective\n",
      "    deepar = DeepAR.from_dataset(\n",
      "  File \"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/deepar/__init__.py\", line 196, in from_dataset\n",
      "    return super().from_dataset(\n",
      "  File \"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py\", line 1483, in from_dataset\n",
      "    return super().from_dataset(dataset, **new_kwargs)\n",
      "  File \"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py\", line 1806, in from_dataset\n",
      "    return super().from_dataset(dataset, **kwargs)\n",
      "  File \"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py\", line 996, in from_dataset\n",
      "    net = cls(**kwargs)\n",
      "  File \"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/deepar/__init__.py\", line 123, in __init__\n",
      "    self.save_hyperparameters()\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/core/mixins/hparams_mixin.py\", line 110, in save_hyperparameters\n",
      "    save_hyperparameters(self, *args, ignore=ignore, frame=frame)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py\", line 268, in save_hyperparameters\n",
      "    obj._hparams_initial = copy.deepcopy(obj._hparams)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 297, in _reconstruct\n",
      "    value = deepcopy(value, memo)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 271, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py\", line 153, in deepcopy\n",
      "    y = copier(memo)\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/torch/_tensor.py\", line 86, in __deepcopy__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment\u001b[0m\n",
      "\u001b[33m[W 2023-04-24 19:50:10,288]\u001b[0m Trial 1 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m   study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m   study\u001b[39m.\u001b[39;49moptimize(objective, timeout\u001b[39m=\u001b[39;49m\u001b[39m12000\u001b[39;49m, n_trials\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m   pruned_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mPRUNED])\n\u001b[1;32m      8\u001b[0m   complete_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mCOMPLETE])\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[7], line 94\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     81\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     82\u001b[0m     max_epochs\u001b[39m=\u001b[39mnum_ep,\n\u001b[1;32m     83\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m#-1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     callbacks\u001b[39m=\u001b[39m[lr_logger,metrics_callback]\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[39m#print(f\"training routing:\\n \\n {trainer}\")\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m deepar \u001b[39m=\u001b[39m DeepAR\u001b[39m.\u001b[39;49mfrom_dataset(\n\u001b[1;32m     95\u001b[0m     train_dataset,\n\u001b[1;32m     96\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m     97\u001b[0m     hidden_size\u001b[39m=\u001b[39;49mneu,\n\u001b[1;32m     98\u001b[0m     rnn_layers\u001b[39m=\u001b[39;49mlay,\n\u001b[1;32m     99\u001b[0m     dropout\u001b[39m=\u001b[39;49mdrop,\n\u001b[1;32m    100\u001b[0m     loss\u001b[39m=\u001b[39;49mLoss,\n\u001b[1;32m    101\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m    102\u001b[0m     log_val_interval\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m    103\u001b[0m     log_gradient_flow\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    104\u001b[0m     \u001b[39m# reduce_on_plateau_patience=3,\u001b[39;49;00m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    108\u001b[0m torch\u001b[39m.\u001b[39mset_num_threads(\u001b[39m10\u001b[39m)\n\u001b[1;32m    109\u001b[0m trainer\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    110\u001b[0m     deepar,\n\u001b[1;32m    111\u001b[0m     train_dataloaders\u001b[39m=\u001b[39mtrain_dataloader,\n\u001b[1;32m    112\u001b[0m     val_dataloaders\u001b[39m=\u001b[39mval_dataloader,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/deepar/__init__.py:196\u001b[0m, in \u001b[0;36mDeepAR.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(new_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), MultivariateDistributionLoss):\n\u001b[1;32m    193\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    194\u001b[0m         dataset\u001b[39m.\u001b[39mmin_prediction_length \u001b[39m==\u001b[39m dataset\u001b[39m.\u001b[39mmax_prediction_length\n\u001b[1;32m    195\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mMultivariate models require constant prediction lenghts\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(\n\u001b[1;32m    197\u001b[0m     dataset, allowed_encoder_known_variable_names\u001b[39m=\u001b[39;49mallowed_encoder_known_variable_names, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py:1483\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m new_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   1464\u001b[0m     static_categoricals\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   1465\u001b[0m     time_varying_categoricals_encoder\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1480\u001b[0m     categorical_groups\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mvariable_groups,\n\u001b[1;32m   1481\u001b[0m )\n\u001b[1;32m   1482\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> 1483\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n",
      "File \u001b[0;32m~/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py:1806\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \u001b[39massert\u001b[39;00m lag \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(lags\u001b[39m.\u001b[39mget(target, [])), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall target lags in dataset must be the same but found \u001b[39m\u001b[39m{\u001b[39;00mlags\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mtarget_lags\u001b[39m\u001b[39m\"\u001b[39m, {name: dataset\u001b[39m.\u001b[39m_get_lagged_names(name) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m lags})\n\u001b[0;32m-> 1806\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/base_model.py:996\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moutput_transformer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    995\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_transformer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtarget_normalizer\n\u001b[0;32m--> 996\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    997\u001b[0m net\u001b[39m.\u001b[39mdataset_parameters \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_parameters()\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m dataset\u001b[39m.\u001b[39mmulti_target:\n",
      "File \u001b[0;32m~/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/pytorch_forecasting/models/deepar/__init__.py:123\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         n_plotting_samples \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_hyperparameters()\n\u001b[1;32m    124\u001b[0m \u001b[39m# store loss function separately as it is a module\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(loss\u001b[39m=\u001b[39mloss, logging_metrics\u001b[39m=\u001b[39mlogging_metrics, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/core/mixins/hparams_mixin.py:110\u001b[0m, in \u001b[0;36mHyperparametersMixin.save_hyperparameters\u001b[0;34m(self, ignore, frame, logger, *args)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m current_frame:\n\u001b[1;32m    109\u001b[0m         frame \u001b[39m=\u001b[39m current_frame\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 110\u001b[0m save_hyperparameters(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, ignore\u001b[39m=\u001b[39;49mignore, frame\u001b[39m=\u001b[39;49mframe)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268\u001b[0m, in \u001b[0;36msave_hyperparameters\u001b[0;34m(obj, ignore, frame, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m         rank_zero_warn(\n\u001b[1;32m    263\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttribute \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m!r}\u001b[39;00m\u001b[39m is an instance of `nn.Module` and is already saved during checkpointing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m It is recommended to ignore them using `self.save_hyperparameters(ignore=[\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m!r}\u001b[39;00m\u001b[39m])`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    267\u001b[0m \u001b[39m# make a deep copy so there are no other runtime changes reflected\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m obj\u001b[39m.\u001b[39m_hparams_initial \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(obj\u001b[39m.\u001b[39;49m_hparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/torch/_tensor.py:86\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__deepcopy__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, memo)\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly Tensors created explicitly by the user \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(graph leaves) support the deepcopy protocol at the moment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment"
     ]
    }
   ],
   "source": [
    "########## optuna results #####################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  study = optuna.create_study(direction=\"minimize\")\n",
    "  study.optimize(objective, timeout=12000, n_trials=150)\n",
    "\n",
    "  pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "  complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "  print(\"Study statistics: \")\n",
    "  print(\"  Number of finished trials: \", len(study.trials))\n",
    "  print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "  print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "  print(\"Best trial:\")\n",
    "  trial = study.best_trial\n",
    "\n",
    "  print(\"  Value: \", trial.value)\n",
    "\n",
    "  print(\"  Params: \")\n",
    "  for key, value in trial.params.items(): ## this is same as study.best_params\n",
    "      print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "  fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "  fig.show()\n",
    "\n",
    "  fig = optuna.visualization.plot_optimization_history(study)\n",
    "  fig.show()\n",
    "\n",
    "  fig = optuna.visualization.plot_slice(study)\n",
    "  fig.show()\n",
    "\n",
    "  fig = optuna.visualization.plot_param_importances(study)\n",
    "  fig.show()\n",
    "\n",
    "  print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "  neurons = study.best_params[\"neu\"]\n",
    "  layers = study.best_params[\"lay\"]\n",
    "  batch_size = study.best_params[\"bat\"]\n",
    "  learning_rate = study.best_params[\"lr\"]\n",
    "  dropout = study.best_params[\"dropout\"]\n",
    "  encoder_length = encoder_length\n",
    "  max_epochs = study.best_params[\"num_ep\"]\n",
    "\n",
    "  train_and_forecast(neurons,layers,batch_size,learning_rate,dropout,encoder_length,max_epochs,region,full_train_data,val_data,test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepar-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
