{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "\u001b[32m[I 2023-04-28 15:48:51,020]\u001b[0m A new study created in memory with name: no-name-fbe35df9-fc94-44ce-86d5-71f2fe9c7b56\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data has null values? False\n",
      "train_data has null values? False\n",
      "val_data has null values? False\n",
      "test_data has null values? False\n",
      "This trial parameters, neu: 1200, lay: 2, bat: 4, lr: 0.005710679494280534, enc_len: 12, drop: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "Missing logger folder: /home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/demand_data/standalone/country_level/train_val_test_data/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                             | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | loss                   | NegativeBinomialDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList                       | 0     \n",
      "2 | embeddings             | MultiEmbedding                   | 1.5 K \n",
      "3 | rnn                    | LSTM                             | 18.0 M\n",
      "4 | distribution_projector | Linear                           | 2.4 K \n",
      "----------------------------------------------------------------------------\n",
      "18.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.0 M    Total params\n",
      "72.016    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|â–Œ         | 1715/30192 [00:14<04:04, 116.57it/s, loss=1.4, v_num=0, train_loss_step=0.812]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-28 15:49:09,739]\u001b[0m Trial 0 failed with parameters: {'neu': 1200, 'lay': 2, 'bat': 4, 'lr': 0.005710679494280534, 'enc_len': 12, 'dropout': 0.2} because of the following error: ValueError('attempt to get argmin of an empty sequence').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/optimusprime/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_6368/537116922.py\", line 357, in objective\n",
      "    min_val_loss_epoch = np.argmin(metrics_list)\n",
      "  File \"<__array_function__ internals>\", line 200, in argmin\n",
      "  File \"/home/optimusprime/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 1338, in argmin\n",
      "    return _wrapfunc(a, 'argmin', axis=axis, out=out, **kwds)\n",
      "  File \"/home/optimusprime/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 54, in _wrapfunc\n",
      "    return _wrapit(obj, method, *args, **kwds)\n",
      "  File \"/home/optimusprime/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n",
      "    result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "ValueError: attempt to get argmin of an empty sequence\u001b[0m\n",
      "\u001b[33m[W 2023-04-28 15:49:09,743]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 391\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39m######### optuna results #####################\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m# if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m    390\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, timeout\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m    393\u001b[0m pruned_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mPRUNED])\n\u001b[1;32m    394\u001b[0m complete_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mCOMPLETE])\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/deepar-gpu/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[1], line 357\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39m#   metrics_list = [ metrics[\"val_RMSE\"].item()  for metrics in  metrics_callback.metrics[1:]]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m#   min_val_rmse_epoch = np.argmin(metrics_list)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39m#   min_val_rmse = np.min(metrics_list) #metrics_list[-1]\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m#   print('\\n\\n\\nmin_val_rmse ',min_val_rmse,'\\n\\n\\n\\n')\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39m#   print('\\n\\n\\min_val_rmse_epoch ',min_val_rmse_epoch,'\\n\\n\\n\\n')\u001b[39;00m\n\u001b[1;32m    356\u001b[0m metrics_list \u001b[39m=\u001b[39m [ metrics[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()  \u001b[39mfor\u001b[39;00m metrics \u001b[39min\u001b[39;00m  metrics_callback\u001b[39m.\u001b[39mmetrics[\u001b[39m1\u001b[39m:]]\n\u001b[0;32m--> 357\u001b[0m min_val_loss_epoch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmin(metrics_list)\n\u001b[1;32m    358\u001b[0m min_val_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmin(metrics_list) \u001b[39m#metrics_list[-1]\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mmin_val_loss \u001b[39m\u001b[39m'\u001b[39m,min_val_loss,\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1338\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[39mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1338\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmin\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pympler\n",
    "from pympler import asizeof\n",
    "\"\"\"Windows\n",
    "os.chdir(\"c:/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR-pytorch/My_model/2_freq_nbinom_LSTM\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Linux\"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM\")\n",
    "sys.path.append(os.path.abspath(os.path.join(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM\")))\n",
    "\n",
    "\n",
    "\"\"\"colab\n",
    "os.chdir('/content/drive/MyDrive/DeepAR_demand_prediction-linux-gpu/2_freq_nbinom_LSTM')\n",
    "\"\"\"\n",
    "\n",
    "#from ctypes import FormatError\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os,sys\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM')))\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data\\weather_data')))\n",
    "# sys.path.append(os.path.abspath(os.path.join('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\2_freq_nbinom_LSTM\\1_cluster_demand_prediction\\data\\demand_data')))\n",
    "\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, RMSE\n",
    "from torchmetrics import R2Score, SymmetricMeanAbsolutePercentageError, MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import torch\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "import os,sys\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from scipy.signal import find_peaks\n",
    "import operator\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "from pytorch_forecasting import Baseline\n",
    "import random\n",
    "from pytorch_forecasting import DeepAR,NegativeBinomialDistributionLoss\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import plotly\n",
    "#from deepar_RegionWise_LinuxGpu_prediction_dev import train_and_forecast\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\"\"\"\n",
    "Set Random seed\n",
    "\"\"\"\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "## additional seeding to ensure reproduciblility.\n",
    "pl.seed_everything(0)\n",
    "\n",
    "\"\"\"windows\n",
    "os.chdir(\"c:/Work/WORK_PACKAGE/Demand_forecasting/github/DeepAR-pytorch/My_model/2_freq_nbinom_LSTM/1_cluster_demand_prediction\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# def train_and_forecast(neurons,layers,batch_size,learning_rate,dropout,encoder_length,max_epochs,region,full_train_data,val_data,test_data,full_train_datetime, test_datetime,season_len,num_past_seas):\n",
    "\n",
    "\n",
    "#     return\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "set inputs here\n",
    "(hyperparameters grid search)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "######### MISCELLANEOUS ###################\n",
    "region =\"Singapore\"\n",
    "cov_lag_len= 0 #we can use forecasted values, even for inflow\n",
    "Target = 'target'\n",
    "#encoder_length = 18\n",
    "######### MISCELLANEOUS ###################\n",
    "\n",
    "\n",
    "######### Network Architecture ###################\n",
    "p = 7 # patience no. of epochs\n",
    "\n",
    "Loss=NegativeBinomialDistributionLoss()\n",
    "\n",
    "######### Network Architecture ###################\n",
    "\n",
    "\n",
    "######### Training Routine ###################\n",
    "fdv_steps = 10 # fast_dev_run\n",
    "######### Training Routine ###################\n",
    "\n",
    "\n",
    "############## Inputs for 2) Persistance model ( seasonal naive forecast ) #######################\n",
    "season_len = 164 # length of season\n",
    "num_past_seas = 6 # number of past seasons to use in averaging\n",
    "#seas_pred_strt_idx = 2035 # seasonal naive forecast start index, in hours use the df dataframe\n",
    "############## Inputs for 2) Persistance model ( seasonal naive forecast ) #######################\n",
    "\n",
    "\"\"\"Linux\"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/demand_data/standalone/country_level/train_val_test_data\")\n",
    "\n",
    "\n",
    "\"\"\" colab \n",
    "os.chdir('/content/drive/MyDrive/DeepAR_demand_prediction-linux-gpu/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/demand_data/standalone/country_level/train_val_test_data')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Import pre-processed Data\n",
    "\n",
    "response and target are the same thing\n",
    "\"\"\"\n",
    "\n",
    "###### All clusters training ########\n",
    "# all_clstr_train_dem_data = pd.read_csv(region+'_all_clstr_train_dem_data.csv')\n",
    "# all_clstr_full_train_dem_data = pd.read_csv(region+'_all_clstr_full_train_dem_data.csv')\n",
    "# all_clstr_val_dem_data = pd.read_csv(region+'_all_clstr_val_dem_data.csv')\n",
    "# all_clstr_test_dem_data = pd.read_csv(region+'_all_clstr_test_dem_data.csv')\n",
    "# train_data = all_clstr_train_dem_data\n",
    "# full_train_data = all_clstr_full_train_dem_data\n",
    "# val_data = all_clstr_val_dem_data\n",
    "# test_data = all_clstr_test_dem_data\n",
    "###### All clusters training ########\n",
    "\n",
    "\n",
    "###### fewer number of clusters to train faster ########\n",
    "fewer_clstr_train_dem_data = pd.read_csv(region+'_fewer_clstr_train_dem_data.csv')\n",
    "fewer_clstr_full_train_dem_data = pd.read_csv(region+'_fewer_clstr_full_train_dem_data.csv')\n",
    "fewer_clstr_val_dem_data = pd.read_csv(region+'_fewer_clstr_val_dem_data.csv')\n",
    "fewer_clstr_test_dem_data = pd.read_csv(region+'_fewer_clstr_test_dem_data.csv')\n",
    "train_data = fewer_clstr_train_dem_data\n",
    "full_train_data = fewer_clstr_full_train_dem_data\n",
    "val_data = fewer_clstr_val_dem_data\n",
    "test_data = fewer_clstr_test_dem_data\n",
    "###### fewer number of clusters to train faster ########\n",
    "\n",
    "\n",
    "train_data\n",
    "#################### add date information ts ####################\n",
    "train_data[\"datetime\"] = pd.to_datetime(train_data[\"datetime\"])\n",
    "train_data['_hour_of_day'] = train_data[\"datetime\"].dt.hour.astype(str)\n",
    "train_data['_day_of_week'] = train_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "train_data['_day_of_month'] = train_data[\"datetime\"].dt.day.astype(str)\n",
    "train_data['_day_of_year'] = train_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "train_data['_week_of_year'] = train_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "train_data['_month_of_year'] = train_data[\"datetime\"].dt.month.astype(str)\n",
    "train_data['_year'] = train_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "full_train_data[\"datetime\"] = pd.to_datetime(full_train_data[\"datetime\"])\n",
    "full_train_data['_hour_of_day'] = full_train_data[\"datetime\"].dt.hour.astype(str)\n",
    "full_train_data['_day_of_week'] = full_train_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "full_train_data['_day_of_month'] = full_train_data[\"datetime\"].dt.day.astype(str)\n",
    "full_train_data['_day_of_year'] = full_train_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "full_train_data['_week_of_year'] = full_train_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "full_train_data['_month_of_year'] = full_train_data[\"datetime\"].dt.month.astype(str)\n",
    "full_train_data['_year'] = full_train_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "val_data[\"datetime\"] = pd.to_datetime(val_data[\"datetime\"])\n",
    "val_data['_hour_of_day'] = val_data[\"datetime\"].dt.hour.astype(str)\n",
    "val_data['_day_of_week'] = val_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "val_data['_day_of_month'] = val_data[\"datetime\"].dt.day.astype(str)\n",
    "val_data['_day_of_year'] = val_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "val_data['_week_of_year'] = val_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "val_data['_month_of_year'] = val_data[\"datetime\"].dt.month.astype(str)\n",
    "val_data['_year'] = val_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "#################### add date information ts ####################\n",
    "test_data[\"datetime\"] = pd.to_datetime(test_data[\"datetime\"])\n",
    "test_data['_hour_of_day'] = test_data[\"datetime\"].dt.hour.astype(str)\n",
    "test_data['_day_of_week'] = test_data[\"datetime\"].dt.dayofweek.astype(str)\n",
    "test_data['_day_of_month'] = test_data[\"datetime\"].dt.day.astype(str)\n",
    "test_data['_day_of_year'] = test_data[\"datetime\"].dt.dayofyear.astype(str)\n",
    "test_data['_week_of_year'] = test_data[\"datetime\"].dt.weekofyear.astype(str)\n",
    "test_data['_month_of_year'] = test_data[\"datetime\"].dt.month.astype(str)\n",
    "test_data['_year'] = test_data[\"datetime\"].dt.year.astype(str)\n",
    "#################### add date information ts ####################\n",
    "\n",
    "\n",
    "#print(list(train_data.columns))\n",
    "\n",
    "\"\"\"\n",
    "CHecK for null values\n",
    "\"\"\"\n",
    "\n",
    "print(\"train_data has null values?\",train_data.isnull().values.any())\n",
    "print(\"train_data has null values?\",full_train_data.isnull().values.any())\n",
    "print(\"val_data has null values?\",val_data.isnull().values.any())\n",
    "print(\"test_data has null values?\",test_data.isnull().values.any())\n",
    "\n",
    "\"\"\" \n",
    "Drop datetime column\n",
    "\"\"\"\n",
    "train_datetime = train_data['datetime']\n",
    "full_train_datetime = full_train_data['datetime']\n",
    "val_datetime = val_data['datetime']\n",
    "test_datetime = test_data['datetime']\n",
    "train_data = train_data.drop(columns=['datetime'])\n",
    "full_train_data = full_train_data.drop(columns=['datetime'])\n",
    "val_data = val_data.drop(columns=['datetime'])\n",
    "test_data = test_data.drop(columns=['datetime'])\n",
    "\n",
    "\n",
    "# for c in train_data.columns[3:-7]:\n",
    "#     print(c)\n",
    "\n",
    "\"\"\"\n",
    "Full Training Routine \n",
    "with bayesisan hyperparmeter search\n",
    "\n",
    "Load data into TimeSeriesDataSet object\n",
    "\n",
    "for fast development run\n",
    "uncomment fast_dev_run = fdv_steps\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=p, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "\n",
    "class MetricsCallback(pl.Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)\n",
    "\n",
    "\n",
    "def objective(trial,):  \n",
    "  \n",
    "    neu = trial.suggest_int(name=\"neu\",low=800,high=1200,step=50,log=False)\n",
    "    lay = trial.suggest_int(name=\"lay\",low=2,high=4,step=1,log=False)\n",
    "    bat = trial.suggest_int(name=\"bat\",low=4,high=8,step=4,log=False)\n",
    "    lr = trial.suggest_float(name=\"lr\",low=0.000001,high=0.01,log=True)\n",
    "    num_ep = 30# trial.suggest_int(name=\"num_ep\",low=25,high=40,step=2,log=False)\n",
    "    enc_len = trial.suggest_int(name=\"enc_len\",low=8,high=18,step=2,log=False)\n",
    "    #enc_len = encoder_length\n",
    "    pred_len = 1\n",
    "    drop = trial.suggest_float(name=\"dropout\",low=0,high=0.4,step=0.1,log=False)\n",
    "    print(f'This trial parameters, neu: {neu}, lay: {lay}, bat: {bat}, lr: {lr}, enc_len: {enc_len}, drop: {drop}')\n",
    "\n",
    "    num_cols_list = list(train_data.columns[3:-7])\n",
    "\n",
    "    cat_dict = {\"_hour_of_day\": NaNLabelEncoder(add_nan=True).fit(train_data._hour_of_day), \\\n",
    "    \"_day_of_week\": NaNLabelEncoder(add_nan=True).fit(train_data._day_of_week), \"_day_of_month\" : NaNLabelEncoder(add_nan=True).fit(train_data._day_of_month), \"_day_of_year\" : NaNLabelEncoder(add_nan=True).fit(train_data._day_of_year), \\\n",
    "        \"_week_of_year\": NaNLabelEncoder(add_nan=True).fit(train_data._week_of_year), \"_month_of_year\": NaNLabelEncoder(add_nan=True).fit(train_data._month_of_year) ,\"_year\": NaNLabelEncoder(add_nan=True).fit(train_data._year) }\n",
    "    cat_list = [\"_hour_of_day\",\"_day_of_week\",\"_day_of_month\",\"_day_of_year\",\"_week_of_year\",\"_month_of_year\",\"_year\"]  \n",
    "\n",
    "    train_dataset = TimeSeriesDataSet(\n",
    "        train_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=Target,\n",
    "        categorical_encoders=cat_dict,\n",
    "        group_ids=[\"group\"],\n",
    "        min_encoder_length=enc_len,\n",
    "        max_encoder_length=enc_len,\n",
    "        min_prediction_length=pred_len,\n",
    "        max_prediction_length=pred_len,\n",
    "        time_varying_unknown_reals=[Target],\n",
    "        time_varying_known_reals=num_cols_list,\n",
    "        time_varying_known_categoricals=cat_list,\n",
    "        add_relative_time_idx=False,\n",
    "        randomize_length=False,\n",
    "        scalers={},\n",
    "        target_normalizer=TorchNormalizer(method=\"identity\",center=False,transformation=None )\n",
    "\n",
    "    )\n",
    "\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(train_dataset,val_data, stop_randomization=True, predict=False)\n",
    "\n",
    "    train_dataloader = train_dataset.to_dataloader(train=True, batch_size=bat)\n",
    "    val_dataloader = val_dataset.to_dataloader(train=False, batch_size=bat)\n",
    "    ######### Load DATA #############\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Machine Learning predictions START\n",
    "    1) DeepAR \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_ep,\n",
    "        gpus=-1, #-1\n",
    "        auto_lr_find=False,\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=1.0,\n",
    "        limit_val_batches=1.0,\n",
    "        logger=True,\n",
    "        val_check_interval=1.0,\n",
    "        callbacks=[lr_logger,metrics_callback]\n",
    "    )\n",
    "\n",
    "    #print(f\"training routing:\\n \\n {trainer}\")\n",
    "    deepar = DeepAR.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=lr,\n",
    "        hidden_size=neu,\n",
    "        rnn_layers=lay,\n",
    "        dropout=drop,\n",
    "        loss=Loss,\n",
    "        log_interval=20,\n",
    "        log_val_interval=6,\n",
    "        log_gradient_flow=False,\n",
    "        # reduce_on_plateau_patience=3,\n",
    "    )\n",
    "\n",
    "\n",
    "    torch.set_num_threads(10)\n",
    "    trainer.fit(\n",
    "        deepar,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "\n",
    "    #   metrics_list = [ metrics[\"val_RMSE\"].item()  for metrics in  metrics_callback.metrics[1:]]\n",
    "    #   min_val_rmse_epoch = np.argmin(metrics_list)\n",
    "    #   min_val_rmse = np.min(metrics_list) #metrics_list[-1]\n",
    "    #   print('\\n\\n\\nmin_val_rmse ',min_val_rmse,'\\n\\n\\n\\n')\n",
    "    #   print('\\n\\n\\min_val_rmse_epoch ',min_val_rmse_epoch,'\\n\\n\\n\\n')\n",
    "\n",
    "    metrics_list = [ metrics[\"val_loss\"].item()  for metrics in  metrics_callback.metrics[1:]]\n",
    "    min_val_loss_epoch = np.argmin(metrics_list)\n",
    "    min_val_loss = np.min(metrics_list) #metrics_list[-1]\n",
    "    print('\\n\\n\\nmin_val_loss ',min_val_loss,'\\n\\n\\n\\n')\n",
    "    print('\\n\\n\\min_val_loss_epoch ',min_val_loss_epoch,'\\n\\n\\n\\n')\n",
    "\n",
    "    trial.report(min_val_loss, min_val_loss_epoch)\n",
    "\n",
    "\n",
    "    ##### Clear RAM memory #####\n",
    "    del num_cols_list\n",
    "    del cat_dict\n",
    "    del cat_list\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del train_dataloader\n",
    "    del val_dataloader\n",
    "    del metrics_callback\n",
    "    del trainer\n",
    "    del deepar\n",
    "    ##### Clear RAM memory #####\n",
    "\n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return min_val_loss #min_val_rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### optuna results #####################\n",
    "# if __name__ == '__main__':\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, timeout=None, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items(): ## this is same as study.best_params\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "fig.show()\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "fig = optuna.visualization.plot_slice(study)\n",
    "fig.show()\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "neurons = study.best_params[\"neu\"]\n",
    "layers = study.best_params[\"lay\"]\n",
    "batch_size = study.best_params[\"bat\"]\n",
    "learning_rate = study.best_params[\"lr\"]\n",
    "dropout = study.best_params[\"dropout\"]\n",
    "encoder_length = study.best_params[\"enc_len\"]\n",
    "max_epochs = 30#study.best_params[\"num_ep\"]\n",
    "\n",
    "#train_and_forecast(neurons,layers,batch_size,learning_rate,dropout,encoder_length,max_epochs,region,full_train_data,val_data,test_data, full_train_datetime, test_datetime,season_len,num_past_seas)\n",
    "\n",
    "import os,sys\n",
    "\"\"\" Linux\"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/model/standalone\")\n",
    "sys.path.append(os.path.abspath(os.path.join(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/model/standalone\")))\n",
    "\n",
    "\n",
    "\"\"\" colab \n",
    "os.chdir('/content/drive/MyDrive/DeepAR_demand_prediction-linux-gpu/2_freq_nbinom_LSTM/1_cluster_demand_prediction/model/standalone')\n",
    "\"\"\"\n",
    "import json    \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "\"\"\"\n",
    "Set Random seed\n",
    "\"\"\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "## additional seeding to ensure reproduciblility.\n",
    "pl.seed_everything(0)\n",
    "\n",
    "Target = 'target'\n",
    "\n",
    "print(list(full_train_data.columns))\n",
    "\n",
    "\"\"\"\n",
    "set inputs here\n",
    "(hyperparameters grid search)\n",
    "\n",
    "\"\"\"\n",
    "######### Network Architecture definition ###################\n",
    "\n",
    "###### Create hyperparameters grid ###### \n",
    "pred_len = 1\n",
    "hparams_grid = {\"LSTM_neuron_size\":[neurons],\n",
    "                \"num_layers\":[layers],\n",
    "                \"batch_size\":[batch_size],\n",
    "                \"learning_rate\":[learning_rate],\n",
    "                \"max_encoder_length\":[encoder_length],\n",
    "                \"max_prediction_length\":[pred_len],\n",
    "                \"dropout\":[dropout],\n",
    "                #\"cov_pair\":cov_pairs_list,# [cov_pairs_list[7]],\n",
    "                \"Num_epochs\":[max_epochs]}#[18,20,22,24,26,28,30]}\n",
    "                #\"Num_epochs\":[16,18,20,22,24,26,28]}\n",
    "\n",
    "###### Create hyperparameters grid ###### \n",
    "\n",
    "\n",
    "Loss=NegativeBinomialDistributionLoss()\n",
    "######### Network Architecture definition ###################\n",
    "\n",
    "\n",
    "######### Training Routine ###################\n",
    "fdv_steps = 10 # fast_dev_run\n",
    "######### Training Routine ###################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Full Training Routine \n",
    "with hyperparmeter grid search\n",
    "\n",
    "Load data into TimeSeriesDataSet object\n",
    "\n",
    "for fast development run\n",
    "uncomment fast_dev_run = fdv_steps\n",
    "\n",
    "\"\"\"\n",
    "#early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-8, patience=p, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "RMSE_list = [] # FIND minimum RMSE case\n",
    "hyperparams_list = [] # FIND minimum RMSE case\n",
    "\n",
    "num_cols_list = []\n",
    "\n",
    "cat_dict = {\"_hour_of_day\": NaNLabelEncoder(add_nan=True).fit(full_train_data._hour_of_day), \\\n",
    "\"_day_of_week\": NaNLabelEncoder(add_nan=True).fit(full_train_data._day_of_week), \"_day_of_month\" : NaNLabelEncoder(add_nan=True).fit(full_train_data._day_of_month), \"_day_of_year\" : NaNLabelEncoder(add_nan=True).fit(full_train_data._day_of_year), \\\n",
    "    \"_week_of_year\": NaNLabelEncoder(add_nan=True).fit(full_train_data._week_of_year), \"_month_of_year\": NaNLabelEncoder(add_nan=True).fit(full_train_data._month_of_year) ,\"_year\": NaNLabelEncoder(add_nan=True).fit(full_train_data._year) }\n",
    "cat_list = [\"_hour_of_day\",\"_day_of_week\",\"_day_of_month\",\"_day_of_year\",\"_week_of_year\",\"_month_of_year\",\"_year\"]  \n",
    "\n",
    "num_cols_list = list(full_train_data.columns[3:-7]) \n",
    "\n",
    "# param_comb_cnt=-1\n",
    "for neu,lay,bat,lr,enc_len,pred_len,drop,num_ep in product(*[x for x in hparams_grid.values()]):\n",
    "        \n",
    "    ######### Load DATA #############\n",
    "    full_train_dataset = TimeSeriesDataSet(\n",
    "        full_train_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=Target,\n",
    "        categorical_encoders=cat_dict,\n",
    "        group_ids=[\"group\"],\n",
    "        min_encoder_length=enc_len,\n",
    "        max_encoder_length=enc_len,\n",
    "        min_prediction_length=pred_len,\n",
    "        max_prediction_length=pred_len,\n",
    "        time_varying_unknown_reals=[Target],\n",
    "        time_varying_known_reals=num_cols_list,\n",
    "        time_varying_known_categoricals=cat_list,\n",
    "        add_relative_time_idx=False,\n",
    "        randomize_length=False,\n",
    "        scalers={},\n",
    "        target_normalizer=TorchNormalizer(method=\"identity\",center=False,transformation=None )\n",
    "    )\n",
    "\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(full_train_dataset,val_data, stop_randomization=True, predict=False)\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(full_train_dataset,test_data, stop_randomization=True)\n",
    "\n",
    "    full_train_dataloader = full_train_dataset.to_dataloader(train=True, )\n",
    "    val_dataloader = val_dataset.to_dataloader(train=False,  )\n",
    "    test_dataloader = test_dataset.to_dataloader(train=False, )\n",
    "    ######### Load DATA #############\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Machine Learning predictions START\n",
    "    1) DeepAR\n",
    "\n",
    "    \"\"\"\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_ep,\n",
    "        gpus=-1, #-1\n",
    "        auto_lr_find=False,\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=1.0,\n",
    "        limit_val_batches=1.0,\n",
    "        #fast_dev_run=fdv_steps,\n",
    "        logger=True,\n",
    "        #log_every_n_steps=10,\n",
    "        # profiler=True,\n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        #enable_checkpointing=True,\n",
    "        #default_root_dir=\"C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\2_freq_nbinom_LSTM\\1_cluster_demand_prediction\\logs\"\n",
    "    )\n",
    "\n",
    "\n",
    "    #print(f\"training routing:\\n \\n {trainer}\")\n",
    "    deepar = DeepAR.from_dataset(\n",
    "        full_train_dataset,\n",
    "        learning_rate=lr,\n",
    "        hidden_size=neu,\n",
    "        rnn_layers=lay,\n",
    "        dropout=drop,\n",
    "        loss=Loss,\n",
    "        log_interval=20,\n",
    "        log_val_interval=1.0,\n",
    "        log_gradient_flow=False,\n",
    "        # reduce_on_plateau_patience=3,\n",
    "    )\n",
    "\n",
    "        \n",
    "    #print(f\"Number of parameters in network: {deepar.size()/1e3:.1f}k\")\n",
    "    # print(f\"Model :\\n \\n {deepar}\")\n",
    "    #torch.set_num_threads(10)\n",
    "    trainer.fit(\n",
    "        deepar,\n",
    "        train_dataloaders=full_train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    \"\"\" \n",
    "    ########## DEEPAR Prediction #####################\n",
    "    \"\"\"\n",
    "    ########## DEEPAR Prediction #####################\n",
    "    test_output = deepar.predict(data=test_dataloader,mode='prediction',return_index=True,num_workers=8,show_progress_bar=True)\n",
    "\n",
    "    ## test_output data structure ##\n",
    "    ## test_output = [ [x,x,x,x,x,...], [ {'time_idx': [x,x,x,x,x,...] , 'group': [x,x,x,x,x,...]} ]  ]\n",
    "    ## len(test_output[0]) includes all the groups combined\n",
    "\n",
    "    actual = {}\n",
    "    prediction = {}\n",
    "\n",
    "    for i in range(len(test_output[0])):\n",
    "        time_idx_i = test_output[1]['time_idx'][i]\n",
    "        grp_i = test_output[1]['group'][i]\n",
    "\n",
    "        actual[(time_idx_i,grp_i)] = test_data[(test_data['time_idx'] == time_idx_i) & (test_data['group'] == grp_i)]['target'].values[0]\n",
    "        prediction[(time_idx_i,grp_i)] = test_output[0][i]\n",
    "\n",
    "    groups_list = np.unique(test_output[1]['group'])\n",
    "    time_idx_list = np.unique(test_output[1]['time_idx'])\n",
    "\n",
    "    deepar_RMSE_grp_dict = {}\n",
    "    deepar_actuals_grp_dict = {}\n",
    "    deepar_predictions_grp_dict = {}\n",
    "\n",
    "    for grp in groups_list:\n",
    "        actuals_grp_list = []\n",
    "        predictions_grp_list = []\n",
    "        for t in time_idx_list:\n",
    "            actuals_grp_list.append( int(actual[(t,grp)] ))\n",
    "            predictions_grp_list.append(int(prediction[(t,grp)]))\n",
    "\n",
    "        deepar_actuals_grp_dict[str(grp)] = actuals_grp_list\n",
    "        deepar_predictions_grp_dict[str(grp)] = predictions_grp_list\n",
    "\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.title('DEEPAR: line plot, cluster: '+str(grp))  \n",
    "        plt.plot(actuals_grp_list,'^-')\n",
    "        plt.plot(predictions_grp_list,'*-')\n",
    "        plt.show()\n",
    "\n",
    "        plt.xlabel('actual')\n",
    "        plt.ylabel('prediction')\n",
    "        plt.title('DEEPAR: scatter plot, cluster: '+str(grp))       \n",
    "        plt.scatter(actuals_grp_list,predictions_grp_list )\n",
    "        plt.show()\n",
    "\n",
    "        # cm = confusion_matrix(actuals_grp_list,predictions_grp_list)\n",
    "        # max_classes = min(len(np.unique(actuals_grp_list)),len(np.unique(predictions_grp_list)))\n",
    "        # df_cm = pd.DataFrame(cm, index = [i for i in range(max_classes)],\n",
    "        #                 columns = [i for i in range(max_classes)])\n",
    "        # plt.figure(figsize = (10,7))      \n",
    "        # s = sn.heatmap(df_cm, annot=True, )\n",
    "        # s.set(xlabel='Predicted-Label', ylabel='True-Label',title='DEEPAR: cluster: '+str(grp))\n",
    "        max_classes = max(np.unique(actuals_grp_list)[-1], np.unique(predictions_grp_list)[-1])\n",
    "        cm = confusion_matrix(actuals_grp_list, predictions_grp_list, labels=range(max_classes))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(max_classes))\n",
    "        disp.plot()\n",
    "        plt.title('DEEPAR: cluster: '+str(grp))\n",
    "        plt.show()\n",
    "\n",
    "        deepar_RMSE_grp_dict[str(grp)] = np.sqrt( mean_squared_error( np.array(actuals_grp_list) , np.array(predictions_grp_list) )   )\n",
    "        print(f'DEEPAR: RMSE from cluster: {grp} = {deepar_RMSE_grp_dict[str(grp)]}')\n",
    "        print(f'\\nDEEPAR: PRECISION, RECALL & F1 scores for cluster: {grp} = {classification_report(actuals_grp_list,predictions_grp_list)}\\n')\n",
    "\n",
    "    total_actual_list = []\n",
    "    total_prediction_list = []\n",
    "    for k in actual.keys():\n",
    "        total_actual_list.append(int(actual[(k[0],k[1])]))\n",
    "        total_prediction_list.append(int(prediction[(k[0],k[1])] ))\n",
    "\n",
    "    # cm = confusion_matrix(total_actual_list,total_prediction_list)\n",
    "    # max_classes = min(len(np.unique(total_actual_list)),len(np.unique(total_prediction_list)))\n",
    "    # df_cm = pd.DataFrame(cm, index = [i for i in range(max_classes)],\n",
    "    #                 columns = [i for i in range(max_classes)])\n",
    "    # plt.figure(figsize = (10,7))      \n",
    "    # s = sn.heatmap(df_cm, annot=True, )\n",
    "    # s.set(xlabel='Predicted-Label', ylabel='True-Label',title='DEEPAR, Region Total: '+str(region))\n",
    "    max_classes = max(np.unique(total_actual_list)[-1], np.unique(total_prediction_list)[-1])\n",
    "    cm = confusion_matrix(total_actual_list, total_prediction_list, labels=range(max_classes))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(max_classes))\n",
    "    disp.plot()\n",
    "    plt.title('DEEPAR: Region: '+region)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    Total_rmse = np.sqrt( mean_squared_error( np.array(total_actual_list) , np.array(total_prediction_list) )   )\n",
    "    print('DEEPAR: Total RMSE from all clusters: ',Total_rmse)\n",
    "    print(f'\\nDEEPAR: PRECISION, RECALL & F1 scores for Region: {region} = {classification_report(total_actual_list,total_prediction_list)}\\n')\n",
    "\n",
    "    print('\\n Hyperparameters: neu,lay,bat,lr,enc_len,pred_len,drop,\\n')\n",
    "    print(neu,lay,bat,lr,enc_len,pred_len,drop,' \\n')\n",
    "\n",
    "    ########## DEEPAR Prediction #####################\n",
    "    \"\"\"\n",
    "    Machine Learning predictions END\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    ########## HISTORIC AVERAGE Prediction #####################\n",
    "    \"\"\"\n",
    "    ########## HA Prediction #####################\n",
    "    #TODO: merge full_train_data and test_data to do historic average predictions\n",
    "\n",
    "    full_train_data['datetime'] = full_train_datetime\n",
    "    test_data['datetime'] = test_datetime\n",
    "    entire_data =  pd.concat([full_train_data,test_data]).sort_values(by=['group','datetime']).reset_index(drop=True)  \n",
    "\n",
    "    ha_RMSE_grp_dict = {}\n",
    "    ha_actuals_grp_dict = {}\n",
    "    ha_predictions_grp_dict = {}\n",
    "\n",
    "    total_actual_list = []\n",
    "    total_prediction_list = []\n",
    "\n",
    "    for grp in groups_list:\n",
    "        err_list = np.array([])\n",
    "        actuals_grp_list = []\n",
    "        predictions_grp_list = []\n",
    "        for t in time_idx_list:\n",
    "            actuals_grp_list.append( int(test_data[(test_data['time_idx'] == t) & (test_data['group'] == grp)]['target'].values[0]) )\n",
    "\n",
    "            dt = test_data[ (test_data['time_idx'] == t) & (test_data['group'] == grp) ]['datetime']\n",
    "            hist = [ entire_data[(entire_data['group']== grp) & (entire_data['datetime'] == (dt - pd.Timedelta(hour, \"h\")).values[0])]['target'].values for hour in range(season_len,season_len*num_past_seas,season_len) ]\n",
    "            predictions_grp_list.append(int(np.mean(hist)))\n",
    "\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.title('HA: line plot, cluster: '+str(grp))  \n",
    "        plt.plot(actuals_grp_list,'^-')\n",
    "        plt.plot(predictions_grp_list,'*-')\n",
    "        plt.show()\n",
    "\n",
    "        ha_actuals_grp_dict[str(grp)] = actuals_grp_list\n",
    "        ha_predictions_grp_dict[str(grp)] = predictions_grp_list\n",
    "\n",
    "        ha_RMSE_grp_dict[str(grp)] = np.sqrt( mean_squared_error( np.array(actuals_grp_list) , np.array(predictions_grp_list) )   )\n",
    "        print(f'HA: RMSE from cluster: {grp} = {ha_RMSE_grp_dict[str(grp)]}')\n",
    "        print(f'\\nHA: PRECISION, RECALL & F1 scores for cluster: {grp} = {classification_report(actuals_grp_list,predictions_grp_list)}\\n')\n",
    "\n",
    "        total_actual_list.append(actuals_grp_list)\n",
    "        total_prediction_list.append(predictions_grp_list)\n",
    "\n",
    "    tot_actual_flat_arr = np.array(total_actual_list).flatten()\n",
    "    tot_prediction_flat_arr = np.array(total_prediction_list).flatten()\n",
    "\n",
    "    Total_rmse = np.sqrt( mean_squared_error( tot_actual_flat_arr , tot_prediction_flat_arr )  )\n",
    "    print('HA: Total RMSE from all clusters: ',Total_rmse)\n",
    "    print(f'\\HA: PRECISION, RECALL & F1 scores for Region: {region} = {classification_report(tot_actual_flat_arr,tot_prediction_flat_arr)}\\n')\n",
    "\n",
    "    ########## HA Prediction #####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############### Saving Results and optimal hyperparameters ########################\n",
    "\"\"\"\n",
    "SAVE all results and hparams to json files for dashboard visualization\n",
    "\n",
    "\"\"\"\n",
    "results_dict = {\"actuals_dem\":deepar_actuals_grp_dict,\"all_clstrs_deepar_pred\":deepar_predictions_grp_dict, \"all_clstrs_hist_avg\":ha_predictions_grp_dict}\n",
    "results_dict[\"best_deepar_hyperparams\"] = {\"neu\":neurons,\"lay\":layers,\"bat\":batch_size,\"lr\":learning_rate,\"drop\":dropout,\"enc_len\":encoder_length}\n",
    "results_dict[\"RMSE_DAR\"] = deepar_RMSE_grp_dict\n",
    "results_dict[\"HA_RMSE\"] = ha_RMSE_grp_dict\n",
    "\"\"\" linux\"\"\"\n",
    "os.chdir(\"/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/results_data\")\n",
    "\n",
    "\n",
    "\"\"\" colab \n",
    "os.chdir('/content/drive/MyDrive/DeepAR_demand_prediction-linux-gpu/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/results_data')\n",
    "\"\"\"\n",
    "\n",
    "# Serializing json\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "json_object = json.dumps(results_dict, indent=4, default=np_encoder)\n",
    "# Writing to .json\n",
    "#with open(region+\"_fewer_dev2.json\", \"w\") as outfile:\n",
    "with open(region+\"_fewer_clstrs.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "############### Saving Results and optimal hyperparameters ########################\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepar-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
