{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['126', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
      "       '175'],\n",
      "      dtype='object')\n",
      "Index(['126', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
      "       '175'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNeed to adjust the dataset for this.\\n\\nTODO: Need to handle cov_lag_len here.\\n\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data')\n",
    "\n",
    "df_dem = pd.read_csv('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data\\demand_data\\\\tampines_region_demand2.csv')\n",
    "df_inf = pd.read_csv('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data\\demand_data\\\\tampines_region_inflow2.csv')\n",
    "\n",
    "cov_lag_len = 1\n",
    "\n",
    "\n",
    "df_dem = pd.DataFrame(df_dem.iloc[cov_lag_len:]).reset_index(drop=True)\n",
    "df_inf = pd.DataFrame(df_inf.iloc[:-cov_lag_len]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_dem_lag_168 = df_dem[:-168].reset_index()\n",
    "df_dem_lag_336 = df_dem[:-336].reset_index()\n",
    "\n",
    "df_inf_lag_168 = df_inf[:-168].reset_index()\n",
    "df_inf_lag_336 = df_inf[:-336].reset_index()\n",
    "\n",
    "df_dem = df_dem[584:].reset_index() # make it a medium dataset\n",
    "df_inf = df_inf[584:].reset_index() # make it a medium dataset\n",
    "\n",
    "\n",
    "df_dem_lag_168 = df_dem_lag_168[416:].reset_index() # make it a medium dataset\n",
    "df_inf_lag_168 = df_inf_lag_168[416:].reset_index() # make it a medium dataset\n",
    "\n",
    "df_dem_lag_336 = df_dem_lag_336[248:].reset_index() # make it a medium dataset\n",
    "df_inf_lag_336 = df_inf_lag_336[248:].reset_index() # make it a medium dataset\n",
    "\n",
    "\n",
    "# df_dem_lag_168 = df_dem_lag_168[168:].reset_index()\n",
    "# df_dem = df_dem[336:].reset_index()\n",
    "\n",
    "# df_inf_lag_168 = df_inf_lag_168[168:].reset_index()\n",
    "# df_inf = df_inf[336:].reset_index()\n",
    "\n",
    "\n",
    "df_dem = df_dem.drop([\"index\"],axis=1)\n",
    "df_inf = df_inf.drop([\"index\"],axis=1)\n",
    "df_dem_lag_168 = df_dem_lag_168.drop([\"index\", \"level_0\"],axis=1)\n",
    "df_inf_lag_168 = df_inf_lag_168.drop([\"index\", \"level_0\"],axis=1)\n",
    "df_dem_lag_336 = df_dem_lag_336.drop([\"index\"],axis=1)\n",
    "df_inf_lag_336 = df_inf_lag_336.drop([\"index\"],axis=1)\n",
    "\n",
    "print(df_dem.columns)\n",
    "print(df_dem.columns)\n",
    "\n",
    "#new_df = pd.DataFrame(columns=[\"target\",\"group\",\"inflow\",\"dem_lag_168\",\"dem_lag_336\",\"inf_lag_168\",\"inf_lag_336\" ])\n",
    "\n",
    "#arr = np.empty((0,7), int)\n",
    "train_arr = np.empty((0,7), int)\n",
    "val_arr = np.empty((0,7), int)\n",
    "test_arr = np.empty((0,7), int)\n",
    "\n",
    "g=0\n",
    "for c in df_dem.columns:\n",
    "\n",
    "    s = pd.Series(np.repeat(g,len(df_inf)))\n",
    "    df_grp = pd.DataFrame({c:s})\n",
    "\n",
    "    df_concat = pd.concat([df_dem[c], df_grp, df_inf[c], df_dem_lag_168[c],df_dem_lag_336[c], df_inf_lag_168[c], df_inf_lag_336[c] ],axis=1 )\n",
    "    \n",
    "\n",
    "\n",
    "    df_concat = df_concat.to_numpy()\n",
    "    len_df = df_concat.shape[0]\n",
    "    #print(arr.shape)\n",
    "\n",
    "    #arr = np.append(arr, df_concat,axis=0) \n",
    "    tr_stop_idx = int(0.7*len_df)\n",
    "    val_stop_idx = (int(0.7*len_df) + int(0.2*len_df))\n",
    "    # print(tr_stop_idx)\n",
    "    # print(val_stop_idx)\n",
    "\n",
    "    train_arr = np.append(train_arr, df_concat[:tr_stop_idx],axis=0)\n",
    "    val_arr = np.append(val_arr, df_concat[tr_stop_idx:val_stop_idx ],axis=0)\n",
    "    test_arr = np.append(test_arr, df_concat[val_stop_idx:],axis=0)\n",
    "\n",
    "    g+=1\n",
    "\n",
    "# new_df = pd.DataFrame(arr, columns=[\"target\",\"group\",\"inflow\",\"dem_lag_168\",\"dem_lag_336\",\"inf_lag_168\",\"inf_lag_336\" ])\n",
    "# new_df['time_idx'] = np.arange(0,len(new_df),1)\n",
    "# first_column = new_df.pop('time_idx')\n",
    "# new_df.insert(0, 'time_idx', first_column)\n",
    "\n",
    "\n",
    "new_train_df = pd.DataFrame(train_arr, columns=[\"target\",\"group\",\"inflow\",\"dem_lag_168\",\"dem_lag_336\",\"inf_lag_168\",\"inf_lag_336\" ])\n",
    "new_train_df['time_idx'] = np.arange(0,len(new_train_df),1)\n",
    "first_column = new_train_df.pop('time_idx')\n",
    "new_train_df.insert(0, 'time_idx', first_column)\n",
    "\n",
    "\n",
    "new_val_df = pd.DataFrame(val_arr, columns=[\"target\",\"group\",\"inflow\",\"dem_lag_168\",\"dem_lag_336\",\"inf_lag_168\",\"inf_lag_336\" ])\n",
    "new_val_df['time_idx'] = np.arange(0,len(new_val_df),1)\n",
    "first_column = new_val_df.pop('time_idx')\n",
    "new_val_df.insert(0, 'time_idx', first_column)\n",
    "\n",
    "\n",
    "new_test_df = pd.DataFrame(test_arr, columns=[\"target\",\"group\",\"inflow\",\"dem_lag_168\",\"dem_lag_336\",\"inf_lag_168\",\"inf_lag_336\" ])\n",
    "new_test_df['time_idx'] = np.arange(0,len(new_test_df),1)\n",
    "first_column = new_test_df.pop('time_idx')\n",
    "new_test_df.insert(0, 'time_idx', first_column)\n",
    "\n",
    "\n",
    "#print(new_df)\n",
    "\n",
    "os.chdir(\"C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\\\2_freq_nbinom_LSTM\\\\1_cluster_demand_prediction\\data\\demand_data\")\n",
    "\n",
    "#new_df.to_csv('tampines_all_clstr_dem_data.csv')\n",
    "new_train_df.to_csv('tampines_all_clstr_train_dem_data.csv')\n",
    "new_val_df.to_csv('tampines_all_clstr_val_dem_data.csv')\n",
    "new_test_df.to_csv('tampines_all_clstr_test_dem_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Need to adjust the dataset for this.\n",
    "\n",
    "TODO: Need to handle cov_lag_len here.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0213e7566dd6df184afb4bbdab7d267fb988f5f901680f5ca1af43b2a6441d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
