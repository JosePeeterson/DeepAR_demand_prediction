{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":24171,"status":"ok","timestamp":1690098310512,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"blHmfLEZi-WZ","outputId":"4161036c-12b8-4bf6-f0fd-86ed0bdfe9ac"},"outputs":[{"data":{"text/plain":["'\\nextract features and cluster feature vectors from all stations.\\nuse closest actual station to centroid to be the representative station.\\nOnly analyze the representative stations and NOT all.\\nThis is a dimensionality reduction technique.\\n\\n'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from datetime import datetime\n","import datetime as dt\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.signal import correlate\n","import plotly.graph_objects as go\n","import statsmodels.api as sm\n","from scipy import stats\n","from scipy.stats import iqr\n","from itertools import combinations\n","#from sklearn_extra.cluster import KMedoids\n","from sklearn.cluster import DBSCAN\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.manifold import TSNE\n","from umap import UMAP\n","import matplotlib.image as mpimg\n","import hdbscan\n","from sklearn.cluster import AgglomerativeClustering\n","from tslearn.metrics import dtw,lcss\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from tslearn.clustering import TimeSeriesKMeans\n","#from tslearn.clustering import silhouette_score\n","from sklearn.metrics import silhouette_samples,silhouette_score\n","import geopandas as gpd\n","import time\n","from collections import Counter\n","import math\n","from sklearn.manifold import Isomap\n","import pywt\n","import pywt.data\n","import plotly.subplots as sp\n","from ipywidgets import interact, interactive, fixed, interact_manual\n","import ipywidgets as widgets\n","from IPython.display import display\n","from IPython.display import clear_output\n","from mpl_toolkits.mplot3d import Axes3D\n","import plotly.express as px\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from matplotlib.pyplot import get_cmap\n","from matplotlib.pyplot import cm\n","from scipy.spatial import distance\n","from sklearn.cluster import AgglomerativeClustering\n","import scipy.cluster.hierarchy as sch\n","import matplotlib.colors as mcolors\n","import matplotlib.dates as mdates\n","from scipy.signal import find_peaks\n","from statsmodels.tsa.stattools import acf,pacf\n","\n","%matplotlib inline\n","#%matplotlib notebook\n","\"\"\"\n","extract features and cluster feature vectors from all stations.\n","use closest actual station to centroid to be the representative station.\n","Only analyze the representative stations and NOT all.\n","This is a dimensionality reduction technique.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1690098310512,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"MtI3CF_Ii-Wd","outputId":"0ad86ce1-9d61-4539-bcd1-dfd741d4a163"},"outputs":[{"data":{"text/plain":["'\\nVisualize demand on a map\\n'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Visualize demand on a map\n","\"\"\"\n","\n","# housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n","# s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n","# c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n","# )\n","# plt.legend()\n","\n","#TODO: AIDITI"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"VejI4ctVi-Wd"},"outputs":[{"data":{"text/plain":["\"\\nos.chdir('C:\\\\Work\\\\WORK_PACKAGE\\\\Demand_forecasting\\\\github\\\\DeepAR-pytorch\\\\My_model\\\\Rental_Fleet_Demand_Forecast\\\\data\\\\station_level\\\\inflow_data')\\n\""]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Extract the station-level inflow and outflow data\n","\n","\"\"\"\n","\n","# unmet outflow\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_level\\outflow_data')\n","\"\"\"\n","\"\"\"colab\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_level\\outflow_data')\n","\n","station_unmet_60min_outflow_df = pd.read_parquet(\"UnmetDemand_df_station_level.parquet\")\n","# met outflow\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_level\\outflow_data')\n","\"\"\"\n","#station_met_outflow_10min_df = pd.read_csv(\"station_outflow_10_0.01_('2021-09-24', 0)_to_('2021-12-23', 114).csv\")\n","station_met_outflow_60min_df = pd.read_csv(\"station_outflow_60_0.01_('2021-09-24', 0)_to_('2021-12-23', 19).csv\")\n","station_met_outflow_60min_df.index = station_unmet_60min_outflow_df.index\n","station_met_outflow_60min_df.insert(0, 'hour',station_met_outflow_60min_df.index.strftime('%H') )\n","station_met_outflow_60min_df.insert(0, 'weekday',station_met_outflow_60min_df.index.strftime('%A') )\n","station_met_outflow_60min_df.insert(0, 'week',station_met_outflow_60min_df.index.strftime('%W') )\n","## multiindexing\n","days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n","#Using Categorical() function to set the order according to how it is arranged above\n","station_met_outflow_60min_df[\"weekday\"] = pd.Categorical(station_met_outflow_60min_df.weekday, categories=days, ordered=True)\n","\n","\n","# total outflow\n","\n","\n","# inflow\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_level\\inflow_data')\n","\"\"\"\n","#station_inflow_10min_df = pd.read_csv(\"station_inflow_10_0.01_('2021-09-24', 0)_to_('2021-12-23', 114).csv\")\n","#station_inflow_60min_df = pd.read_csv(\"station_inflow_60_0.01_('2021-09-24', 0)_to_('2021-12-23', 19).csv\")"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"YxLj6jL3cfbU"},"outputs":[],"source":["\"\"\"\n","                                            Dimensionality Reduction along time axis from 9 weeks to 1 week by seasonality.\n","\n","\"\"\"\n","\n","\"\"\"\n","####### station unmet demand ########\n","Dim_Red_stn_unmet_60min_outflow_df = station_unmet_60min_outflow_df.copy()\n","Dim_Red_stn_unmet_60min_outflow_df.insert(0, 'datetime',Dim_Red_stn_unmet_60min_outflow_df.index )\n","Dim_Red_stn_unmet_60min_outflow_df = Dim_Red_stn_unmet_60min_outflow_df.reset_index(drop=True)\n","Dim_Red_stn_unmet_60min_outflow_df\n","\n","#Dim_Red_stn_unmet_60min_outflow_df['datetime'] = Dim_Red_stn_unmet_60min_outflow_df['datetime'].apply( lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\") )\n","\n","Dim_Red_stn_unmet_60min_outflow_df.insert(1, 'weekday',Dim_Red_stn_unmet_60min_outflow_df['datetime'].dt.strftime('%A') )\n","Dim_Red_stn_unmet_60min_outflow_df.insert(1, 'hour',Dim_Red_stn_unmet_60min_outflow_df['datetime'].dt.strftime('%H') )\n","\n","#You first create your list in the order you want it\n","days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n","#Using Categorical() function to set the order according to how it is arranged above\n","Dim_Red_stn_unmet_60min_outflow_df[\"weekday\"] = pd.Categorical(Dim_Red_stn_unmet_60min_outflow_df.weekday, categories=days, ordered=True)\n","#Dim_Red_stn_unmet_60min_outflow_df.drop(columns=['datetime'],inplace=True)\n","Dim_Red_stn_unmet_60min_outflow_week_df = Dim_Red_stn_unmet_60min_outflow_df.groupby ( by=[\"weekday\",\"hour\"] ).agg([np.mean])\n","\n","Dim_Red_stn_unmet_60min_outflow_week_df.drop(columns=['datetime'],inplace=True)\n","# fig, ax = plt.subplots(figsize=(35,50))         # Sample figsize in inches\n","# ax.set_title('Weekday-hour mean Groupby TOTAL outflow/demand')\n","# sns.heatmap(Dim_Red_stn_unmet_60min_outflow_week_df, annot=True,linewidths=.5, ax=ax)\n","####### station unmet demand ########\n","\"\"\"\n","\n","\n","####### station met demand ########\n","\n","Dim_Red_stn_met_60min_outflow_week_df = station_met_outflow_60min_df.drop(columns=['week']).groupby ( by=[\"weekday\",\"hour\"] ).agg([np.mean])\n","\n","# fig, ax = plt.subplots(figsize=(35,50))         # Sample figsize in inches\n","# ax.set_title('Weekday-hour mean Groupby TOTAL outflow/demand')\n","# sns.heatmap(Dim_Red_stn_met_60min_outflow_week_df, annot=True,linewidths=.5, ax=ax)\n","####### station met demand ########\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Usbgf779x5l6"},"outputs":[],"source":["## staiton to gps map to visualize clusters of staitons on map\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_names_n_cluster_center_id')\n","stations_df = pd.read_csv('stations.csv')\n","stations_df\n","\n","station_to_gps_dict = {}\n","\n","for r in range(len(stations_df)):\n","    stn = stations_df.loc[r,'ID']\n","    lat = stations_df.loc[r,'x']\n","    long = stations_df.loc[r,'y']\n","    station_to_gps_dict[stn] = (lat,long)\n","\n","\n","#station_to_gps_dict\n","\n","station_to_index_dict = {}\n","\n","for r in range(len(stations_df)):\n","    stn = stations_df.loc[r,'ID']\n","    lat = stations_df.loc[r,'x']\n","    long = stations_df.loc[r,'y']\n","    station_to_gps_dict[stn] = (lat,long)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"J2vkxdRAhbdZ"},"outputs":[],"source":["# def feature_standardization(X_train):\n","#     # ensure that features of different scale are comparable using euclidean distance\n","#     X_train_Df = pd.DataFrame(X_train)\n","#     X_train_normalized =(X_train_Df-X_train_Df.mean())/X_train_Df.std()\n","#     X_train_normalized = X_train_normalized.fillna(0)\n","#     return X_train_normalized.to_numpy()\n","\n","class feat_transformation():\n","    def __init__(self,):\n","        return\n","    def feature_normalization(self,X_train):\n","        # ensure that features of different scale are comparable using euclidean distance\n","        self.scaler = MinMaxScaler() #StandardScaler()\n","        self.scaler.fit(X_train)\n","        X_train_normalized = self.scaler.transform(X_train)\n","        return X_train_normalized\n","\n","    def inv_feature_normalization(self,X_train_normalized):\n","        # ensure that features of different scale are comparable using euclidean distance\n","        X_train = self.scaler.inverse_transform(X_train_normalized)\n","        return X_train\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"vpRjc-JssACp"},"outputs":[],"source":["## Visualize the clustered time series\n","\n","\n","def plot_stations_of_clusters(labels, df,TS_type,X_train,plot_type,angular_labels,plot_title,mother_wavelet,limit_level):\n","    unique_clusters = np.unique(labels)\n","    num_clusters = len(unique_clusters)\n","    cluster_to_stations_dict = { l: [] for l in unique_clusters}\n","\n","    cols = df.columns\n","    station_to_index_dict = {}\n","    index=0\n","    if isinstance(cols[index], tuple): # differentiate between Dim_Red_unmet_60min_outflow_week_df DF and unmet_60min_outflow_df DF\n","      for stn in cols: # station to index mapping\n","        station_to_index_dict[stn[0]] = index\n","        index+=1\n","    else:\n","      for stn in cols: # station to index mapping\n","        station_to_index_dict[stn] = index\n","        index+=1\n","\n","    idx=0\n","    if isinstance(cols[idx], tuple): # differentiate between Dim_Red_unmet_60min_outflow_week_df DF and unmet_60min_outflow_df DF\n","      for l in labels:\n","        cluster_to_stations_dict[l].append(cols[idx][0])\n","        idx+=1\n","    else:\n","      for l in labels:\n","        cluster_to_stations_dict[l].append(cols[idx])\n","        idx+=1\n","\n","    shapefile_data = gpd.read_file('sgp_map/SGP_ADM1.shp',)\n","\n","    num_features = X_train.shape[1]\n","    all_cluster_feat_mins = np.array([])\n","    all_cluster_feat_maxs = np.array([])\n","\n","    ################## par coords plot ##################\n","    if(plot_type == 'par_coords'): # parallel coordinates\n","        for key in cluster_to_stations_dict.keys():\n","          #plt.figure(figsize=(10,10))\n","          fig0, ax = plt.subplots()\n","          shapefile_data.plot(ax=ax,edgecolor='black', linewidth=0.9, color='lightblue',figsize=(5,5))\n","\n","          #fig = go.Figure()\n","          #fig1 = go.Figure()\n","          fig2, ax2 = plt.subplots(figsize=(30,5))\n","          fig3, ax3 = plt.subplots(figsize=(60,5))\n","          fig4, ax4 = plt.subplots(figsize=(30,5))\n","\n","          color_feature = 0  # The feature index to use for colors\n","          color_map = get_cmap('tab20')  # Choose a colormap\n","\n","          d = {}\n","\n","          feat_min_per_cluster = [1000]*num_features\n","          feat_max_per_cluster = [-1000]*num_features\n","\n","          for station in cluster_to_stations_dict[key]:\n","            ax.scatter(station_to_gps_dict[int(station)][0], station_to_gps_dict[int(station)][1], color='red', marker='.')\n","\n","            y_arr = np.array(df[station].values).reshape(-1)\n","            # fig = fig.add_trace(go.Scatter(x=np.arange(len(y_arr)), y=y_arr,mode='lines+markers',name=station))\n","            # fig.update_layout(title='cluster: '+ str(key),xaxis_title=\"time (Monday 0:00 - Sunday 23:00)\",\n","            # yaxis_title=TS_type,autosize=False,width=1800,height=400, )\n","\n","            wave_coeffs = pywt.wavedec( unmet_60min_outflow_df[station] , mother_wavelet, level=limit_level)\n","            rec_wave = pywt.waverec(wave_coeffs, mother_wavelet)\n","            ym = np.median(rec_wave)\n","            ax3.plot(np.linspace(0, 1., num=len(rec_wave)), rec_wave-ym)\n","            #ax3.plot(unmet_60min_outflow_df[station])\n","\n","            d['r'] = X_train[station_to_index_dict[station]]\n","            # d['theta'] = angular_labels\n","            # fig1.add_trace(go.Scatterpolar(\n","            #     r=d['r'],\n","            #     theta=d['theta'],\n","            #     fill='toself',name=station,\n","            # ))\n","\n","            sample = X_train[station_to_index_dict[station]]  # Get the current sample\n","            color = color_map(sample[color_feature])  # Get the color based on the color feature value\n","            ax2.plot(range(num_features), sample, color=color, alpha=0.7, label=station)\n","\n","            sns.histplot(data=unmet_60min_outflow_df[station], kde=False, stat=\"count\",binwidth=1, ax=ax4)\n","            ax5 = ax4.twinx()\n","            sns.kdeplot(data=unmet_60min_outflow_df[station], ax=ax5)\n","\n","            for f in range(num_features):\n","                feat_min_per_cluster[f] = min(feat_min_per_cluster[f],sample[f])\n","                feat_max_per_cluster[f] = max(feat_max_per_cluster[f],sample[f])\n","\n","          all_cluster_feat_mins = np.append(all_cluster_feat_mins,np.array(feat_min_per_cluster))\n","          all_cluster_feat_maxs = np.append(all_cluster_feat_maxs,np.array(feat_max_per_cluster))\n","\n","          # fig1.update_layout(title='cluster: '+ str(key),\n","          #     polar=dict(\n","          #         radialaxis=dict(visible=True,))#range=[-2, 7]))\n","          #     ,height=800,width=800,\n","          #     showlegend=True\n","          # )\n","\n","          ax.set_xlabel('Longitude')\n","          ax.set_ylabel('Latitude')\n","          ax.set_title('Singapore Stations of cluster: '+ str(key))\n","          fig0.show()\n","          #fig.show()\n","          #fig1.show()\n","\n","          ax2.set_xticks(range(num_features))\n","          ax2.set_xticklabels([i for i in angular_labels])\n","          ax2.set_ylabel('Normalized Value')\n","          # Adjust the y-axis limits based on your data range\n","          #ax2.set_ylim(0, 1)  # Modify as needed\n","          ax2.set_title(plot_title+', cluster: '+str(key)+' has '+str(len(cluster_to_stations_dict[key]))+' stations')\n","          #ax2.legend()\n","          ax2.grid(True, linestyle='--')\n","          #fig2.tight_layout()\n","          fig2.show()\n","\n","          #ax3.set_xticklabels([i for i in unmet_60min_outflow_df.index])\n","          plt.xticks(rotation=90)\n","          ax3.set_ylabel('Low frequency, '+'level: '+ str(limit_level) + ', '+ mother_wavelet +' Wavelet reconstruction')\n","          #ax3.set_ylim(0, 1)  # Modify as needed\n","          ax3.set_title(plot_title+', cluster: '+str(key)+' has '+str(len(cluster_to_stations_dict[key]))+' stations')\n","          #ax3.legend()\n","          ax3.grid(True, linestyle='--')\n","          #fig3.tight_layout()\n","          fig3.show()\n","\n","          ax4.set_xlabel('Demand value')\n","          ax4.set_ylabel('Density/Frequency')\n","          ax4.set_title('station''s Kernel Density Plots of cluster: '+str(key))\n","          ax4.legend()\n","          fig4.show()\n","          print('Number of stations in cluster '+ str(key)+' : ', len(cluster_to_stations_dict[key]))\n","          print('Stations in cluster: ',cluster_to_stations_dict[key])\n","    ################## par coords plot ##################\n","\n","    ################## polar plot ##################\n","\n","    elif(plot_type == 'polar'):\n","        angular_labels = [ str(l) for l in range( X_train.shape[1] )]\n","        for key in cluster_to_stations_dict.keys():\n","          fig0, ax = plt.subplots(figsize=(10,5))\n","          shapefile_data.plot(ax=ax,edgecolor='black', linewidth=0.9, color='lightblue',figsize=(5,5))\n","\n","          #fig = go.Figure()\n","          fig1 = go.Figure()\n","\n","          d = {}\n","\n","          feat_min_per_cluster = [1000]*num_features\n","          feat_max_per_cluster = [-1000]*num_features\n","\n","          for station in cluster_to_stations_dict[key]:\n","            ax.scatter(station_to_gps_dict[int(station)][0], station_to_gps_dict[int(station)][1], color='red', marker='.')\n","\n","            y_arr = np.array(df[station].values).reshape(-1)\n","            # fig = fig.add_trace(go.Scatter(x=np.arange(len(y_arr)), y=y_arr,mode='lines+markers',name=station))\n","            # fig.update_layout(title='cluster: '+ str(key),xaxis_title=\"time (Monday 0:00 - Sunday 23:00)\",\n","            # yaxis_title=TS_type,autosize=False,width=1800,height=400, )\n","\n","            d['r'] = X_train[station_to_index_dict[station]]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=station,\n","            ))\n","\n","            sample = X_train[station_to_index_dict[station]]  # Get the current sample\n","            for f in range(num_features):\n","                feat_min_per_cluster[f] = min(feat_min_per_cluster[f],sample[f])\n","                feat_max_per_cluster[f] = max(feat_max_per_cluster[f],sample[f])\n","\n","          all_cluster_feat_mins = np.append(all_cluster_feat_mins,np.array(feat_min_per_cluster))\n","          all_cluster_feat_maxs = np.append(all_cluster_feat_maxs,np.array(feat_max_per_cluster))\n","\n","          fig1.update_layout(title='cluster: '+ str(key),\n","              polar=dict(\n","                  radialaxis=dict(visible=True,))#range=[-2, 7]))\n","              ,height=1000,width=1000,\n","              showlegend=True\n","          )\n","\n","          ax.set_xlabel('Longitude')\n","          ax.set_ylabel('Latitude')\n","          ax.set_title('Singapore Stations of cluster '+ str(key))\n","          fig0.show()\n","          #fig.show()\n","          fig1.show()\n","          print('Number of stations in cluster '+ str(key)+' : ', len(cluster_to_stations_dict[key]))\n","          print('Stations in cluster: ',cluster_to_stations_dict[key])\n","    ################## polar plot ##################\n","    else:\n","        raise ValueError('only par_coords or are polar allowed')\n","\n","    all_cluster_feat_mins = np.hstack(all_cluster_feat_mins)\n","    all_cluster_feat_maxs = np.hstack(all_cluster_feat_maxs)\n","    return all_cluster_feat_mins,all_cluster_feat_maxs\n","\n","#plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'outflow')\n","\n","def plot_silhouette_scores(X_train,labels,metric,linkage):\n","    unique_clusters = np.unique(labels)\n","    num_clusters = len(unique_clusters)\n","    sample_silhouette_values = silhouette_samples(X=X_train, labels=labels,metric=metric)\n","    silhouette_avg = silhouette_score(X=X_train, labels=labels,metric=metric)\n","    fig, ax1 = plt.subplots()\n","    y_lower = 10\n","\n","    for i in range(num_clusters): # exclude the\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n","        ith_cluster_silhouette_values.sort()\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","        color = cm.nipy_spectral(float(i) / num_clusters)\n","        ax1.fill_betweenx(\n","            np.arange(y_lower, y_upper),\n","            0,\n","            ith_cluster_silhouette_values,\n","            facecolor=color,\n","            edgecolor=color,\n","            alpha=0.7,\n","        )\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","    ax1.set_title(\"metric: \"+metric+\", linkage: \"+linkage+\", silhouette_avg: \"+str(silhouette_avg)+\", num_clusters: \"+str(num_clusters))\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","    return\n","\n","\n","def plot_cluster_representatives(X_train,all_cluster_feat_mins,all_cluster_feat_maxs,plot_type,angular_labels):\n","    num_features = X_train.shape[1]\n","    low_list =  all_cluster_feat_mins[num_features:] ## remove outlier cluster\n","    high_list = all_cluster_feat_maxs[num_features:] ## remove outlier cluster\n","    num_clusters = int(len(all_cluster_feat_mins[num_features:] )/num_features) ## remove outlier cluster\n","    color = cm.rainbow(np.linspace(0, 1, num_clusters))\n","\n","    low = {}\n","    high = {}\n","    c = 0\n","    idx=0\n","    low[0] = []\n","    high[0] = []\n","    for l,h in zip(low_list,high_list):\n","        low[c].append(l)\n","        high[c].append(h)\n","        idx+=1\n","        if idx%num_features==0:\n","            c+=1\n","            low[c] = []\n","            high[c] = []\n","\n","\n","    if(plot_type == 'par_coords'): # parallel coordinates\n","        plt.close('all')\n","\n","        #fig,ax = plt.subplots(figsize=(20,7))\n","\n","        cluster_rep_arr = np.empty((num_clusters,num_features))\n","        adj=0\n","        for c in range(num_clusters):\n","            for f in range(num_features):\n","              cluster_rep_arr[c,f] = np.average([low[c][f],high[c][f]])\n","              #ax.plot([f+adj]*2,[low[c][f],high[c][f]],c=color[c],marker='*',label=str(c))\n","            adj += 0.05\n","\n","        # ax.set_xticks(range(num_features))\n","        # ax.set_xticklabels([i for i in angular_labels])\n","        # ax.set_title('Representative points from each cluster for '+str(num_clusters) + ' clusters '+ 'excluding outlier cluster')\n","        # ax.set_xlabel('TS stats features')\n","        # ax.set_ylabel('Normalized value')\n","        # ax.grid(True, linestyle='--')\n","        # ax.legend()\n","        # fig.show()\n","\n","\n","        # Convert the array and labels to a DataFrame\n","        df = pd.DataFrame(cluster_rep_arr, columns=angular_labels)\n","        df['sample'] = range(len(df))\n","        # Create parallel coordinates figure using Plotly Express\n","        fig2 = px.parallel_coordinates(df,color='sample',color_continuous_scale=\"picnic\",dimensions=df.columns[:-1],)\n","        # Display the plot\n","        fig2.update_layout(\n","            plot_bgcolor='black',  # Set the background color to black\n","            paper_bgcolor='black',  # Set the plot area background color to black\n","            font_color='white'  # Set the font color to white for better visibility\n","        )\n","        fig2.show()\n","\n","\n","    elif(plot_type == 'polar'): # polar plot\n","        plt.close('all')\n","        angular_labels = [ str(l) for l in range( X_train.shape[1] )]\n","\n","        cmap = cm.get_cmap('tab20c')\n","        colors = [mcolors.rgb2hex(cmap(i)[:3]) for i in range(num_clusters)]\n","\n","        d = {}\n","        fig1 = go.Figure()\n","\n","        for c in range(num_clusters):\n","            color = colors[c % num_clusters]\n","            d['r'] = low[c]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=str(c)+'_low',\n","                line=dict(color=color),\n","            ))\n","            d['r'] = high[c]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=str(c)+'_high',\n","                line=dict(color=color),\n","            ))\n","\n","        fig1.update_layout(title='Representative DWT points from each cluster. (excluding outlier cluster)',\n","            polar=dict(\n","                radialaxis=dict(visible=True,))#range=[-2, 7]))\n","            ,height=1000,width=1000,\n","            showlegend=True\n","        )\n","        fig1.show()\n","    return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"dpfVHs5QyKxj"},"outputs":[],"source":["\n","def compute_mahalanobis_dist_mat(X):\n","    # Calculate the covariance matrix\n","    cov_matrix = np.cov(X.T)\n","    reg_param = 0.01  # Regularization parameter\n","    # Calculate the inverse of the covariance matrix\n","    inv_cov_matrix = np.linalg.inv(cov_matrix + reg_param * np.eye(cov_matrix.shape[0]))\n","    #inv_cov_matrix = np.linalg.inv(cov_matrix)\n","    # Calculate the Mahalanobis distance matrix\n","    mahalanobis_dist_mat = distance.cdist(X, X, metric='mahalanobis', VI=inv_cov_matrix)\n","    return mahalanobis_dist_mat"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"UptRw6ygsUsN"},"outputs":[],"source":["class plot_all():\n","    def __init__(self,print_msg):\n","        self.print_msg = print_msg\n","        return\n","\n","    def plot_tSNE_2d(self,X_train,print_msg,ax,col):\n","        ## t-SNE\n","        plot_kwds = {'alpha' : 0.25, 's' : 10, 'linewidths':0}\n","        projection = TSNE(n_components=2).fit_transform(X_train)\n","        ax[col].scatter(*projection.T, **plot_kwds)\n","        ax[col].set_title('t-SNE, '+ print_msg)\n","        #plt.show()\n","        return ax\n","\n","    def plot_UMAP_2d(self,X_train,print_msg):\n","        umap_2d = UMAP(n_components=2, init='random', random_state=0)\n","        proj_2d = umap_2d.fit_transform(X_train)\n","        fig = go.Scatter(\n","            x=proj_2d[:, 0],  # X-axis values\n","            y=proj_2d[:, 1],  # Y-axis values\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ), name=print_msg\n","        )\n","        #fig.update_layout(title=print, scene=dict(xaxis_title='Dimension 1', yaxis_title='Dimension 2', zaxis_title='Dimension 3'))\n","        return fig\n","\n","\n","    def plot_UMAP_3d(self,X_train,n_neighbors,min_dist,metric):\n","        umap_3d = UMAP(n_components=3,n_neighbors=n_neighbors,min_dist=min_dist,metric=metric, init='random', random_state=0)\n","        proj_3d = umap_3d.fit_transform(X_train)\n","        fig = go.Scatter3d(\n","            x=proj_3d[:, 0],  # X-axis values from the low-dimensional representation\n","            y=proj_3d[:, 1],  # Y-axis values from the low-dimensional representation\n","            z=proj_3d[:, 2],  # Z-axis values from the low-dimensional representation\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ),name=self.print_msg\n","        )\n","        #fig.show()\n","        return fig\n","\n","    def plot_UMAP_interactive(self,n_neighbors,min_dist,metric):\n","        umap_3d = UMAP(n_components=3,n_neighbors=n_neighbors,min_dist=min_dist,metric=metric, init='random', random_state=0)\n","        proj_3d = umap_3d.fit_transform(X_train)\n","        x=proj_3d[:,0]\n","        y=proj_3d[:,1]\n","        z=proj_3d[:,2]\n","        # Create the figure and subplots\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n","\n","        # Plot 1 - 2D Plot\n","        ax1.scatter(x, y,c=np.arange(len(X_train)))\n","        ax1.set_xlabel('X')\n","        ax1.set_ylabel('Y')\n","        ax1.set_title(self.print_msg)\n","\n","        # Plot 2 - 3D Plot\n","        ax2 = fig.add_subplot(122, projection='3d')\n","        ax2.scatter(x, y, z, c=np.arange(len(X_train)))\n","        ax2.set_xlabel('X')\n","        ax2.set_ylabel('Y')\n","        ax2.set_zlabel('Z')\n","        ax2.set_title(self.print_msg)\n","\n","        # Adjust the spacing between subplots\n","        plt.subplots_adjust(wspace=0.5)\n","\n","        # Display the plot\n","        plt.show()\n","        return\n","\n","\n","    def plot_ISOMAP_2d(self,X_train,print_msg,n_neighbors=5):\n","        ## isomap\n","        # Create an instance of the Isomap class and specify the desired parameters\n","        n_components = 2  # Number of dimensions in the low-dimensional representation\n","        # Number of nearest neighbors used in constructing the neighborhood graph\n","        isomap = Isomap(n_components=n_components, n_neighbors=n_neighbors)\n","        # Fit the Isomap model to your data\n","        isomap_embedding = isomap.fit_transform(X_train)\n","        # Access the low-dimensional representation\n","        low_dim_data = isomap_embedding\n","        fig = go.Scatter(\n","            x=low_dim_data[:, 0],  # X-axis values\n","            y=low_dim_data[:, 1],  # Y-axis values\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ), name=print_msg\n","        )\n","        #fig.update_layout(title=print, scene=dict(xaxis_title='Dimension 1', yaxis_title='Dimension 2', zaxis_title='Dimension 3'))\n","        #fig.show()\n","        return fig\n","\n","    def plot_ISOMAP_3d(self,X_train,print_msg,n_neighbors=5):\n","        ## isomap\n","        # Create an instance of the Isomap class and specify the desired parameters\n","        n_components = 3  # Number of dimensions in the low-dimensional representation\n","        n_neighbors = 5  # Number of nearest neighbors used in constructing the neighborhood graph\n","        isomap = Isomap(n_components=n_components, n_neighbors=n_neighbors)\n","        # Fit the Isomap model to your data\n","        isomap_embedding = isomap.fit_transform(X_train)\n","        # Access the low-dimensional representation\n","        low_dim_data = isomap_embedding\n","        # Create a 3D scatter plot using Plotly\n","        fig = go.Scatter3d(\n","            x=low_dim_data[:, 0],  # X-axis values from the low-dimensional representation\n","            y=low_dim_data[:, 1],  # Y-axis values from the low-dimensional representation\n","            z=low_dim_data[:, 2],  # Z-axis values from the low-dimensional representation\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ),name=print_msg\n","        )\n","        #fig.show()\n","        return fig\n","\n","    def plot_hist_n_corr(self,X_train):\n","        X_train_df = pd.DataFrame(X_train)\n","        ######### histogram plot #########\n","        # check to see if any features are all zeros\n","        X_train_df.hist(figsize=(10,10))\n","        plt.show()\n","        # ######### plot correlation #########\n","        # # perform dimensionality reduction by removing highly correlated features.\n","        X_train_df_corr = X_train_df.corr()\n","        X_train_corr = X_train_df_corr.to_numpy()\n","        fig, ax = plt.subplots()\n","        cax = ax.matshow(X_train_corr, cmap='coolwarm')\n","        cbar = fig.colorbar(cax)\n","        # Loop over the data and add text annotations\n","        for i in range(X_train_corr.shape[0]):\n","            for j in range(X_train_corr.shape[1]):\n","                text = ax.text(j, i, f'{X_train_corr[i, j]:.3f}', ha='center', va='center')\n","        ax.set_title('Feature correlation matrix')\n","        plt.show()\n","        return"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"wOuAuC_VAPRo"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","Feature extracted: Interpretable Time series statistical features\n","\n","1) High/Low average demand for each day of 7 days\n","\n","\n","\"\"\"\n","\n","# groupby\n","# pivot_table\n","# resample\n","\n","def calculate_mean_crossings(list_values):\n","    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) >= np.nanmean(list_values)))[0]\n","    no_mean_crossings = len(mean_crossing_indices)\n","    return no_mean_crossings\n","\n","def calculate_max_deviation_from_n95(list_values):\n","    return np.max(list_values) - np.percentile(list_values,0.95)\n","\n","def calculate_zero_demands(list_values):\n","    return np.where(list_values==0)[0].size\n","\n","def median_of_lists(list_of_lists):\n","    combined_lists  = np.array([sublist for sublist in list_of_lists])\n","    medians = np.median(combined_lists,axis=0)\n","    return medians\n","\n","def calculate_positive_peak_autocorrelation_of_demand(list_values):\n","    view_lags = len(list_values)\n","    feat_len = 20\n","    acf1 = acf(list_values, nlags=view_lags)\n","    peak_idx_high = np.array((find_peaks(acf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(acf1)))\n","    signific_high_idx = np.where(acf1[peak_idx_high] > 1.96/np.sqrt(len(x)))\n","    positive_autocorr_lags = peak_idx_high[signific_high_idx][:feat_len]\n","    positive_autocorr_lags = np.concatenate((positive_autocorr_lags,np.zeros(feat_len-len(positive_autocorr_lags))))\n","    return list(positive_autocorr_lags)\n","\n","def calculate_negative_peak_autocorrelation_of_demand(list_values):\n","    view_lags = len(list_values)\n","    feat_len = 20\n","    acf1 = acf(list_values, nlags=view_lags)\n","    peak_idx_low = np.array((find_peaks(-1*acf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(acf1)))\n","    signific_low_idx = np.where(acf1[peak_idx_low] < (-1*1.96)/np.sqrt(len(x)))\n","    negative_autocorr_lags = peak_idx_low[signific_low_idx][:feat_len]\n","    negative_autocorr_lags = np.concatenate((negative_autocorr_lags,np.zeros(feat_len-len(negative_autocorr_lags))))\n","    return list(negative_autocorr_lags)\n","\n","def calculate_positive_peak_partial_autocorrelation_of_demand(list_values):\n","    view_lags = int(len(list_values)/2) - 1\n","    feat_len = 20\n","    pacf1 = pacf(list_values, nlags=view_lags)\n","    peak_idx_high = np.array((find_peaks(pacf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(pacf1)))\n","    signific_high_idx = np.where(pacf1[peak_idx_high] > 1.96/np.sqrt(len(x)))\n","    positive_partial_autocorr_lags = peak_idx_high[signific_high_idx][:feat_len]\n","    positive_partial_autocorr_lags = np.concatenate((positive_partial_autocorr_lags,np.zeros(feat_len-len(positive_partial_autocorr_lags))))\n","    return list(positive_partial_autocorr_lags)\n","\n","def calculate_negative_peak_partial_autocorrelation_of_demand(list_values):\n","    view_lags = int(len(list_values)/2) - 1\n","    feat_len = 20\n","    pacf1 = pacf(list_values, nlags=view_lags)\n","    peak_idx_low = np.array((find_peaks(-1*pacf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(pacf1)))\n","    signific_low_idx = np.where(pacf1[peak_idx_low] < (-1*1.96)/np.sqrt(len(x)))\n","    negative_partial_autocorr_lags = peak_idx_low[signific_low_idx][:feat_len]\n","    negative_partial_autocorr_lags = np.concatenate((negative_partial_autocorr_lags,np.zeros(feat_len-len(negative_partial_autocorr_lags))))\n","    return list(negative_partial_autocorr_lags)    \n","\n","\n","\n","def average_demand(station_met_outflow_60min_df):\n","    every_days_avg_hourly_dem = station_met_outflow_60min_df.groupby ( by=[\"weekday\",\"hour\"]).agg([np.mean])\n","    every_days_avg_hourly_dem = every_days_avg_hourly_dem.reset_index(level=[1])\n","    every_days_avg_hourly_dem = every_days_avg_hourly_dem.drop(columns=['week','hour'])\n","    every_days_avg_dem = every_days_avg_hourly_dem.groupby ( by=['weekday']).agg([np.mean])\n","    return every_days_avg_dem.values.T\n","\n","def peak_n95_demand(station_met_outflow_60min_df):\n","    every_days_n95_hourly_dem = station_met_outflow_60min_df.groupby ( by=[\"weekday\",\"hour\"]).agg(lambda x: x.quantile(0.95))\n","    every_days_n95_hourly_dem = every_days_n95_hourly_dem.reset_index(level=[1])\n","    every_days_n95_hourly_dem = every_days_n95_hourly_dem.drop(columns=['hour'])\n","    every_days_n95_dem = every_days_n95_hourly_dem.groupby ( by=['weekday']).agg(lambda x: x.quantile(0.95))\n","    return every_days_n95_dem.values.T\n","\n","\n","def max_demand(station_met_outflow_60min_df):\n","    every_days_max_hourly_dem = station_met_outflow_60min_df.groupby ( by=[\"weekday\",\"hour\"]).agg([np.max])\n","    every_days_max_hourly_dem = every_days_max_hourly_dem.reset_index(level=[1])\n","    every_days_max_hourly_dem = every_days_max_hourly_dem.drop(columns=['week','hour'])\n","    every_days_max_dem = every_days_max_hourly_dem.groupby ( by=['weekday']).agg([np.median])\n","    return every_days_max_dem.values.T\n","\n","def no_mean_crossing_demand(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_demand = station_met_outflow_60min_df.resample('D', on='Date').apply(calculate_mean_crossings)\n","    daily_demand['DayOfWeek'] = daily_demand.index.dayofweek\n","    every_days_n95_dem = pd.pivot_table(daily_demand, index='DayOfWeek', values=daily_demand.columns[:-1], aggfunc='median') # remove DayOfWeek columns\n","    return every_days_n95_dem.values.T\n","\n","\n","def outlier_demands(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['Date'])\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_demand = station_met_outflow_60min_df.resample('D', on='Date').apply(calculate_max_deviation_from_n95)\n","    daily_demand['DayOfWeek'] = daily_demand.index.dayofweek\n","    max_deviation_from_n95 = pd.pivot_table(daily_demand, index='DayOfWeek', values=daily_demand.columns[:-1], aggfunc='median') # remove DayOfWeek columns\n","    return max_deviation_from_n95.values.T\n","\n","def std_demand(station_met_outflow_60min_df):\n","    every_days_std_hourly_dem = station_met_outflow_60min_df.groupby ( by=[\"weekday\",\"hour\"]).agg([np.std])\n","    every_days_std_hourly_dem = every_days_std_hourly_dem.reset_index(level=[1])\n","    every_days_std_hourly_dem = every_days_std_hourly_dem.drop(columns=['week','hour'])\n","    every_days_std_dem = every_days_std_hourly_dem.groupby ( by=['weekday']).agg([np.median])\n","    return every_days_std_dem.values.T\n","\n","\n","def count_zero_demands(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['Date'])\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_demand = station_met_outflow_60min_df.resample('D', on='Date').apply(calculate_zero_demands)\n","    daily_demand['DayOfWeek'] = daily_demand.index.dayofweek\n","    every_days_zero_demands = pd.pivot_table(daily_demand, index='DayOfWeek', values=daily_demand.columns[:-1], aggfunc='median') # remove DayOfWeek columns\n","    return every_days_zero_demands.values.T\n","\n","\n","def kurt_of_demands(station_met_outflow_60min_df):\n","    every_days_hourly_kurt = station_met_outflow_60min_df.groupby ( by=[\"weekday\",\"hour\"]).agg([stats.kurtosis])\n","    every_days_hourly_kurt = every_days_hourly_kurt.reset_index(level=[1])\n","    every_days_hourly_kurt = every_days_hourly_kurt.drop(columns=['hour'])\n","    every_days_kurt = every_days_hourly_kurt.groupby ( by=['weekday']).agg([np.median])\n","    return every_days_kurt.values.T\n","\n","\n","def demands_positive_autocorr_lags(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_pos_autocorr_lags = station_met_outflow_60min_df.apply(calculate_positive_peak_autocorrelation_of_demand, axis=0)\n","    return daily_pos_autocorr_lags.values.T\n","\n","def demands_negative_autocorr_lags(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_neg_autocorr_lags = station_met_outflow_60min_df.apply(calculate_negative_peak_autocorrelation_of_demand, axis=0)\n","    return daily_neg_autocorr_lags.values.T\n","\n","def demands_positive_partial_autocorr_lags(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_pos_partial_autocorr_lags = station_met_outflow_60min_df.apply(calculate_positive_peak_partial_autocorrelation_of_demand, axis=0)\n","    return daily_pos_partial_autocorr_lags.values.T\n","\n","def demands_negative_partial_autocorr_lags(station_met_outflow_60min_df):\n","    station_met_outflow_60min_df['Date'] = station_met_outflow_60min_df.index\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour'])\n","    daily_neg_partial_autocorr_lags = station_met_outflow_60min_df.apply(calculate_negative_peak_partial_autocorrelation_of_demand, axis=0)\n","    return daily_neg_partial_autocorr_lags.values.T\n","\n","\n","\n","def fft_of_demand(station_met_outflow_60min_df):\n","    #station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week','weekday','hour','Date'])\n","    station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week', 'weekday', 'hour','Date'])\n","    stations = station_met_outflow_60min_df.columns\n","    num_common_freq=180\n","    top_x_frequencies = 11\n","    freq_rounding_num = 6\n","    common_freq_set = set()\n","    X_train = np.empty(shape=(0,top_x_frequencies-1)) # removed dc\n","\n","    for station in stations:\n","        # Perform the FFT on the data\n","        fft_result = np.fft.fft(station_met_outflow_60min_df[station].values)\n","        # Compute the magnitudes of the FFT result\n","        fft_magnitudes = np.abs(fft_result)\n","        # Generate the corresponding frequency axis\n","        sampling_rate = 1.0  # Assuming a sampling rate of 1.0 (change as needed)\n","        frequency_axis = np.fft.fftfreq(len(fft_result), 1 / sampling_rate)\n","        positive_freq_indices = np.where(frequency_axis >= 0)\n","\n","        \"\"\" TOP periods/frequencies in the time series\"\"\"\n","        # Sort the magnitudes and frequencies in descending order\n","        sorted_indices = np.argsort(fft_magnitudes[positive_freq_indices])[::-1]\n","        sorted_magnitudes = fft_magnitudes[positive_freq_indices][sorted_indices]\n","        sorted_frequencies = frequency_axis[positive_freq_indices][sorted_indices]\n","        # Select the top periods\n","\n","        top_freq = sorted_frequencies[1:top_x_frequencies] # REMOVE dc/mean component\n","        top_periods = 1/top_freq\n","        top_periods = [round(p) for p in top_periods]\n","        #print(f'Top FFT periods in demand are : {top_periods}')\n","        X_train = np.vstack((X_train,top_periods))\n","    return X_train\n","\n","\n","\n"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WPkGutXdyQ72AceCbSJsq4ISp2P-hmXl"},"executionInfo":{"elapsed":485921,"status":"error","timestamp":1690098806919,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"vjDl0_mv4P9q","outputId":"d7457261-1346-4526-8a40-eae517d6441b"},"outputs":[{"data":{"text/plain":["(382, 10)"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"                                                                     FEATURE Extraction\n","Feature extracted: Interpretable Time series statistical features\n","\n","1) High/Low average demand for each day of 7 days\n","\n","\n","\"\"\"\n","\"\"\"\n","every_days_avg_dem = average_demand(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_avg_dem = every_days_avg_dem[:,:4] # excludes friday\n","weekends_avg_dem = every_days_avg_dem[:,4:] # includes friday\n","\n","every_days_n95_dem = peak_n95_demand(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_n95_dem = every_days_n95_dem[:,:4] # excludes friday\n","weekends_n95_dem = every_days_n95_dem[:,4:] # includes friday\n","\n","\n","every_days_max_dem = max_demand(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_max_dem = every_days_max_dem[:,:4] # excludes friday\n","weekends_max_dem = every_days_max_dem[:,4:] # includes friday\n","\n","\n","no_mean_cross_Dem = no_mean_crossing_demand(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_nmc_dem = no_mean_cross_Dem[:,:4] # excludes friday\n","weekends_nmc_dem = no_mean_cross_Dem[:,4:] # includes friday\n","\n","\n","max_deviation_from_n95 = outlier_demands(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_max_dev_n95_dem = max_deviation_from_n95[:,:4] # excludes friday\n","weekends_max_dev_n95_dem = max_deviation_from_n95[:,4:] # includes friday\n","\n","\n","every_days_std_dem = std_demand(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_std_dem = every_days_std_dem[:,:4] # excludes friday\n","weekends_std_dem = every_days_std_dem[:,4:] # includes friday\n","\n","\n","every_days_zero_dem = count_zero_demands(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_zero_dem = every_days_zero_dem[:,:4] # excludes friday\n","weekends_zero_dem = every_days_zero_dem[:,4:] # includes friday\n","\n","\n","every_days_kurt = kurt_of_demands(station_met_outflow_60min_df)\n","# ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","# print_msg = 'every_days_avg_dem'\n","# plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n","weekdays_kurt = every_days_kurt[:,:4] # excludes friday\n","weekends_kurt = every_days_kurt[:,4:] # includes friday\n","\"\"\"\n","\n","# every_days_pos_autocorr_lags = demands_positive_autocorr_lags(station_met_outflow_60min_df)\n","# every_days_neg_autocorr_lags = demands_negative_autocorr_lags(station_met_outflow_60min_df)\n","# every_days_pos_partial_autocorr_lags = demands_positive_partial_autocorr_lags(station_met_outflow_60min_df)\n","# every_days_neg_partial_autocorr_lags = demands_negative_partial_autocorr_lags(station_met_outflow_60min_df)\n","\n","\n","peak_fft_periods = fft_of_demand(station_met_outflow_60min_df)\n","peak_fft_periods.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpffsGLCUwqj"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","\n","Feature extracted: Time series statistical features\n","\n","1) Total demand, Mean, mode, median, upper & Lower whisker, max, min, std, kurtosis, skewness,\n","2)variance, standard deviation, Mean\n","3) Median, 25th percentile value, 75th percentile value\n","4) Root Mean Square value; square of the average of the squared amplitude values\n","5) The mean of the derivative\n","6) Zero crossing rate, i.e. the number of times a signal crosses y = 0\n","7) Mean crossing rate, i.e. the number of times a signal crosses y = mean(y)\n","\n","\n","\"\"\"\n","plt.close('all')\n","\n","def calculate_entropy(list_values):\n","    counter_values = Counter(list_values).most_common()\n","    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n","    entropy= stats.entropy(probabilities)\n","    return [entropy]\n","\n","def calculate_crossings(list_values):\n","    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) >= np.nanmean(list_values)))[0]\n","    no_mean_crossings = len(mean_crossing_indices)\n","    return [no_mean_crossings]\n","\n","def calculate_statistics(list_values):\n","    #n5 = np.nanpercentile(list_values, 5) # only zeros\n","    #n25 = np.nanpercentile(list_values, 25) # only zeros\n","    #n75 = np.nanpercentile(list_values, 75) # only zeros\n","    n95 = np.nanpercentile(list_values, 95)\n","    #median = np.nanpercentile(list_values, 50) # only zeros\n","    #mean = np.nanmean(list_values) # too correlated with others, help red dim.\n","    #std = np.nanstd(list_values) # too correlated with var, help red dim.\n","    var = np.nanvar(list_values)\n","    rms = np.nanmean(np.sqrt(list_values**2))\n","    #total_dem = np.nansum(list_values) # too correlated with others, help red dim.\n","    #mode = stats.mode(list_values,keepdims=True)[0][0] # only zeros\n","    #w = n25 - 1.5*stats.iqr(list_values) # only zeros\n","    #low_whisk = 0 if 0 >= w else w # only zeros\n","    #up_whisk = n75 + 1.5*stats.iqr(list_values) # only zeros\n","    kurt = stats.kurtosis(list_values)\n","    #skew = stats.skew(list_values) # too correlated with kurt, help red dim.\n","    max = np.nanmax(list_values)\n","    #min = np.nanmin(list_values) # only zeros\n","    #return [n5, n25, n75, n95, median, mean, std, var, rms, total_dem, mode, up_whisk, low_whisk, kurt, skew,max]#, min]\n","    return [n95,rms, var, kurt,max]\n","\n","def get_features(list_values):\n","    entropy = calculate_entropy(list_values)\n","    crossings = calculate_crossings(list_values)\n","    statistics = calculate_statistics(list_values)\n","    return entropy + crossings + statistics\n","\n","\n","stations = unmet_60min_outflow_df.columns\n","X_train = np.empty(shape=(0,7))\n","\n","\n","for station in stations:\n","    # Perform the FFT on the data\n","    list_values = unmet_60min_outflow_df[station].values\n","    X_train = np.vstack((X_train, np.array(get_features(list_values)) ) )\n","\n","ts_stats_feat_transform = feat_transformation()\n","X_train_normalized = ts_stats_feat_transform.feature_normalization(X_train)\n","\n","\n","print_msg = 'Summary statistics'\n","######### histogram plot #########\n","# check to see if any features are all zeros\n","######### plot correlation #########\n","# perform dimensionality reduction by removing highly correlated features.\n","plot_all(print_msg).plot_hist_n_corr(X_train_normalized)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zmn11cArsXES"},"outputs":[],"source":["\"\"\"                                                                     PLOT UMAP for Time series statistical features\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","#fig, ax_tsne_2d = plt.subplots(ncols=2,figsize=(25,7))\n","# fig_umap_2d = sp.make_subplots(rows=1, cols=1)\n","# fig_umap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","#fig_isomap_2d = sp.make_subplots(rows=1, cols=1)\n","#fig_isomap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","\n","print_msg = 'Summary statistics'\n","plotter = plot_all(print_msg)\n","# plotter.plot_tSNE_2d(X_train_normalized,print_msg,ax_tsne_2d,0)\n","# fig.show()\n","\n","# fig_umap_2d.add_trace(plotter.plot_UMAP_2d(X_train_normalized,print_msg),row=1,col=1)\n","# fig_umap_3d.add_trace(plotter.plot_UMAP_3d(X_train_normalized))\n","\n","#fig_isomap_2d.add_trace(plotter.plot_ISOMAP_2d(X_train_normalized,print_msg),row=1,col=1)\n","#fig_isomap_3d.add_trace(plotter.plot_ISOMAP_3d(X_train_normalized,print_msg))\n","\n","# fig_umap_2d.update_layout(height=900 , width=900,title='UMAP 2d,')\n","# fig_umap_2d.show()\n","\n","# fig_umap_3d.update_layout(height=900 , width=900,title='UMAP 3d,')\n","# fig_umap_3d.show()\n","\n","interactive_UMAP_plot=interactive(plotter.plot_UMAP_interactive,X_train=X_train_normalized,print_msg=print_msg,n_neighbors=(10,100,20),min_dist=(0,1.0,0.1),metric=['euclidean','mahalanobis','manhattan'])\n","display(interactive_UMAP_plot)\n","\n","# interactive_umap_2d_plot=interactive(plotter.plot_UMAP_2d_interactive,X_train=X_train_normalized,print_msg=print_msg,n_neighbors=(10,100,10),min_dist=(0,1.0,0.1),metric=['euclidean','mahalanobis'])\n","# display(interactive_umap_2d_plot)\n","\n","\n","# fig_isomap_2d.update_layout(height=600 , width=600,title='ISOMAP 2d,')\n","# fig_isomap_2d.show()\n","\n","#fig_isomap_3d.update_layout(height=600 , width=3000,title='ISOMAP 3d,')\n","# fig_isomap_3d.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUgNWdwryEEt"},"outputs":[],"source":["\"\"\"                                                                    ### DETAILED 3d UMAP for Time series statistical features\n","\n","use above to get correct n_neighbors ,min_dist and metric to visuzalize in detain\n","\"\"\"\n","plt.close('all')\n","\n","fig_umap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","\n","fig_umap_3d.add_trace(plotter.plot_UMAP_3d(X_train_normalized,10,0.0,'manhattan'))\n","\n","fig_umap_3d.update_layout(height=900 , width=900,title='UMAP 3d,'+print_msg)\n","fig_umap_3d.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LB_OveK4MSQN"},"outputs":[],"source":["\"\"\"\n","Parameter search for best clustering using TS statistical Features\n","\n","\"\"\"\n","\n","best_score = 0\n","best_score_num_cluster_dict = {}\n","cov_mat = np.cov(X_train_normalized.T)\n","\n","for metric in ['euclidean','manhattan','mahalanobis']:# need to implement mahalanobis distance\n","    if metric != 'mahalanobis':\n","        for min_cluster_size in range(5,80,5):\n","            for min_samples in range(1,40,2):\n","                for cluster_selection_method in ['eom','leaf']:\n","                    # for each combination of parameters of hdbscan\n","                    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n","                                          cluster_selection_method=cluster_selection_method, metric=metric,\n","                                          gen_min_span_tree=True).fit(X_train_normalized)\n","                    # DBCV score\n","                    score = hdb.relative_validity_\n","                    outliers = (np.array(hdb.labels_)==-1).sum()\n","                    best_score_num_cluster_dict[score] = [len(np.unique(hdb.labels_)),min_cluster_size,min_samples,cluster_selection_method,metric,outliers]\n","                    #print(f'score: {score}, num of unique clusters: {len(np.unique(hdb.labels_))}' )\n","                    # if we got a better DBCV, store it and the parameters\n","                    if score > best_score:\n","                        best_score = score\n","                        best_parameters = {'min_cluster_size': min_cluster_size,\n","                                  ' min_samples':  min_samples, 'cluster_selection_method': cluster_selection_method,\n","                                  'metric': metric}\n","    else: ## mahalanobis distance\n","        for min_cluster_size in range(5,80,5):\n","            for min_samples in range(1,40,2):\n","                for cluster_selection_method in ['eom','leaf']:\n","                    # for each combination of parameters of hdbscan\n","                    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n","                                          cluster_selection_method=cluster_selection_method, metric=metric,V=cov_mat,\n","                                          gen_min_span_tree=True).fit(X_train_normalized)\n","                    # DBCV score\n","                    score = hdb.relative_validity_\n","                    outliers = (np.array(hdb.labels_)==-1).sum()\n","                    best_score_num_cluster_dict[score] = [len(np.unique(hdb.labels_)),min_cluster_size,min_samples,cluster_selection_method,metric,outliers]\n","                    #print(f'score: {score}, num of unique clusters: {len(np.unique(hdb.labels_))}' )\n","                    # if we got a better DBCV, store it and the parameters\n","                    if score > best_score:\n","                        best_score = score\n","                        best_parameters = {'min_cluster_size': min_cluster_size,\n","                                  ' min_samples':  min_samples, 'cluster_selection_method': cluster_selection_method,\n","                                  'metric': metric}\n","\n","\n","print(\"Best DBCV score: {:.3f}\".format(best_score))\n","print(\"Best parameters: {}\".format(best_parameters))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNlR2_lU3Rm6"},"outputs":[],"source":["sorted(best_score_num_cluster_dict.items())[-40:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUlsuWfYG-1g"},"outputs":[],"source":["\"\"\"\n","Plot Minimum spanning tree and dbscan cluster hierarchy from TS statistical features\n","\"\"\"\n","\n","plt.close('all')\n","\n","clusterer = hdbscan.HDBSCAN(min_cluster_size= 5, min_samples= 1,cluster_selection_method='eom', metric='euclidean', gen_min_span_tree=True)\n","clusterer.fit(X_train_normalized)\n","\n","fig = plt.figure(figsize=(10,5))\n","# Plotting the minimum spanning tree in the first subplot\n","clusterer.minimum_spanning_tree_.plot(\n","    edge_cmap='viridis',\n","    edge_alpha=0.6,\n","    edge_linewidth=2\n",")\n","fig.show()\n","\n","fig = plt.figure(figsize=(20,10))\n","# Plotting the condensed tree in the second subplot\n","clusterer.condensed_tree_.plot(\n","    select_clusters=True,\n","    selection_palette=sns.color_palette(),\n",")\n","fig.show()\n","\n","print(clusterer.labels_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT6ghWs_c5Dj"},"outputs":[],"source":["\"\"\"\n","Visualize Time series statistical features of all clusters using parcoords plot\n","\n","\"\"\"\n","\n","plt.close('all')\n","plot_title = 'TS statistical features'\n","mother = 'rbio2.2'\n","limit_level = 2\n","angular_labels = ['entropy','no_mean_crossing','n95','rms', 'var', 'kurt','max']\n","\n","X_train = ts_stats_feat_transform.inv_feature_normalization(X_train_normalized)\n","\n","all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(clusterer.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_normalized,'par_coords',angular_labels,plot_title,mother,limit_level)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zck1N7t49fgn"},"outputs":[],"source":["\"\"\"\n","Visualize cluster representatives of Time series statistical features using parcoords plot\n","\n","\"\"\"\n","\n","plot_cluster_representatives(X_train_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',angular_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87SnidR54qRO"},"outputs":[],"source":["\"\"\"\n","## Analysis of statistical features from parcoords plots\n","\n","\"\"\"\n","plot_cluster_representatives(X_train, all_cluster_feat_mins, all_cluster_feat_maxs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfsvg5KcV5wg"},"outputs":[],"source":["\"\"\"\n","Plot all the wavelets to identify suitable mother wavelet for our demand time series\n","\"\"\"\n","\n","\"\"\"\n","def plot_wav_families(discrete_wavelets):\n","    fig,ax = plt.subplots(nrows=1,ncols=len(discrete_wavelets),figsize=(25,3))\n","    for col_no, waveletname in enumerate(discrete_wavelets):\n","        wavelet = pywt.Wavelet(waveletname)\n","        family_name = wavelet.family_name\n","        biorthogonal = wavelet.biorthogonal\n","        orthogonal = wavelet.orthogonal\n","        symmetry = wavelet.symmetry\n","\n","        _ = wavelet.wavefun()\n","        wavelet_function = _[0]\n","        x_values = _[-1]\n","\n","        ax[col_no].set_title(\"{}\".format(waveletname), fontsize=16)\n","        ax[col_no].plot(x_values, wavelet_function)\n","\n","    plt.show()\n","    return\n","\n","for i in range(11):\n","  discrete_wavelets = pywt.wavelist(kind='discrete')[i*10:i*10 + 10]\n","  plot_wav_families(discrete_wavelets)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFxnIrLk1dRw"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","\n","Visualize detail coefficients in inverse Discrete Wavelet Transform\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","def reconstruction_plot(yyy, **kwargs):\n","    ###\"\"\"Plot signal vector on x [0,1] independently of amount of values it contains.\"\"\"\n","    #plt.figure()\n","    #plt.plot(np.linspace(0, 1, len(yyy)), yyy, **kwargs)\n","    ym = np.median(yyy)\n","    plt.plot(np.linspace(0, 1., num=len(yyy)), yyy-ym, **kwargs)\n","\n","\n","def reconstruction_stem(yyy, xmax, **kwargs):\n","    ###\"\"\"Plot coefficient vector on x [0,1] independently of amount of values it contains.\"\"\"\n","    ymax = yyy.max()\n","    plt.stem(np.linspace(0, 1., num=len(yyy)), yyy*(xmax/ymax), **kwargs)\n","\n","\n","### original ecg data\n","### x = pywt.data.ecg()\n","### w = pywt.Wavelet('sym5')\n","x = unmet_60min_outflow_df['5'].values\n","w = pywt.Wavelet('coif1')\n","nl = 8\n","coeffs = pywt.wavedec(x, w, level=nl)\n","\n","\n","'''\n","plt.figure()\n","plt.stem(coeffs[1]); plt.legend(['Lvl 6 detail coefficients'])\n","plt.figure()\n","plt.stem(coeffs[2]); plt.legend(['Lvl 5 detail coefficients'])\n","plt.figure()\n","plt.stem(coeffs[3]); plt.legend(['Lvl 4 detail coefficients'])\n","plt.figure()\n","plt.stem(coeffs[4]); plt.legend(['Lvl 3 detail coefficients'])\n","plt.figure()\n","plt.stem(coeffs[5]); plt.legend(['Lvl 2 detail coefficients'])\n","plt.figure()\n","plt.stem(coeffs[6]); plt.legend(['Lvl 1 detail coefficients'])\n","'''\n","\n","\n","xmax = x.max()\n","for i in range(nl):\n","    plt.figure(figsize=(40,7))\n","    reconstruction_plot(x) # original signal\n","    #reconstruction_plot(pywt.waverec(coeffs, w)) # full reconstruction\n","    reconstruction_plot(pywt.waverec(coeffs[:i+2] + [None] * (nl-i-1), w)) # partial reconstruction\n","    reconstruction_stem(coeffs[i+1], xmax, markerfmt ='none', linefmt='r-')\n","    #plt.legend(['Original', 'Full reconstruction', ('Rec to lvl %d')%(nl-i), ('Details for lvl %d')%(nl-i)])\n","    plt.legend(['Original', ('Rec to lvl %d')%(nl-i), ('Details for lvl %d')%(nl-i)])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4dybSjWgF2v"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","\n","Feature extracted: Discrete Wavelet Transform after selecting some mother wavelets\n","\n","\n","\"\"\"\n","plt.close('all')\n","\n","def features_info(unmet_60min_outflow_df,wave_name,limit_level):\n","    list_coeff = pywt.wavedec(unmet_60min_outflow_df['5'].values, wave_name)\n","    print('wave_name: ',wave_name)\n","    num_feats = 0\n","    for l,coeff in enumerate(list_coeff):\n","        lvl = max(0,l-1)\n","        print('level: ',lvl,' num_of_coeffs: ',len(coeff))\n","        if lvl <= limit_level:\n","            num_feats+=len(coeff)\n","    return num_feats\n","\n","def get_stations_dwt_features(signal, waveletname,num_feats,limit_level):\n","    stations = signal.columns\n","    all_features = np.empty(shape=(0,num_feats))\n","    for station in stations:\n","        list_coeff = pywt.wavedec(signal[station].values, waveletname)\n","        features = np.hstack(list_coeff[:limit_level+2])\n","        features = features.reshape((1,num_feats))\n","        all_features = np.vstack((all_features,features))\n","    print('X_train shape: ',all_features.shape)\n","    return all_features\n","\n","\n","## shortlist mothers that have wavelets similar to portions of demand TS\n","## mothers = ['bior2.2','bior4.4','bior5.5','coif1','coif2','db3','db22','rbio2.2','rbio2.6','sym3','sym8']\n","## ONLY select mothers with higher multi-frequency resolution. i.e. higher depth/levels have better time localizations of frequnecy\n","mothers = ['bior2.2','coif1','db3','rbio2.2','sym3']\n","limit_level = 1 # includes this level\n","\n","for mother in mothers:\n","    num_feats = features_info(unmet_60min_outflow_df,mother,limit_level)  # display number of feats\n","    X_train = get_stations_dwt_features(signal=unmet_60min_outflow_df, waveletname=mother,num_feats=num_feats,limit_level=limit_level) #choose wavelets and levels < 9, #output = 382 x 180 features\n","\n","    ts_dwt_feat_transform = feat_transformation()\n","    X_train_dwt_normalized = ts_dwt_feat_transform.feature_normalization(X_train)\n","\n","    print_msg = 'wavelet :'+mother\n","\n","    break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElIhaHIMOMNp"},"outputs":[],"source":["\"\"\"\n","Identify aggolmeratic clustering parameter combination with best silhouette score for DWT features\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","mother = 'rbior'\n","limit_level = 2 # [0,1,2,3,4]\n","\n","\n","num_feats = features_info(unmet_60min_outflow_df,mother,limit_level)  # display number of feats\n","X_train = get_stations_dwt_features(signal=unmet_60min_outflow_df, waveletname=mother,num_feats=num_feats,limit_level=limit_level) #choose wavelets and levels < 9, #output = 382 x 180 features\n","\n","ts_dwt_feat_transform = feat_transformation()\n","X_train_dwt_normalized = ts_dwt_feat_transform.feature_normalization(X_train)\n","\n","num_clusters_list = range(2,35,1)\n","\n","\n","methods = ['single','complete','average']#['single','complete','average','weighted',]\n","metrics = ['euclidean','cityblock']\n","for metric in metrics:\n","    for method in methods:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_dwt_normalized)\n","            labels = model.labels_\n","            plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)\n","\n","\n","methods = ['ward']#['median','centroid','ward']\n","metrics = ['euclidean']\n","for method in methods:\n","    for metric in metrics:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_dwt_normalized)\n","            labels = model.labels_\n","            plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)\n","\n","\n","methods = ['complete','average']#['single','complete','average','weighted',]\n","metrics = ['mahalanobis','manhattan']\n","for metric in metrics:\n","    for method in methods:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_dwt_normalized)\n","            labels = model.labels_\n","            plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiexVNRwQx7G"},"outputs":[],"source":["\"\"\"\n","Identify kmeans clustering parameter combination with best silhouette score for DWT features\n","\n","\"\"\"\n","\n","### xxxx POOR performance xxxx ###\n","\n","metrics = ['euclidean']\n","methods = ['single','complete','average','ward']\n","\n","for num_cluster in range(2,35,2):\n","    for metric in metrics:\n","        for method in methods:\n","            km = TimeSeriesKMeans(n_clusters=num_cluster, metric=metric)\n","            km.fit(X_train_dwt_normalized)\n","            plot_silhouette_scores(X_train_dwt_normalized,km.labels_,metric,method)\n","            #plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyIiykeyUuyx"},"outputs":[],"source":["\"\"\"\n","Identify k-medoids clustering parameter combination with best silhouette score for DWT features\n","\n","\"\"\"\n","\n","### xxxx POOR performance xxxx ###\n","\n","mahalanobis_dist = compute_mahalanobis_dist_mat(X_train_dwt_normalized)\n","\n","for k in range(2,35,2):\n","    for method in methods:\n","        kmedoids = KMedoids(n_clusters=k, metric='precomputed', init='random', method=\"alternate\").fit(mahalanobis_dist)\n","        plot_silhouette_scores(X_train_dwt_normalized,kmedoids.labels_,metric,method)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_V10hmnzjgH"},"outputs":[],"source":["\"\"\"\n","SHORTLISTED aggolmeratic clustering parameters combination for DWT features\n","\n","\"\"\"\n","\n","plt.close('all')\n","limit_level = 2\n","\n","mother = 'rbio2.2'\n","num_feats = features_info(unmet_60min_outflow_df,mother,limit_level)  # display number of feats\n","X_train = get_stations_dwt_features(signal=unmet_60min_outflow_df, waveletname=mother,num_feats=num_feats,limit_level=limit_level) #choose wavelets and levels < 9, #output = 382 x 180 features\n","\n","ts_dwt_feat_transform = feat_transformation()\n","X_train_dwt_normalized = ts_dwt_feat_transform.feature_normalization(X_train)\n","\n","num_clusters_list = [21]\n","\n","methods = ['complete']#['single','complete','average','weighted',]\n","metrics = ['cityblock']\n","for metric,method,num_clusters in zip(metrics,methods,num_clusters_list):\n","    print('\\n',metric,method,num_clusters)\n","    plot_title = 'DWT features, '+ 'limit level: ' + str(limit_level) +', metric: '+metric+', method: '+', num_clusters: '+str(num_clusters)+'. '\n","    model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","    model.fit(X_train_dwt_normalized)\n","    labels = model.labels_\n","    #plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)\n","    #all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'polar')\n","    #plot_cluster_representatives(X_train, all_cluster_feat_mins, all_cluster_feat_maxs,'polar')\n","    all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'par_coords',range(X_train_dwt_normalized.shape[1]),plot_title,mother,limit_level)\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',range(X_train_dwt_normalized.shape[1]))\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'polar',range(X_train_dwt_normalized.shape[1]))\n","\n","\n","\n","\"\"\"\n","mother = 'rbio2.2'\n","num_feats = features_info(unmet_60min_outflow_df,mother,limit_level)  # display number of feats\n","X_train = get_stations_dwt_features(signal=unmet_60min_outflow_df, waveletname=mother,num_feats=num_feats,limit_level=limit_level) #choose wavelets and levels < 9, #output = 382 x 180 features\n","\n","ts_dwt_feat_transform = feat_transformation()\n","X_train_dwt_normalized = ts_dwt_feat_transform.feature_normalization(X_train)\n","\n","\n","num_clusters_list = [23,10,8]\n","methods = ['complete','complete','complete']#['single','complete','average','weighted',]\n","metrics = ['cityblock',,'cityblock','cityblock']\n","for metric,method,num_clusters in zip(metrics,methods,num_clusters_list):\n","    print('\\n',metric,method,num_clusters)\n","    plot_title = 'DWT features, '+ 'limit level: ' + str(limit_level) +', metric: '+metric+', method: '+', num_clusters: '+str(num_clusters)+'. '\n","    model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","    model.fit(X_train_dwt_normalized)\n","    labels = model.labels_\n","    #plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)\n","    #all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'polar')\n","    #plot_cluster_representatives(X_train, all_cluster_feat_mins, all_cluster_feat_maxs,'polar')\n","    all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'par_coords',range(X_train_dwt_normalized.shape[1]),plot_title)\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',range(X_train_dwt_normalized.shape[1]))\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'polar',range(X_train_dwt_normalized.shape[1]))\n","\n","\n","\n","num_clusters_list = [13,24]\n","methods = ['complete','average']#['single','complete','average','weighted',]\n","metrics = ['euclidean','cityblock']\n","for metric,method,num_clusters in zip(metrics,methods,num_clusters_list):\n","    print('\\n',metric,method,num_clusters)\n","    plot_title = 'DWT features, '+ 'limit level: ' + str(limit_level) +', metric: '+metric+', method: '+', num_clusters: '+str(num_clusters)+'. '\n","    model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","    model.fit(X_train_dwt_normalized)\n","    labels = model.labels_\n","    #plot_silhouette_scores(X_train_dwt_normalized,labels,metric,method)\n","    #all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'polar')\n","    #plot_cluster_representatives(X_train, all_cluster_feat_mins, all_cluster_feat_maxs,'polar')\n","    all_cluster_feat_mins, all_cluster_feat_maxs = plot_stations_of_clusters(model.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand',X_train_dwt_normalized,'par_coords',range(X_train_dwt_normalized.shape[1]),plot_title)\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',range(X_train_dwt_normalized.shape[1]))\n","    #plot_cluster_representatives(X_train_dwt_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'polar',range(X_train_dwt_normalized.shape[1]))\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSQVP-fW5M77"},"outputs":[],"source":["\"\"\"                                                                     PLOT UMAP for Discrete Wavelet Transform features\n","\n","\"\"\"\n","plt.close('all')\n","mother = 'bior2.2'\n","\n","num_feats = features_info(unmet_60min_outflow_df,mother,limit_level)  # display number of feats\n","X_train = get_stations_dwt_features(signal=unmet_60min_outflow_df, waveletname=mother,num_feats=num_feats,limit_level=limit_level) #choose wavelets and levels < 9, #output = 382 x 180 features\n","\n","ts_dwt_feat_transform = feat_transformation()\n","X_train_dwt_normalized = ts_dwt_feat_transform.feature_normalization(X_train)\n","\n","\n","plt.close('all')\n","plotter_dwt = plot_all(print_msg)\n","interactive_UMAP_plot=interactive(plotter_dwt.plot_UMAP_interactive,X_train=X_train_dwt_normalized,print_msg=print_msg,n_neighbors=(10,100,20),min_dist=(0,1.0,0.1),metric=['euclidean','mahalanobis','manhattan'])\n","display(interactive_UMAP_plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhKDBFL0glTf"},"outputs":[],"source":["\"\"\"                                                                    ### DETAILED 3d UMAP for Discrete wavelet features\n","\n","use above to get correct n_neighbors ,min_dist and metric to visuzalize in detain\n","\"\"\"\n","plt.close('all')\n","\n","fig_umap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","\n","fig_umap_3d.add_trace(plotter_dwt.plot_UMAP_3d(X_train_dwt_normalized,30,0.1,'manhattan'))\n","\n","fig_umap_3d.update_layout(height=900 , width=900,title='UMAP 3d,'+print_msg)\n","fig_umap_3d.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G22ylR-3O3-m"},"outputs":[],"source":["plot_cluster_representatives(X_train, all_cluster_feat_mins, all_cluster_feat_maxs,'polar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VI2UX21LkCb_"},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","# mahalanobis_dist_mat = compute_mahalanobis_dist_mat(X_train_dwt_normalized)\n","# agg = AgglomerativeClustering(n_clusters=25, metric='precomputed', linkage = 'complete')\n","# agg.fit_predict(mahalanobis_dist_mat)  # Returns class labels.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1b3J_J-kCW1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQzwsTfHkCP9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fR_yMoFpi-We"},"outputs":[],"source":["\"\"\"                                 Distance Matrix calculation\n","Distance Measure: Pearson Correlation and Spearman's correlation\n","\n","\"\"\"\n","\n","\"\"\"\n","stations = unmet_60min_outflow_df.columns\n","unique_station_pairs = list(combinations(stations, 2))\n","\n","## station to index map\n","station_to_index_map = {}\n","i=0\n","for s in stations:\n","    station_to_index_map[s] = i\n","    i+=1\n","\n","## index to station map\n","index_to_station_map = {}\n","i=0\n","for s in stations:\n","    index_to_station_map[i] = s\n","    i+=1\n","\n","## create indices of 2d distance matrix\n","arr_dim = len(stations)\n","index = []\n","rows = 1\n","for c in np.arange(arr_dim-1):\n","  for r in np.arange(rows,arr_dim):\n","    index.append((r,c))\n","  rows = rows+1\n","\n","pearson_distance_matrix = pd.DataFrame(columns=unique_station_pairs,index=range(1))\n","pearr_arr = np.array([])\n","pearr_arr_2d = np.zeros(shape=(len(stations),len(stations)))\n","i=0\n","for pair in unique_station_pairs:\n","    r, p = stats.pearsonr(unmet_60min_outflow_df[pair[0]].values, unmet_60min_outflow_df[pair[1]].values)\n","    pearr_arr = np.append(pearr_arr,r)\n","    pearr_arr_2d[index[i]] = r\n","    i+=1\n","pearson_distance_matrix.loc[0] = pearr_arr\n","pearr_arr_2d = pearr_arr_2d + pearr_arr_2d.T # to make it symmetric\n","pearr_arr_2d = 1 - pearr_arr_2d # convert disimilarity to similarity for all clusterings\n","np.fill_diagonal(pearr_arr_2d, 0)\n","\n","\n","spearmans_distance_matrix = pd.DataFrame(columns=unique_station_pairs,index=range(1))\n","spearr_arr = np.array([])\n","spearr_arr_2d = np.zeros(shape=(len(stations),len(stations)))\n","i=0\n","for pair in unique_station_pairs:\n","    r, p = stats.spearmanr(unmet_60min_outflow_df[pair[0]].values, unmet_60min_outflow_df[pair[1]].values)\n","    spearr_arr = np.append(spearr_arr,r)\n","    spearr_arr_2d[index[i]] = r\n","    i+=1\n","spearmans_distance_matrix.loc[0] = spearr_arr\n","spearr_arr_2d = spearr_arr_2d + spearr_arr_2d.T # to make it symmetric\n","spearr_arr_2d = 1 - spearr_arr_2d # convert disimilarity to similarity for all clusterings\n","np.fill_diagonal(spearr_arr_2d, 0)\n","\n","pd.DataFrame(pearr_arr_2d).to_csv('pearson_distance_matrix_2d.csv',index=False)\n","pd.DataFrame(spearr_arr_2d).to_csv('spearman_distance_matrix_2d.csv',index=False)\n","pearson_distance_matrix.to_csv('pearson_distance_matrix.csv',index=False)\n","spearmans_distance_matrix.to_csv('spearmans_distance_matrix.csv',index=False)\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2AcUSHV8gAg"},"outputs":[],"source":["pearr_arr_2d = pd.read_csv('pearson_distance_matrix_2d.csv')\n","pearr_arr_2d = pearr_arr_2d.to_numpy()\n","\n","spearr_arr_2d = pd.read_csv('spearman_distance_matrix_2d.csv')\n","spearr_arr_2d = spearr_arr_2d.to_numpy()\n","\n","#pearson_distance_matrix = pd.read_csv('pearson_distance_matrix.csv')\n","pearson_distance_vector = pd.read_csv('pearson_distance_matrix.csv')\n","\n","\n","#spearmans_distance_matrix = pd.read_csv('spearmans_distance_matrix.csv')\n","spearmans_distance_vector = pd.read_csv('spearmans_distance_matrix.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FprxZhbNwCT4"},"outputs":[],"source":["\"\"\"                                                      Naive Analysis of pearson and spearman distance\n","\n","Can we just rank the distance matrix and group the largest positive/negative and smallest correlation ???\n","\"\"\"\n","\n","\"\"\"\n","plt.hist(spearmans_distance_matrix.loc[0].values,bins=100000)\n","plt.title('histogram')\n","plt.xlabel('spearmans_corr_dist_matrix')\n","plt.show()\n","plt.hist(pearson_distance_matrix.loc[0].values,bins=100000)\n","plt.title('histogram')\n","plt.xlabel('pearson_corr_dist_matrix')\n","plt.show()\n","\"\"\"\n","\n","pearson_distance_vector = pd.read_csv('pearson_distance_matrix.csv')\n","pearson_df = pd.DataFrame({'pearson':pearson_distance_vector.loc[0].values})\n","fig = px.histogram(pearson_df, x=\"pearson\",nbins=100000)\n","fig.show()\n","\n","spearmans_distance_vector = pd.read_csv('spearmans_distance_matrix.csv')\n","spearman_df = pd.DataFrame({'spearman':spearmans_distance_vector.loc[0].values})\n","fig = px.histogram(spearman_df, x=\"spearman\",nbins=100000)\n","fig.show()\n","\n","\n","# image = mpimg.imread('pearsons_hist_number_of_cluster.PNG')\n","# plt.imshow(image)\n","# plt.show()\n","\n","# image = mpimg.imread('spearmans_hist_number_of_cluster.PNG')\n","# plt.imshow(image)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvaSEk61y4OT"},"outputs":[],"source":["\"\"\"                                                               Distance Matrix calculation\n","Distance Measure: Dynamic Time Warping\n","\n","\"\"\"\n","\n","\"\"\"\n","stations = unmet_60min_outflow_df.columns\n","unique_station_pairs = list(combinations(stations, 2))\n","\n","## station to index map\n","station_to_index_map = {}\n","i=0\n","for s in stations:\n","    station_to_index_map[s] = i\n","    i+=1\n","\n","## index to station map\n","index_to_station_map = {}\n","i=0\n","for s in stations:\n","    index_to_station_map[i] = s\n","    i+=1\n","\n","## create indices of 2d distance matrix\n","arr_dim = len(stations)\n","index = []\n","rows = 1\n","for c in np.arange(arr_dim-1):\n","  for r in np.arange(rows,arr_dim):\n","    index.append((r,c))\n","  rows = rows+1\n","\n","dtw_distance_matrix = pd.DataFrame(columns=unique_station_pairs,index=range(1))\n","dtw_arr = np.array([])\n","dtw_arr_2d = z = np.zeros(shape=(len(stations),len(stations)))\n","i=0\n","for pair in unique_station_pairs:\n","    dtw_dist = dtw(unmet_60min_outflow_df[pair[0]].values, unmet_60min_outflow_df[pair[1]].values)\n","    dtw_arr = np.append(dtw_arr,dtw_dist)\n","    dtw_arr_2d[index[i]] = dtw_dist\n","    i+=1\n","dtw_distance_matrix.loc[0] = dtw_arr\n","dtw_arr_2d = dtw_arr_2d + dtw_arr_2d.T # to make it symmetric\n","dtw_arr_2d = dtw_arr_2d + 1 # to make corr non-negative\n","\n","\n","pd.DataFrame(dtw_arr_2d).to_csv('dtw_distance_matrix_2d.csv',index=False)\n","\n","dtw_distance_matrix.to_csv('dtw_distance_matrix.csv',index=False)\n","\n","dtw_arr_2d = pd.read_csv('dtw_distance_matrix_2d.csv')\n","dtw_arr_2d = dtw_arr_2d.to_numpy()\n","\n","dtw_distance_vector = pd.read_csv('dtw_distance_matrix.csv')\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LRE6Kc_IDIZ"},"outputs":[],"source":["\n","\"\"\"                                                                Naive Analysis of DTW distance\n","\n","Can we just rank the distance matrix and group the largest positive/negative and smallest dtw ???\n","\"\"\"\n","dtw_distance_vector = pd.read_csv('dtw_distance_matrix.csv')\n","# plt.figure(figsize=(50,10))\n","# plt.hist(dtw_distance_vector.loc[0].values,bins=100000)\n","# plt.title('histogram')\n","# plt.xlabel('dynamic_time_warping_dist_matrix')\n","# plt.savefig('dtw_hist_number_of_cluster.PNG')\n","# plt.show()\n","\n","dtw_df = pd.DataFrame({'dtw':dtw_distance_vector.loc[0].values})\n","fig = px.histogram(dtw_df, x=\"dtw\",nbins=100000)\n","fig.show()\n","\n","\"\"\"\n","image = mpimg.imread('dtw_hist_number_of_cluster.PNG')\n","plt.imshow(image)\n","plt.show()\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJwrJtJvGuy-"},"outputs":[],"source":["\"\"\"                                 Distance Matrix calculation\n","Distance Measure: Longest common subsequence, LCSS\n","\n","\"\"\"\n","\n","\"\"\"\n","stations = unmet_60min_outflow_df.columns\n","unique_station_pairs = list(combinations(stations, 2))\n","\n","## station to index map\n","station_to_index_map = {}\n","i=0\n","for s in stations:\n","    station_to_index_map[s] = i\n","    i+=1\n","\n","## index to station map\n","index_to_station_map = {}\n","i=0\n","for s in stations:\n","    index_to_station_map[i] = s\n","    i+=1\n","\n","## create indices of 2d distance matrix\n","arr_dim = len(stations)\n","index = []\n","rows = 1\n","for c in np.arange(arr_dim-1):\n","  for r in np.arange(rows,arr_dim):\n","    index.append((r,c))\n","  rows = rows+1\n","\n","lcss_distance_matrix = pd.DataFrame(columns=unique_station_pairs,index=range(1))\n","lcss_arr = np.array([])\n","lcss_arr_2d = z = np.zeros(shape=(len(stations),len(stations)))\n","i=0\n","for pair in unique_station_pairs:\n","    x = unmet_60min_outflow_df[pair[0]].values\n","    y = unmet_60min_outflow_df[pair[1]].values\n","    xNormed = (x - x.mean())/(x.std()) # standardize the TS\n","    yNormed = (y - y.mean())/(y.std()) # standardize the TS\n","    lcss_dist = lcss(s1 = xNormed, s2 = yNormed ,eps=0.3, global_constraint= \"sakoe_chiba\",sakoe_chiba_radius=0.3)\n","    lcss_arr = np.append(lcss_arr,lcss_dist)\n","    lcss_arr_2d[index[i]] = lcss_dist\n","    i+=1\n","lcss_distance_matrix.loc[0] = lcss_arr\n","lcss_arr_2d = lcss_arr_2d + lcss_arr_2d.T # to make it symmetric\n","lcss_arr_2d = 1 - lcss_arr_2d # to make similarity to distance matrix\n","\n","\n","pd.DataFrame(lcss_arr_2d).to_csv('lcss_distance_matrix_2d.csv',index=False)\n","lcss_distance_matrix.to_csv('lcss_distance_matrix.csv',index=False)\n","\"\"\"\n","\n","\n","lcss_arr_2d = pd.read_csv('lcss_distance_matrix_2d.csv')\n","lcss_arr_2d = lcss_arr_2d.to_numpy()\n","\n","\n","#lcss_distance_vector = pd.read_csv('lcss_distance_matrix.csv')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRT2PyUoIiC5"},"outputs":[],"source":["\n","\"\"\"                                                                Naive Analysis of lcss distance\n","\n","Can we just rank the distance matrix and group the largest positive/negative and smallest lcss ???\n","\"\"\"\n","lcss_distance_vector = pd.read_csv('lcss_distance_matrix.csv')\n","# plt.figure(figsize=(50,10))\n","# plt.hist(dtw_distance_vector.loc[0].values,bins=100000)\n","# plt.title('histogram')\n","# plt.xlabel('dynamic_time_warping_dist_matrix')\n","# plt.savefig('dtw_hist_number_of_cluster.PNG')\n","# plt.show()\n","\n","lcss_df = pd.DataFrame({'lcss':lcss_distance_vector.loc[0].values})\n","fig = px.histogram(lcss_df, x=\"lcss\",nbins=100000)\n","fig.show()\n","\n","\"\"\"\n","image = mpimg.imread('dtw_hist_number_of_cluster.PNG')\n","plt.imshow(image)\n","plt.show()\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AM4WRDIWI8s"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","\n","Feature extracted: Fast Fourier Transform\n","\n","1) Identify the top 10 frequency components in each time series\n","2) Identify all the unique frequency components from all time series\n","3) Identify these frequency components in each time series\n","4) Use the magnitudes of these FIXED frequency components as features for each series.\n","\n","\"\"\"\n","\n","stations = unmet_60min_outflow_df.columns\n","#unique_station_pairs = list(combinations(stations, 2))\n","\n","top_x_frequencies = 5\n","num_common_freq=180\n","common_freq_set = set()\n","X_train = np.empty(shape=(0,num_common_freq))\n","\n","for station in stations:\n","    # Perform the FFT on the data\n","    fft_result = np.fft.fft(unmet_60min_outflow_df[station].values)\n","    # Compute the magnitudes of the FFT result\n","    fft_magnitudes = np.abs(fft_result)\n","    # Generate the corresponding frequency axis\n","    sampling_rate = 1.0  # Assuming a sampling rate of 1.0 (change as needed)\n","    frequency_axis = np.fft.fftfreq(len(fft_result), 1 / sampling_rate)\n","    positive_freq_indices = np.where(frequency_axis >= 0)\n","\n","    \"\"\" TOP periods/frequencies in the time series\"\"\"\n","    # Sort the magnitudes and frequencies in descending order\n","    sorted_indices = np.argsort(fft_magnitudes[positive_freq_indices])[::-1]\n","    sorted_magnitudes = fft_magnitudes[positive_freq_indices][sorted_indices]\n","    sorted_frequencies = frequency_axis[positive_freq_indices][sorted_indices]\n","    # Select the top periods\n","    top_mag = sorted_magnitudes[1:top_x_frequencies+1]\n","    top_freq = sorted_frequencies[1:top_x_frequencies+1] # REMOVE dc/mean component\n","    common_freq_set.update(top_freq)\n","    #print(\"Top mag: \",top_mag )\n","\n","common_freq_arr = np.sort(np.array(list(common_freq_set)))#[:num_common_freq]\n","#common_freq_arr = np.sort(np.array(list(common_freq_set)))[:num_common_freq]\n","\n","\n","for station in stations:\n","    # Perform the FFT on the data\n","    fft_result = np.fft.fft(unmet_60min_outflow_df[station].values)\n","    # Compute the magnitudes of the FFT result\n","    fft_magnitudes = np.abs(fft_result)\n","    # Generate the corresponding frequency axis\n","    sampling_rate = 1.0  # Assuming a sampling rate of 1.0 (change as needed)\n","    frequency_axis = np.fft.fftfreq(len(fft_result), 1 / sampling_rate)\n","    positive_freq_indices = np.where(frequency_axis >= 0)\n","\n","    unique_freqs_indices = np.where(np.isin(frequency_axis[positive_freq_indices], common_freq_arr))[0]\n","    #print(len(unique_freqs_indices))\n","    fft_mag_arr = fft_magnitudes[positive_freq_indices][unique_freqs_indices]\n","    normalized_magnitudes = fft_mag_arr / np.max(fft_mag_arr) # normalize it so that they are comparable\n","\n","    X_train = np.vstack((X_train,normalized_magnitudes[:num_common_freq]))\n","    #X_train = np.vstack((X_train,normalized_magnitudes))\n","\n","#print(X_train.shape)\n","\n","X_train = feature_standardization(X_train)\n","\n","fig, ax_tsne_2d = plt.subplots(ncols=2,figsize=(25,7))\n","fig_umap_2d = sp.make_subplots(rows=1, cols=1)\n","fig_umap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","fig_isomap_2d = sp.make_subplots(rows=1, cols=1)\n","fig_isomap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","\n","print_msg = 'FFT'\n","plotter = plot_all(print_msg)\n","plotter.plot_tSNE_2d(X_train,print_msg,ax_tsne_2d,0)\n","fig.show()\n","\n","fig_umap_2d.add_trace(plotter.plot_UMAP_2d(X_train,print_msg),row=1,col=1)\n","fig_umap_3d.add_trace(plotter.plot_UMAP_3d(X_train,))\n","\n","fig_isomap_2d.add_trace(plotter.plot_ISOMAP_2d(X_train,print_msg),row=1,col=1)\n","fig_isomap_3d.add_trace(plotter.plot_ISOMAP_3d(X_train,print_msg))\n","\n","fig_umap_2d.update_layout(height=600 , width=600,title='UMAP 2d, '+print_msg)\n","fig_umap_2d.show()\n","\n","fig_umap_3d.update_layout(height=600 , width=600,title='UMAP 3d, '+print_msg)\n","fig_umap_3d.show()\n","\n","fig_isomap_2d.update_layout(height=600 , width=600,title='ISOMAP 2d, '+print_msg)\n","fig_isomap_2d.show()\n","\n","fig_isomap_3d.update_layout(height=600 , width=600,title='ISOMAP 3d, '+print_msg)\n","fig_isomap_3d.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MjPsuLIs_Fu"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","Kmeans clustering using dimensionality reduced TS\n","\n","\"\"\"\n","\n","\n","\"\"\"\n","silhouette_index = []\n","min_clusters = 2\n","max_clusters = 30\n","\n","X_train = Dim_Red_unmet_60min_outflow_week_df.to_numpy().T\n","\n","for num_clsuter in range(min_clusters,max_clusters):\n","  km = TimeSeriesKMeans(n_clusters=30, metric=\"dtw\")\n","  km.fit(X_train)\n","  score = silhouette_score(X_train, km.labels_, metric=\"dtw\")\n","  silhouette_index.append(score)\n","\n","plt.figure(figsize=(15,5))\n","plt.xticks(range(min_clusters,max_clusters))\n","plt.plot(range(min_clusters,max_clusters),silhouette_index) # mean score for all samples. It is between [-1,1] where -1 needs reassignment, 1 is best.\n","plt.title('Number of clusters selection for kmeans')\n","plt.xlabel('number of clusters')\n","plt.ylabel('silhouette_score')\n","plt.savefig('silhouette_score_kmeans.PNG')\n","plt.show()\n","\"\"\"\n","\n","image = mpimg.imread('silhouette_score_kmeans.PNG')\n","plt.figure(figsize=(15,5))\n","plt.imshow(image)\n","plt.show()\n","\n","proposed_number_of_clusters = 3#25,\n","\n","for num_clsuter in [proposed_number_of_clusters]:\n","  km = TimeSeriesKMeans(n_clusters=num_clsuter, metric=\"dtw\")\n","  X_train = Dim_Red_unmet_60min_outflow_week_df\n","  km.fit(X_train)\n","  plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jR1zRXSUywbQ"},"outputs":[],"source":["proposed_number_of_clusters = 15#25,\n","\n","for num_clsuter in [proposed_number_of_clusters]:\n","  km = TimeSeriesKMeans(n_clusters=num_clsuter, metric=\"dtw\")\n","  X_train = Dim_Red_unmet_60min_outflow_week_df\n","  km.fit(X_train)\n","  plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KZBHL1EVp4u"},"outputs":[],"source":["proposed_number_of_clusters = 25#25,\n","\n","for num_clsuter in [proposed_number_of_clusters]:\n","  km = TimeSeriesKMeans(n_clusters=num_clsuter, metric=\"dtw\")\n","  X_train = Dim_Red_unmet_60min_outflow_week_df\n","  km.fit(X_train)\n","  plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SI7q0A1t_uw"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","K-Medoids using pearson and spearman distance matrix\n","\"\"\"\n","\n","## pearson correlation\n","silhouette_index = []\n","min_clusters = 2\n","max_clusters = 200\n","intertia_list = []\n","dist_metric = 'correlation'\n","\n","X_train = Dim_Red_unmet_60min_outflow_week_df.to_numpy().T\n","\n","for k in range(min_clusters, max_clusters):\n","    #kmedoids = KMedoids(n_clusters=k, metric='precomputed', init='random', method=\"alternate\").fit(pearr_arr_2d)\n","    kmedoids = KMedoids(n_clusters=k, metric=dist_metric, init='random', method=\"alternate\").fit(X_train)\n","    intertia_list.append(kmedoids.inertia_)\n","    #X_train = Dim_Red_unmet_60min_outflow_week_df\n","    score = silhouette_score(pearr_arr_2d, kmedoids.labels_, metric='precomputed')\n","    silhouette_index.append(score)\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.xticks(range(min_clusters,max_clusters))\n","plt.plot(range(min_clusters,max_clusters),silhouette_index) # mean score for all samples. It is between [-1,1] where -1 needs reassignment, 1 is best. # Scores around zero indicate overlapping clusters.\n","plt.title('Number of clusters selection for K-Medoids')\n","plt.xlabel('number of clusters')\n","plt.ylabel('silhouette_score')\n","plt.savefig('silhouette_score_K-Medoids_pearson.PNG')\n","plt.show()\n","\n","\"\"\"\n","image = mpimg.imread('silhouette_score_K-Medoids.PNG')\n","plt.figure(figsize=(15,5))\n","plt.imshow(image)\n","plt.show()\n","\"\"\"\n","\n","# ## spearman correlation\n","# max_num_clusters_spr = 3\n","# labels_list_spr = []\n","# cluster_centers_list_spr = []\n","# intertia_list_spr = []\n","# for k in range(2, max_num_clusters_spr):\n","#     kmedoids = KMedoids(n_clusters=k, metric=\"precomputed\", method=\"pam\").fit(spearr_arr_2d)\n","#     labels_list_spr.append(kmedoids.labels_)\n","#     cluster_centers_list_spr.append(kmedoids.cluster_centers_)\n","#     intertia_list_spr.append(kmedoids.inertia_)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdPfX9szynE0"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","K-Medoids using DTW distance matrix\n","\"\"\"\n","\n","dtw_arr_2d = pd.read_csv('dtw_distance_matrix_2d.csv')\n","dtw_arr_2d = dtw_arr_2d.to_numpy()\n","#dtw_arr_2d = dtw_arr_2d - 1\n","\n","## pearson correlation\n","silhouette_index = []\n","min_clusters = 2\n","max_clusters = 40\n","dist_metric = 'dtw'\n","\n","X_train = Dim_Red_unmet_60min_outflow_week_df.to_numpy().T\n","\n","for k in range(min_clusters, max_clusters):\n","    kmedoids = KMedoids(n_clusters=k, metric='precomputed', init='random', method=\"alternate\").fit(dtw_arr_2d)\n","    #kmedoids = KMedoids(n_clusters=k, metric=dist_metric, init='random', method=\"alternate\").fit(X_train)\n","    #X_train = Dim_Red_unmet_60min_outflow_week_df\n","    score = silhouette_score(X_train, kmedoids.labels_, metric=dist_metric)\n","    silhouette_index.append(score)\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.xticks(range(min_clusters,max_clusters))\n","plt.plot(range(min_clusters,max_clusters),silhouette_index) # mean score for all samples. It is between [-1,1] where -1 needs reassignment, 1 is best. # Scores around zero indicate overlapping clusters.\n","plt.title('Number of clusters selection for K-Medoids with dynamic time warping')\n","plt.xlabel('number of clusters')\n","plt.ylabel('silhouette_score')\n","plt.savefig('silhouette_score_K-Medoids_dtw.PNG')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xaTakJ-Lv04"},"outputs":[],"source":["lcss_arr_2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKqbvZ7nKN2H"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","K-Medoids using LCSS distance matrix\n","\"\"\"\n","\n","lcss_arr_2d = pd.read_csv('lcss_distance_matrix_2d.csv')\n","lcss_arr_2d = lcss_arr_2d.to_numpy()\n","np.fill_diagonal(lcss_arr_2d, 0)\n","\n","silhouette_index = []\n","min_clusters = 2\n","max_clusters = 40\n","dist_metric = 'lcss'\n","\n","X_train = Dim_Red_unmet_60min_outflow_week_df.to_numpy().T\n","\n","for k in range(min_clusters, max_clusters):\n","    kmedoids = KMedoids(n_clusters=k, metric='precomputed', method=\"pam\",max_iter=1000).fit(lcss_arr_2d)\n","    #kmedoids = KMedoids(n_clusters=k, metric=dist_metric, init='random', method=\"alternate\").fit(X_train)\n","    #X_train = Dim_Red_unmet_60min_outflow_week_df\n","    score = silhouette_score(lcss_arr_2d, kmedoids.labels_, metric=\"precomputed\")\n","    silhouette_index.append(score)\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.xticks(range(min_clusters,max_clusters))\n","plt.plot(range(min_clusters,max_clusters),silhouette_index) # mean score for all samples. It is between [-1,1] where -1 needs reassignment, 1 is best. # Scores around zero indicate overlapping clusters.\n","plt.title('Number of clusters selection for K-Medoids with longest common subsequence')\n","plt.xlabel('number of clusters')\n","plt.ylabel('silhouette_score')\n","plt.savefig('silhouette_score_K-Medoids_LCSS.PNG')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2T-RnaHASWmK"},"outputs":[],"source":["arr = Dim_Red_unmet_60min_outflow_week_df.to_numpy()\n","arr.T.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDbF1ts08FTN"},"outputs":[],"source":["X = np.asarray([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQwKdK3o4dLJ"},"outputs":[],"source":["print(kmedoids.cluster_centers_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIE_ERdZuYpN"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","DBSCAN using pearson and spearman distance matrix\n","\"\"\"\n","\n","########## SOMETHING WRONG HERE!!!!!!!!!!!!!!!!!!!!\n","\n","\n","\n","dbscan = DBSCAN(eps=0.05, min_samples=5,metric=\"precomputed\")\n","dbscan.fit(pearr_arr_2d)\n","\n","dbscan.labels_\n","\n","dbscan = DBSCAN(eps=0.05, min_samples=5,metric=\"precomputed\")\n","dbscan.fit(spearr_arr_2d)\n","\n","dbscan.labels_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmbz_tVIAXKp"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","HDBSCAN using pearson and spearman distance matrix\n","\"\"\"\n","\n","clusterer = hdbscan.HDBSCAN(metric='precomputed',min_cluster_size=2).fit(pearr_arr_2d)\n","clusterer.labels_\n","\n","clusterer = hdbscan.HDBSCAN(metric='precomputed',min_cluster_size=2).fit(spearr_arr_2d)\n","clusterer.labels_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k93iV_B7vS_L"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","Agglomerative clustering using pearson and spearman distance matrix\n","\n","\"\"\"\n","\n","clustering = AgglomerativeClustering(metric='precomputed',linkage='complete' ).fit(pearr_arr_2d)\n","clustering.labels_\n","\n","\n","clustering = AgglomerativeClustering(metric='precomputed',linkage='complete' ).fit(spearr_arr_2d)\n","clustering.labels_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRxGeLFVvrcg"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","K-Medoids using Dynamic Time Warping\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ina2CKDTvg36"},"outputs":[],"source":["\"\"\"                                                               Distance Matrix calculation\n","Distance Measure: Longest common subsequence\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jauRDFgvvg0K"},"outputs":[],"source":["\"\"\"                                                                   CLUSTERING Analysis\n","\n","K-Medoids using  Longest common subsequence\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_br4nq2vgwU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcEZhgJBi-Wg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdveO9oNi-Wg"},"outputs":[],"source":["\"\"\"\n","Feature extracted: Top 5 ACF/PACF lags\n","\n","\"\"\"\n","\n","projection = TSNE().fit_transform(data)\n","plt.scatter(*projection.T, **plot_kwds)\n","\n","\n","\n","umap_3d = UMAP(n_components=3, init='random', random_state=0)\n","\n","proj_2d = umap_2d.fit_transform(features)\n","proj_3d = umap_3d.fit_transform(features)\n","\n","fig_2d = px.scatter(\n","    proj_2d, x=0, y=1,\n","    color=df.species, labels={'color': 'species'}\n",")\n","fig_3d = px.scatter_3d(\n","    proj_3d, x=0, y=1, z=2,\n","    color=df.species, labels={'color': 'species'}\n",")\n","fig_3d.update_traces(marker_size=5)\n","\n","fig_2d.show()\n","fig_3d.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ggKN8yai-Wh"},"outputs":[],"source":["\"\"\"\n","Feature extracted: Total demand, Mean, mode median, upper & Lower whisker, max, min, std, kurtosis, skewness,\n","\n","\"\"\"\n","\n","projection = TSNE().fit_transform(data)\n","plt.scatter(*projection.T, **plot_kwds)\n","\n","\n","\n","umap_3d = UMAP(n_components=3, init='random', random_state=0)\n","\n","proj_2d = umap_2d.fit_transform(features)\n","proj_3d = umap_3d.fit_transform(features)\n","\n","fig_2d = px.scatter(\n","    proj_2d, x=0, y=1,\n","    color=df.species, labels={'color': 'species'}\n",")\n","fig_3d = px.scatter_3d(\n","    proj_3d, x=0, y=1, z=2,\n","    color=df.species, labels={'color': 'species'}\n",")\n","fig_3d.update_traces(marker_size=5)\n","\n","fig_2d.show()\n","fig_3d.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShuMIbNki-Wh"},"outputs":[],"source":["\"\"\"\n","Feature extracted: Discrete Wavelet Transform (DWT)\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiN_nYfNi-Wh"},"outputs":[],"source":["\"\"\"\n","Feature extracted: Symbolic Aggregate approximation (SAX)\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVLtbk4pi-Wh"},"outputs":[],"source":["\"\"\"\n","STATION-LEVEL\n","\n","Compare with information from actual transaction logs\n","\n","Form connectivity matrix for each hour from trascation logs.\n","\n","connectivity matrix = number of cars leaving a source and entering destination\n","\n","    destination stn\n","       A B C D E\n","s    A\n","o    B\n","u    C\n","r    D\n","c    E\n","e\n","stn\n","\n","Create frobenius/euclidean norm of each matrix to quantify large activity\n","Rank the matrices to identify hours of maxium activity\n","Compare the results with correlation analysis from above.\n","\n","\"\"\"\n","\n","#TODO : ADITI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kK4RtzXCi-Wh"},"outputs":[],"source":["\"\"\"\n","#UNMET demand\n","\n","Identify clusters of regions that show similar behaviour.\n","Use a Similarity/distance metric to find Similarity/distance between the entire time series of all pairs of regions\n","Ranking of entire time series similairy for each region with all the other regions.\n","After finding all pariwise distances, cluster them so that a region's nearest neighbours can be identified on a 2D or 3D plot\n","Try out different Similarity/distance metrics and repeat steps\n","Try to form interpretations of theses clusterings. e.g. residential/office/etc.. High/low demand. peak/non-peak season\n","\"\"\"\n","\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\outflow_data')\n","region_unmet_outflow_df = pd.read_csv('region_unmet_outflow.csv')\n","region_unmet_outflow_df = region_unmet_outflow_df.rename(columns={\"Unnamed: 0\": \"datetime\"})\n","region_unmet_outflow_df\n","\n","\n","#TODO: Aditi\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQKbkAuci-Wi"},"outputs":[],"source":["\"\"\"\n","#MET demand\n","\n","Identify clusters of regions that show similar behaviour.\n","Use a Similarity/distance metric to find Similarity/distance between the entire time series of all pairs of regions\n","Ranking of entire time series similairy for each region with all the other regions.\n","After finding all pariwise distances, cluster them so that a region's nearest neighbours can be identified on a 2D or 3D plot\n","Try out different Similarity/distance metrics and repeat steps\n","Try to form interpretations of theses clusterings. e.g. residential/office/etc.. High/low demand. peak/non-peak season\n","\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\outflow_data')\n","region_met_outflow_df = pd.read_csv('region_met_outflow.csv')\n","region_met_outflow_df = region_met_outflow_df.rename(columns={\"Unnamed: 0\": 'datetime'})\n","region_met_outflow_df\n","\n","#TODO: Aditi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_VSj78Ri-Wi"},"outputs":[],"source":["\"\"\"\n","#TOTAL demand\n","\n","Identify clusters of regions that show similar behaviour.\n","Use a Similarity/distance metric to find Similarity/distance between the entire time series of all pairs of regions\n","Ranking of entire time series similairy for each region with all the other regions.\n","After finding all pariwise distances, cluster them so that a region's nearest neighbours can be identified on a 2D or 3D plot\n","Try out different Similarity/distance metrics and repeat steps\n","Try to form interpretations of theses clusterings. e.g. residential/office/etc.. High/low demand. peak/non-peak season\n","\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\outflow_data')\n","region_total_outflow_df = pd.read_csv('region_total_outflow.csv')\n","region_total_outflow_df = region_total_outflow_df.rename(columns={\"Unnamed: 0\": 'datetime'})\n","region_total_outflow_df\n","\n","#TODO: Aditi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2ghZHci-Wi"},"outputs":[],"source":["\"\"\"\n","Where is demand from a region going to become supply in other regions?\n","\n","Cross-Correlation between each region's met demand and all regions' inflow.\n","Cross-Correlation handles current and future inflows\n","### NO NEED Cross-correlation. only check present, 1 hour and 2 hour leading pearson correlaiton coefficient\n","\n","Sometimes demand can become supply within the region, not necessarily leave the region.\n","\n","check linearity of scatter plot before applying pearson correlation.\n","\n","\"\"\"\n","import scipy.stats as stats\n","\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\inflow_data')\n","region_inflow_df = pd.read_csv('region_inflow.csv')\n","region_inflow_df = region_inflow_df.rename(columns={\"Unnamed: 0\": 'datetime'})\n","\n","for col_outf in region_met_outflow_df.columns[1:]:\n","    print('\\n')\n","    print('Region: ',col_outf)\n","    rank_dict = {}\n","    for col_inf in region_inflow_df.columns[1:]:\n","        #if col_outf != col_inf: # check correlation between other regions\n","        ## positive correlation means that the cars entered\n","        ## negative correlation means cars did not enter\n","        ## cross correlation with largest sum of all positive cross correlations indicates region into which cars entered\n","        ## cross correlation with smallest sum of all negative cross correlations indicates region into which cars did NOT entered, this helps to eliminate non-entered regions\n","        time_series1 = region_met_outflow_df[col_outf]\n","        time_series2 = region_inflow_df[col_inf]\n","\n","        time_series2_lead1 = time_series2.shift(periods=-1).dropna()\n","        time_series2_lead2 = time_series2.shift(periods=-2).dropna()\n","\n","        rank_dict[col_inf] = []\n","\n","        # original inflow time series\n","        r, p = stats.pearsonr(time_series1, time_series2)\n","        rank_dict[col_inf] = r\n","        # plt.scatter(x=time_series1,y=time_series2)\n","        # plt.xlabel('demand: '+str(col_outf))\n","        # plt.ylabel('supply: '+str(col_inf))\n","        # plt.title('rho: '+str(r))\n","        # plt.show()\n","\n","        # lead 1 hour inflow time series\n","        r1, p1 = stats.pearsonr(time_series1[:-1], time_series2_lead1)\n","        rank_dict[col_inf+'_lead1'] = r1\n","        # plt.scatter(x=time_series1[:-1],y=time_series2_lead1)\n","        # plt.xlabel('demand: '+str(col_outf))\n","        # plt.ylabel('supply lead 1 hour: '+str(col_inf))\n","        # plt.title('rho: '+str(r1))\n","        # plt.show()\n","\n","        # lead 2 hours inflow time series\n","        r2, p2 = stats.pearsonr(time_series1[:-2], time_series2_lead2)\n","        rank_dict[col_inf+'_lead2'] = r2\n","        # plt.scatter(x=time_series1[:-2],y=time_series2_lead2)\n","        # plt.xlabel('demand: '+str(col_outf))\n","        # plt.ylabel('supply lead 2 hours: '+str(col_inf))\n","        # plt.title('rho: '+str(r2))\n","        # plt.show()\n","\n","    # Sort the dictionary based on values in descending order\n","    sorted_dict = sorted(rank_dict.items(), key=lambda x: x[1], reverse=True)\n","    # Get the three largest key-value pairs\n","    top_sorted = sorted_dict[:3]\n","    # Print the result\n","    print(col_outf,'outflow has LARGEST correlations with following inflows:')\n","    for key, value in top_sorted:\n","        print(key, value)\n","    bottom_sorted = sorted_dict[-3:]\n","    # Print the result\n","    print(col_outf,'outflow has SMALLEST correlations with following inflows:')\n","    for key, value in bottom_sorted:\n","        print(key, value)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tPl2m9Hi-Wj"},"outputs":[],"source":["\"\"\"\n","UNMET demand\n","1)\n","2) check FFT of top 3 periods, check lags of top 3 PACF plots\n","3) boxplot to see distribution of demand during each region's hour of the weekday.\n","4) optional Treemap to verify boxplot above.\n","\n","This info. can be used for performance monitoring. sum of cluster\n","outputs from the same region is within this box plot.\n","\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\outflow_data')\n","region_unmet_outflow_df = pd.read_csv('region_unmet_outflow.csv')\n","region_unmet_outflow_df = region_unmet_outflow_df.rename(columns={\"Unnamed: 0\": 'datetime'})\n","\n","## FFT and IFFT to identify the significant periods in UNMET demand\n","print('\\t\\t\\tFFT analysis of regions\\n')\n","for reg in region_unmet_outflow_df.columns[1:]:\n","    print(reg)\n","\n","    # Perform the FFT on the data\n","    fft_result = np.fft.fft(region_unmet_outflow_df[reg].values)\n","    # Compute the magnitudes of the FFT result\n","    fft_magnitudes = np.abs(fft_result)\n","    # Generate the corresponding frequency axis\n","    sampling_rate = 1.0  # Assuming a sampling rate of 1.0 (change as needed)\n","    frequency_axis = np.fft.fftfreq(len(fft_result), 1 / sampling_rate)\n","    positive_freq_indices = np.where(frequency_axis >= 0)\n","    # Plot the FFT result versus frequency\n","    # fig = go.Figure(data=go.Scatter(x=frequency_axis[positive_freq_indices], y=fft_magnitudes[positive_freq_indices],mode='lines+markers'))\n","    # fig.update_layout(\n","    #     title='FFT Result versus Frequency (unmet demand)',\n","    #     xaxis_title=\"Frequency\",\n","    #     yaxis_title=\"Magnitude\",\n","    #     autosize=False,\n","    #     width=1000,\n","    #     height=800,\n","    # )\n","    # fig.show()\n","\n","    \"\"\" TOP periods/frequencies in the time series\"\"\"\n","    # Sort the magnitudes and frequencies in descending order\n","    sorted_indices = np.argsort(fft_magnitudes[positive_freq_indices])[::-1]\n","    sorted_magnitudes = fft_magnitudes[positive_freq_indices][sorted_indices]\n","    sorted_frequencies = frequency_axis[positive_freq_indices][sorted_indices]\n","    # Select the top 8 periods\n","    top_freq = sorted_frequencies[1:6] # REMOVE dc/mean component\n","    top_periods = 1/top_freq\n","    top_periods = [round(p) for p in top_periods]\n","    print(f'Top FFT periods in unmet demand are : {top_periods}')\n","    if ((12 and 168) not in top_periods):\n","        print('\\t\\tNO daily AND Weekly Seasonality TOGETHER!!')\n","    if ((12 and 168) not in top_periods[:2]):\n","        print('\\t\\tDaily AND Weekly Seasonality are NOT top 2 Frequencies!! Presence of BUSINESS/ECONOMIC cycle!')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BfkoAabi-Wj"},"outputs":[],"source":["\n","\"\"\" PACF plot of region unmet demand\"\"\"\n","\n","## FFT and IFFT to identify the significant periods in UNMET demand\n","print('\\t\\t\\PACF analysis of regions\\n')\n","for reg in region_unmet_outflow_df.columns[1:]:\n","    time_series = region_unmet_outflow_df[reg].values\n","\n","    # Compute the ACF and PACF using the statsmodels library\n","    pacf_values = sm.tsa.stattools.pacf(time_series, nlags=len(time_series)//7, method='ywm')\n","\n","    # Plot the PACF\n","    fig = go.Figure(data=go.Scatter(x=np.arange(len(pacf_values)), y=pacf_values,mode='lines+markers'))\n","    fig.add_hline(y=-1.96/np.sqrt(len(time_series)),line_width=3, line_dash=\"dash\", line_color=\"green\")\n","    fig.add_hline(y=1.96/np.sqrt(len(time_series)),line_width=3, line_dash=\"dash\", line_color=\"green\")\n","    fig.update_layout(\n","        title='Partial Autocorrelation Function (PACF) (unmet demand), Region: '+ reg,\n","        xaxis_title=\"Time Lag\",\n","        yaxis_title=\"PACF\",\n","        autosize=False,\n","        width=1000,\n","        height=800,\n","    )\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJ_1h8gNi-Wj"},"outputs":[],"source":["\"\"\"\n","Pandas Profiling\n","\"\"\"\n","\n","# os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\station_level\\outflow_data')\n","# UnmetDemand_df_station_lvl = pd.read_parquet('UnmetDemand_df.parquet')\n","\n","\n","# # identify if any time series is all 0\n","# for c in UnmetDemand_df_station_lvl.columns:\n","#     if UnmetDemand_df_station_lvl[c].nunique() == 1:\n","#         print('station with all zeros: ',c)\n","\n","# UnmetDemand_df_station_lvl.index.name = 'datetime'\n","\n","# UnmetDemand_df_vstack = pd.DataFrame(columns=['datetime','station_id','unmet_demand'])\n","# # UnmetDemand_df_vstack.index.name = 'datetime'\n","\n","# for c in UnmetDemand_df_station_lvl.columns:\n","#     temp_df = pd.DataFrame(columns=['station_id','unmet_demand'])\n","#     temp_df['station_id'] = np.repeat(int(c),UnmetDemand_df_station_lvl.shape[0])\n","#     temp_df['unmet_demand'] = UnmetDemand_df_station_lvl[c].values\n","#     temp_df['datetime'] = UnmetDemand_df_station_lvl.index\n","#     #temp_df.index = UnmetDemand_df_station_lvl.index\n","#     UnmetDemand_df_vstack = pd.concat([UnmetDemand_df_vstack, temp_df],axis=0)\n","\n","# UnmetDemand_df_vstack['unmet_demand'].hist()\n","# UnmetDemand_df_vstack\n","\n","# # PANDAS profiling\n","# profile = ProfileReport(UnmetDemand_df_vstack, tsmode=True, sortby=\"datetime\")\n","# os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\docs\\\\read_me\\pandas_data_profile')\n","# #profile.to_file('profile_report.html')\n","\n","\n","# # Return the profile per station\n","# for group in UnmetDemand_df_vstack.groupby(\"station_id\"):\n","#     #Running 1 profile per station\n","#     #print(group[1])\n","#     profile = ProfileReport(\n","#         group[1],\n","#         tsmode=True,\n","#         sortby=\"datetime\",\n","#         # title=f\"Air Quality profiling - Site Num: {group[0]}\",\n","#     )\n","\n","#     profile.to_file(f\"Ts_Profile_{group[0]}.html\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"589n3yKHi-Wk"},"outputs":[],"source":["\"\"\"\n","High-level statistical comparison of unmet demand across regions\n","\"\"\"\n","\n","print('\\t\\t High-level statistical comparison of unmet demand across regions\\n')\n","\n","region_total_unmet_outflow = {}\n","region_max_peak_n_datetime = {}\n","region_average_unmet_outflow = {}\n","region_median_unmet_outflow = {}\n","region_std_unmet_outflow = {}\n","\n","for reg in region_unmet_outflow_df.columns[1:]:\n","    region_max_value = region_unmet_outflow_df[reg].max()\n","    max_time = region_unmet_outflow_df[region_unmet_outflow_df[reg] == region_unmet_outflow_df[reg].max()]['datetime']\n","    region_max_peak_n_datetime[reg] = []\n","    region_max_peak_n_datetime[reg].append(region_max_value)\n","    region_max_peak_n_datetime[reg].append(max_time.values)\n","\n","    region_total_unmet_outflow[reg] = region_unmet_outflow_df[reg].sum()\n","    region_average_unmet_outflow[reg] = region_unmet_outflow_df[reg].mean()\n","    region_median_unmet_outflow[reg] = region_unmet_outflow_df[reg].median()\n","    region_std_unmet_outflow[reg] = region_unmet_outflow_df[reg].std()\n","\n","#HIGHEST\n","top_x = 5\n","\n","## 3 regions with Highest total unmet demand\n","total_sorted_up =  sorted(region_total_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with HIGHEST total unmet demand: ',total_sorted_up[:top_x])\n","\n","## 3 regions with Highest average unmet demand\n","avg_sorted_up =  sorted(region_average_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest average unmet demand: ',avg_sorted_up[:top_x])\n","\n","## 3 regions with Highest median unmet demand\n","median_sorted_up =  sorted(region_median_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest median unmet demand: ',median_sorted_up[:top_x])\n","\n","## 3 regions with Highest std unmet demand\n","std_sorted_up =  sorted(region_std_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest std unmet demand: ',std_sorted_up[:top_x])\n","\n","## 3 regions with Maximum peak unmet demand and datetime\n","max_peak_n_datetime_sorted_up = sorted(region_max_peak_n_datetime.items(), key=lambda x: x[1][0],reverse=True )\n","print('Top 3 regions with HIGHEST Maximum peak unmet demand and datetime: ',max_peak_n_datetime_sorted_up[:3])\n","\n","\n","#LOWEST\n","\n","## 3 regions with Lowest total unmet demand\n","total_sorted_down =  sorted(region_total_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST total unmet demand: ',total_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest average unmet demand\n","avg_sorted_down =  sorted(region_average_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST average unmet demand: ',avg_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest median unmet demand\n","median_sorted_down =  sorted(region_median_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST median unmet demand: ',median_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest std unmet demand\n","std_sorted_down =  sorted(region_std_unmet_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST std unmet demand: ',std_sorted_down[-top_x:])\n","\n","## 3 regions with Minimum peak unmet demand and datetime\n","max_peak_n_datetime_sorted_down = sorted(region_max_peak_n_datetime.items(), key=lambda x: x[1][0],reverse=True )\n","print('Top 3 regions with LOWEST Maximum peak unmet demand and datetime: ',max_peak_n_datetime_sorted_down[-3:])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lms-8Yxai-Wk"},"outputs":[],"source":["\"\"\"\n","High-level statistical comparison of TOTAL demand across regions\n","\"\"\"\n","os.chdir('C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\DeepAR-pytorch\\My_model\\Rental_Fleet_Demand_Forecast\\data\\\\region_level\\outflow_data')\n","region_total_outflow_df = pd.read_csv('region_total_outflow.csv')\n","region_total_outflow_df = region_total_outflow_df.rename(columns={\"Unnamed: 0\": 'datetime'})\n","\n","print('\\t\\t High-level statistical comparison of TOTAL demand across regions\\n')\n","\n","region_total_TOTAL_outflow = {}\n","region_max_peak_n_datetime = {}\n","region_average_TOTAL_outflow = {}\n","region_median_TOTAL_outflow = {}\n","region_std_TOTAL_outflow = {}\n","\n","for reg in region_total_outflow_df.columns[1:]:\n","    region_max_value = region_total_outflow_df[reg].max()\n","    max_time = region_total_outflow_df[region_total_outflow_df[reg] == region_total_outflow_df[reg].max()]['datetime']\n","    region_max_peak_n_datetime[reg] = []\n","    region_max_peak_n_datetime[reg].append(region_max_value)\n","    region_max_peak_n_datetime[reg].append(max_time.values)\n","\n","    region_total_TOTAL_outflow[reg] = region_total_outflow_df[reg].sum()\n","    region_average_TOTAL_outflow[reg] = region_total_outflow_df[reg].mean()\n","    region_median_TOTAL_outflow[reg] = region_total_outflow_df[reg].median()\n","    region_std_TOTAL_outflow[reg] = region_total_outflow_df[reg].std()\n","\n","#HIGHEST\n","top_x = 5 # top x number of regions\n","\n","## 3 regions with Highest total TOTAL demand\n","total_sorted_up =  sorted(region_total_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with HIGHEST total TOTAL demand: ',total_sorted_up[:top_x])\n","\n","## 3 regions with Highest average TOTAL demand\n","avg_sorted_up =  sorted(region_average_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest average TOTAL demand: ',avg_sorted_up[:top_x])\n","\n","## 3 regions with Highest median TOTAL demand\n","median_sorted_up =  sorted(region_median_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest median TOTAL demand: ',median_sorted_up[:top_x])\n","\n","## 3 regions with Highest std TOTAL demand\n","std_sorted_up =  sorted(region_std_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with Highest std TOTAL demand: ',std_sorted_up[:top_x])\n","\n","## 3 regions with Maximum peak TOTAL demand and datetime\n","max_peak_n_datetime_sorted_up = sorted(region_max_peak_n_datetime.items(), key=lambda x: x[1][0],reverse=True )\n","print('Top 3 regions with HIGHEST Maximum peak TOTAL demand and datetime: ',max_peak_n_datetime_sorted_up[:3])\n","\n","\n","#LOWEST\n","\n","## 3 regions with Lowest total TOTAL demand\n","total_sorted_down =  sorted(region_total_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST total TOTAL demand: ',total_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest average TOTAL demand\n","avg_sorted_down =  sorted(region_average_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST average TOTAL demand: ',avg_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest median TOTAL demand\n","median_sorted_down =  sorted(region_median_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST median TOTAL demand: ',median_sorted_down[-top_x:])\n","\n","## 3 regions with Lowest std TOTAL demand\n","std_sorted_down =  sorted(region_std_TOTAL_outflow.items(), key=lambda x: x[1],reverse=True )\n","print('Top 3 regions with LOWEST std TOTAL demand: ',std_sorted_down[-top_x:])\n","\n","## 3 regions with Minimum peak TOTAL demand and datetime\n","max_peak_n_datetime_sorted_down = sorted(region_max_peak_n_datetime.items(), key=lambda x: x[1][0],reverse=True )\n","print('Top 3 regions with LOWEST Maximum peak TOTAL demand and datetime: ',max_peak_n_datetime_sorted_down[-3:])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wOt4JrKi-Wk"},"outputs":[],"source":["\"\"\"\n","extraction of seasonal patterns, peak patterns and other persistent\n","patterns for performance monitoring of predictions\n","\n","pass these patterns to performance_monitor.ipynb\n","\n","\t5) We can cluster regions with high unmet demand during certain times of the day.\n","\t\ta. First establish that each of the 32 time series have weekly seasonality and are very similar week over week.\n","\t\tb. Try to aggregate all times into hours in a single week. As all weeks are similar. Create box plots to find out these bounds that can be used by the new loss function.\n","Cluster regions with high demand at different times/hours of the week.\n","\n","\"\"\"\n","\n","region_total_outflow_df['datetime'] = region_total_outflow_df['datetime'].apply( lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\") )\n","\n","region_total_outflow_df.insert(1, 'weekday',region_total_outflow_df['datetime'].dt.strftime('%A') )\n","region_total_outflow_df.insert(1, 'hour',region_total_outflow_df['datetime'].dt.strftime('%H') )\n","\n","#You first create your list in the order you want it\n","days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n","#Using Categorical() function to set the order according to how it is arranged above\n","region_total_outflow_df[\"weekday\"] = pd.Categorical(region_total_outflow_df.weekday, categories=days, ordered=True)\n","#region_total_outflow_df.drop(columns=['datetime'],inplace=True)\n","week_df = region_total_outflow_df.groupby ( by=[\"weekday\",\"hour\"] ).agg([np.mean])\n","\n","week_df.drop(columns=['datetime'],inplace=True)\n","fig, ax = plt.subplots(figsize=(35,50))         # Sample figsize in inches\n","ax.set_title('Weekday-hour mean Groupby TOTAL outflow/demand')\n","sns.heatmap(week_df, annot=True,linewidths=.5, ax=ax)\n","\n","\n","# #Pivot table implementation\n","# pivot = pd.pivot_table(region_total_outflow_df, values=region_total_outflow_df.columns[1:], index=['hour'], columns=['weekday'], aggfunc=np.mean)\n","# pivot.loc['00',('Ang Mo Kio','Friday')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXDcbYSyi-Wk"},"outputs":[],"source":["\"\"\"\n","Boxplot\n","\n","\"\"\"\n","\n","for reg in region_total_outflow_df.columns[3:]:\n","\n","    # pick only sundays\n","    weekday = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\",\"Sunday\"]\n","    for day in weekday:\n","        day_df = region_total_outflow_df[ region_total_outflow_df['weekday'] == day]\n","        ts = day_df[reg].values\n","\n","        # Top and Bottom Whisker to define upper bound and Lower bound on prediciotn data.\n","        hour_df_75_perc = day_df.groupby(by=['hour'])[reg].quantile(.75)# agg(np.percentile(75))\n","        hour_df_25_perc = day_df.groupby(by=['hour'])[reg].quantile(.25) #.agg(np.percentile(25))\n","        hour_df_iqr = day_df.groupby(by=['hour'])[reg].agg(iqr)\n","        hour_top_whisker = hour_df_75_perc + 1.5*hour_df_iqr\n","        hour_bottom_whisker = hour_df_25_perc - 1.5*hour_df_iqr\n","\n","        # print(hour_top_whisker)\n","        # print(hour_bottom_whisker)\n","\n","        fig, ax = plt.subplots(figsize=(35,10))\n","        ax.set_title(str(day) +' TOTAL outflow/demand, Region: '+str(reg))\n","        sns.violinplot(x = day_df['datetime'].dt.hour, y = ts, ax = ax)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GUol4Mpi-Wl"},"outputs":[],"source":["\"\"\"\n","\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5Qb7V7Fi-Wl"},"outputs":[],"source":["\"\"\"\n","Feature engineering\n","\n","shortlist relevant features from analysis of each region to input to deepar model\n","\n","\"\"\"\n","\n","\n","region_total_outflow_df['datetime'][0]\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
