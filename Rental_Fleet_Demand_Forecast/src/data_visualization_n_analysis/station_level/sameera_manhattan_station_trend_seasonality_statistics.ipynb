{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3-KR7aZ3i_jh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691749156221,"user_tz":-480,"elapsed":28134,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"}},"outputId":"f63753f2-9716-4617-8caa-cb86d3a5a719"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/bin/bash: line 1: nvidia-smi: command not found\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Clustering/data')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NBX03TZq-Nr","outputId":"a438c29b-829e-4055-d889-1682959bc485"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn-extra\n","  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.23.5)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.2.0)\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.3.0\n","Collecting umap-learn\n","  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n","Collecting pynndescent>=0.5 (from umap-learn)\n","  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n","Building wheels for collected packages: umap-learn, pynndescent\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82813 sha256=f38d2faa33c91a549f4472a62664dc4e9a88424c6c36d638956cbf5cd12e308d\n","  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55621 sha256=d7ef431ae1da977ab341d499f47b1f9273739146b52cb937fc75eddbdedcdb64\n","  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n","Successfully built umap-learn pynndescent\n","Installing collected packages: pynndescent, umap-learn\n","Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n","Collecting git+https://github.com/scikit-learn-contrib/hdbscan.git\n","  Cloning https://github.com/scikit-learn-contrib/hdbscan.git to /tmp/pip-req-build-8rwt2et7\n","  Running command git clone --filter=blob:none --quiet https://github.com/scikit-learn-contrib/hdbscan.git /tmp/pip-req-build-8rwt2et7\n","  Resolved https://github.com/scikit-learn-contrib/hdbscan.git to commit e5e1b8e7a348fc06e99d71d35e72d0e51c066194\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.33) (0.29.36)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.33) (1.23.5)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.33) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.33) (1.2.2)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.33) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan==0.8.33) (3.2.0)\n","Building wheels for collected packages: hdbscan\n"]}],"source":["#!pip install numpy==1.16.0\n","!pip install scikit-learn-extra\n","!pip install umap-learn\n","# !pip install hdbscan\n","!pip install git+https://github.com/scikit-learn-contrib/hdbscan.git\n","!pip install tslearn\n","!pip install geopandas\n","!pip install -q ipywidgets\n","!pip3 install ipympl\n","%load_ext autoreload\n","%autoreload 2\n","!pip install folium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blHmfLEZi-WZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from datetime import datetime\n","import datetime as dt\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.signal import correlate\n","import plotly.graph_objects as go\n","import statsmodels.api as sm\n","from scipy import stats\n","from scipy.stats import iqr\n","from itertools import combinations\n","from sklearn_extra.cluster import KMedoids\n","from sklearn.cluster import DBSCAN\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.manifold import TSNE\n","from umap import UMAP\n","import matplotlib.image as mpimg\n","import hdbscan\n","from sklearn.cluster import AgglomerativeClustering\n","from tslearn.metrics import dtw,lcss\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from tslearn.clustering import TimeSeriesKMeans\n","#from tslearn.clustering import silhouette_score\n","from sklearn.metrics import silhouette_samples,silhouette_score\n","import geopandas as gpd\n","import time\n","from collections import Counter\n","import math\n","from sklearn.manifold import Isomap\n","import pywt\n","import pywt.data\n","import plotly.subplots as sp\n","from ipywidgets import interact, interactive, fixed, interact_manual\n","import ipywidgets as widgets\n","from IPython.display import display\n","from IPython.display import clear_output\n","from mpl_toolkits.mplot3d import Axes3D\n","import plotly.express as px\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from matplotlib.pyplot import get_cmap\n","from matplotlib.pyplot import cm\n","from scipy.spatial import distance\n","from sklearn.cluster import AgglomerativeClustering\n","import scipy.cluster.hierarchy as sch\n","import matplotlib.colors as mcolors\n","import matplotlib.dates as mdates\n","from scipy.signal import find_peaks\n","from statsmodels.tsa.stattools import acf,pacf\n","import folium\n","\n","%matplotlib inline\n","#%matplotlib notebook\n","\"\"\"\n","extract features and cluster feature vectors from all stations.\n","use closest actual station to centroid to be the representative station.\n","Only analyze the representative stations and NOT all.\n","This is a dimensionality reduction technique.\n","\n","\"\"\""]},{"cell_type":"code","source":["num_of_clusters = 66\n","timeslot_duration = 10\n","total_timelots = int(1440/timeslot_duration)\n","os.chdir('/content/drive/MyDrive/Clustering/data')\n","demand = np.load('A_21.npy',allow_pickle=True).reshape((num_of_clusters, num_of_clusters, total_timelots, timeslot_duration)).sum(axis=3)\n","demand = demand.sum(axis=1).T\n","manhat_demand = pd.DataFrame(data=demand,index=pd.date_range(start='2020-01-21 00:00:00', end='2020-01-21 23:50:00', freq='10T'),columns=[ str(i) for i in range(demand.shape[1])])\n","manhat_demand"],"metadata":{"id":"dmjX9IF68rjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtI3CF_Ii-Wd"},"outputs":[],"source":["\"\"\"\n","Visualize demand on a map\n","\"\"\"\n","\n","# housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n","# s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n","# c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n","# )\n","# plt.legend()\n","\n","#TODO: Refernce: https://github.com/fmaletski/nyc-taxi-map/tree/master"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I56_5ZUOPN4G"},"outputs":[],"source":["\"\"\"\n","Exploratory data analysis: Check if n95 is different from Max and that it lies within the IQR of most stations.\n","\"\"\"\n","df = manhat_demand\n","# df = df.drop(columns=['week','hour','weekday'])\n","percentiles_95 = df.quantile(0.95)\n","num_series = len(df.columns)\n","\n","plt.figure(figsize=(20, 20))\n","plt.boxplot(df, positions=np.arange(num_series), vert=False)\n","plt.scatter(percentiles_95, np.arange(num_series), color='red', marker='o', label='95th Percentile')\n","plt.yticks(np.arange(num_series), df.columns)\n","plt.xlabel('Value')\n","plt.ylabel('Series')\n","plt.title(\"Boxplot of station's Time Series Data with 95th Percentile\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","source":["manhattan_zones = pd.read_csv('manhattan_zones.csv')\n","\n","zone_to_gps_dict = {}\n","\n","for i in manhattan_zones.index:\n","    lat = manhattan_zones.iloc[i]['lat']\n","    lng = manhattan_zones.iloc[i]['lng']\n","    zone_to_gps_dict[i] = (lat,lng)\n"],"metadata":{"id":"u0KA3Wa843b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2vkxdRAhbdZ"},"outputs":[],"source":["# def feature_standardization(X_train):\n","#     # ensure that features of different scale are comparable using euclidean distance\n","#     X_train_Df = pd.DataFrame(X_train)\n","#     X_train_normalized =(X_train_Df-X_train_Df.mean())/X_train_Df.std()\n","#     X_train_normalized = X_train_normalized.fillna(0)\n","#     return X_train_normalized.to_numpy()\n","\n","class feat_transformation():\n","    def __init__(self,):\n","        return\n","    def feature_normalization(self,X_train):\n","        # ensure that features of different scale are comparable using euclidean distance\n","        self.scaler = MinMaxScaler() #StandardScaler()\n","        self.scaler.fit(X_train)\n","        X_train_normalized = self.scaler.transform(X_train)\n","        return X_train_normalized\n","\n","    def inv_feature_normalization(self,X_train_normalized):\n","        # ensure that features of different scale are comparable using euclidean distance\n","        X_train = self.scaler.inverse_transform(X_train_normalized)\n","        return X_train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpRjc-JssACp"},"outputs":[],"source":["## Visualize the clustered time series\n","\n","\n","def plot_stations_of_clusters(labels, df,TS_type,X_train,X_train_inv_normalized,plot_type,angular_labels,plot_title,mother_wavelet,limit_level):\n","    unique_clusters = np.unique(labels)\n","    num_clusters = len(unique_clusters)\n","    cluster_to_stations_dict = { l: [] for l in unique_clusters}\n","\n","    cols = df.columns\n","    station_to_index_dict = {}\n","    index=0\n","    if isinstance(cols[index], tuple): # differentiate between Dim_Red_unmet_60min_outflow_week_df DF and unmet_60min_outflow_df DF\n","      for stn in cols: # station to index mapping\n","        station_to_index_dict[stn[0]] = index\n","        index+=1\n","    else:\n","      for stn in cols: # station to index mapping\n","        station_to_index_dict[stn] = index\n","        index+=1\n","\n","    idx=0\n","    if isinstance(cols[idx], tuple): # differentiate between Dim_Red_unmet_60min_outflow_week_df DF and unmet_60min_outflow_df DF\n","      for l in labels:\n","        cluster_to_stations_dict[l].append(cols[idx][0])\n","        idx+=1\n","    else:\n","      for l in labels:\n","        cluster_to_stations_dict[l].append(cols[idx])\n","        idx+=1\n","\n","    #shapefile_data = gpd.read_file('NYC Taxi Zones (1)/taxi_zones.shp',)\n","\n","    num_features = X_train.shape[1]\n","    all_cluster_feat_mins = np.array([])\n","    all_cluster_feat_maxs = np.array([])\n","\n","    ################## par coords plot ##################\n","    if(plot_type == 'par_coords'): # parallel coordinates\n","        for key in cluster_to_stations_dict.keys():\n","\n","          #fig0, ax0 = plt.subplots()\n","          #shapefile_data.plot(ax=ax0,edgecolor='black', linewidth=0.9, color='lightblue',figsize=(5,5))\n","\n","          #fig = go.Figure()\n","          #fig1 = go.Figure()\n","          fig2, ax2 = plt.subplots(figsize=(30,5))\n","          #fig3, ax3 = plt.subplots(figsize=(60,5))\n","          fig4, ax4 = plt.subplots(figsize=(30,5))\n","\n","          color_feature = 0  # The feature index to use for colors\n","          color_map = get_cmap('tab20')  # Choose a colormap\n","\n","          d = {}\n","\n","          feat_min_per_cluster = [1000]*num_features\n","          feat_max_per_cluster = [-1000]*num_features\n","\n","          cluster_data = []\n","          cluster_stns_gps_coords = []\n","\n","          for station in cluster_to_stations_dict[key]:\n","            #ax0.scatter(zone_to_gps_dict[int(station)][0], zone_to_gps_dict[int(station)][1], color='red', marker='.')\n","\n","            cluster_stns_gps_coords.append(zone_to_gps_dict[int(station)])\n","\n","            #y_arr = np.array(df[station].values).reshape(-1)\n","            # fig = fig.add_trace(go.Scatter(x=np.arange(len(y_arr)), y=y_arr,mode='lines+markers',name=station))\n","            # fig.update_layout(title='cluster: '+ str(key),xaxis_title=\"time (Monday 0:00 - Sunday 23:00)\",\n","            # yaxis_title=TS_type,autosize=False,width=1800,height=400, )\n","\n","            # wave_coeffs = pywt.wavedec( df[station] , mother_wavelet, level=limit_level)\n","            # rec_wave = pywt.waverec(wave_coeffs, mother_wavelet)\n","            # ym = np.median(rec_wave)\n","            # ax3.plot(np.linspace(0, 1., num=len(rec_wave)), rec_wave-ym)\n","            # #ax3.plot(df[station])\n","\n","            d['r'] = X_train[station_to_index_dict[station]]\n","            # d['theta'] = angular_labels\n","            # fig1.add_trace(go.Scatterpolar(\n","            #     r=d['r'],\n","            #     theta=d['theta'],\n","            #     fill='toself',name=station,\n","            # ))\n","\n","            sample = X_train[station_to_index_dict[station]]  # Get the current sample\n","            color = color_map(sample[color_feature])  # Get the color based on the color feature value\n","            ax2.plot(range(num_features), sample, color=color, alpha=0.7, label=station)\n","\n","            #unnormalized features\n","            sample1 = X_train_inv_normalized[station_to_index_dict[station]]  # Get the current sample\n","            cluster_data.append(sample1)\n","\n","            # histogram and kernel density estimate\n","            sns.histplot(data=df[station].values, kde=False, stat=\"count\", ax=ax4)\n","            ax5 = ax4.twinx()\n","            sns.kdeplot(data=df[station].values, ax=ax5)\n","\n","            for f in range(num_features):\n","                feat_min_per_cluster[f] = min(feat_min_per_cluster[f],sample[f])\n","                feat_max_per_cluster[f] = max(feat_max_per_cluster[f],sample[f])\n","\n","          all_cluster_feat_mins = np.append(all_cluster_feat_mins,np.array(feat_min_per_cluster))\n","          all_cluster_feat_maxs = np.append(all_cluster_feat_maxs,np.array(feat_max_per_cluster))\n","\n","          cluster_data = np.array(cluster_data)\n","\n","          # fig1.update_layout(title='cluster: '+ str(key),\n","          #     polar=dict(\n","          #         radialaxis=dict(visible=True,))#range=[-2, 7]))\n","          #     ,height=800,width=800,\n","          #     showlegend=True\n","          # )\n","          # fig1.show()\n","\n","          # Create a map centered on the coordinates of interest\n","          map_center = [40.7831, -73.9712]  # Manhattan coordinates (latitude, longitude)\n","          map_zoom = 13  # Adjust the zoom level as needed\n","          my_map = folium.Map(location=map_center, zoom_start=map_zoom,width='40%', height='40%')\n","          for coord in cluster_stns_gps_coords:\n","              folium.Marker(location=coord).add_to(my_map)\n","          # Display the map\n","          display(my_map)\n","\n","          # ax0.set_xlabel('Longitude')\n","          # ax0.set_ylabel('Latitude')\n","          # ax0.set_title('manhattan zones of cluster: '+ str(key))\n","          # fig0.show()\n","\n","          #fig.show()\n","\n","          ax2.set_xticks(range(num_features))\n","          ax2.set_xticklabels([i for i in angular_labels])\n","          ax2.set_ylabel('Normalized Value')\n","          #ax2.set_ylim(0, 1)  # Modify as needed\n","          ax2.set_title(plot_title+', cluster: '+str(key)+' has '+str(len(cluster_to_stations_dict[key]))+' stations')\n","          ax2.grid(True, linestyle='--')\n","          #fig2.tight_layout()\n","          fig2.show()\n","\n","          # #ax3.set_xticklabels([i for i in unmet_60min_outflow_df.index])\n","          # plt.xticks(rotation=90)\n","          # ax3.set_ylabel('Low frequency, '+'level: '+ str(limit_level) + ', '+ mother_wavelet +' Wavelet reconstruction')\n","          # #ax3.set_ylim(0, 1)  # Modify as needed\n","          # ax3.set_title(plot_title+', cluster: '+str(key)+' has '+str(len(cluster_to_stations_dict[key]))+' stations')\n","          # #ax3.legend()\n","          # ax3.grid(True, linestyle='--')\n","          # #fig3.tight_layout()\n","          # fig3.show()\n","\n","\n","          cluster_df = pd.DataFrame(cluster_data, columns=angular_labels)\n","          cluster_df['sample'] = range(len(cluster_df))\n","          fig6 = px.parallel_coordinates(cluster_df,color='sample',color_continuous_scale=\"picnic\",dimensions=cluster_df.columns[:-1],)\n","          fig6.update_layout(\n","              plot_bgcolor='black',  # Set the background color to black\n","              paper_bgcolor='black',  # Set the plot area background color to black\n","              font_color='white',  # Set the font color to white for better visibility\n","              title='unnormalized features of cluster: '+str(key) )\n","          fig6.show()\n","\n","          # fig7 = go.Figure(go.Parcoords(\n","          #     line=dict(color=cluster_data[:, 0], showscale=True),\n","          #     dimensions=[dict(range=[min(cluster_data[:, i]), max(cluster_data[:, i])], label=l, values=cluster_data[:, i]) for i,l in zip( range(len(angular_labels)),angular_labels)],\n","          # ))\n","          # fig7.update_layout(title='unnormalized stats features', coloraxis_colorbar=dict(title='Color'))\n","          # fig7.show()\n","\n","          ax4.set_xlabel('Demand value')\n","          ax4.set_ylabel('Density/Frequency')\n","          ax4.set_title('station''s Kernel Density Plots of cluster: '+str(key))\n","          ax4.legend()\n","          fig4.show()\n","\n","          print('Number of stations in cluster '+ str(key)+' : ', len(cluster_to_stations_dict[key]))\n","          print('Stations in cluster: ',cluster_to_stations_dict[key])\n","    ################## par coords plot ##################\n","\n","\n","    ################## polar plot ##################\n","\n","    elif(plot_type == 'polar'):\n","        angular_labels = [ str(l) for l in range( X_train.shape[1] )]\n","        for key in cluster_to_stations_dict.keys():\n","          fig0, ax = plt.subplots(figsize=(10,5))\n","          shapefile_data.plot(ax=ax,edgecolor='black', linewidth=0.9, color='lightblue',figsize=(5,5))\n","\n","          #fig = go.Figure()\n","          fig1 = go.Figure()\n","\n","          d = {}\n","\n","          feat_min_per_cluster = [1000]*num_features\n","          feat_max_per_cluster = [-1000]*num_features\n","\n","          for station in cluster_to_stations_dict[key]:\n","            ax.scatter(zone_to_gps_dict[int(station)][0], zone_to_gps_dict[int(station)][1], color='red', marker='.')\n","\n","            y_arr = np.array(df[station].values).reshape(-1)\n","            # fig = fig.add_trace(go.Scatter(x=np.arange(len(y_arr)), y=y_arr,mode='lines+markers',name=station))\n","            # fig.update_layout(title='cluster: '+ str(key),xaxis_title=\"time (Monday 0:00 - Sunday 23:00)\",\n","            # yaxis_title=TS_type,autosize=False,width=1800,height=400, )\n","\n","            d['r'] = X_train[station_to_index_dict[station]]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=station,\n","            ))\n","\n","            sample = X_train[station_to_index_dict[station]]  # Get the current sample\n","            for f in range(num_features):\n","                feat_min_per_cluster[f] = min(feat_min_per_cluster[f],sample[f])\n","                feat_max_per_cluster[f] = max(feat_max_per_cluster[f],sample[f])\n","\n","          all_cluster_feat_mins = np.append(all_cluster_feat_mins,np.array(feat_min_per_cluster))\n","          all_cluster_feat_maxs = np.append(all_cluster_feat_maxs,np.array(feat_max_per_cluster))\n","\n","          fig1.update_layout(title='cluster: '+ str(key),\n","              polar=dict(\n","                  radialaxis=dict(visible=True,))#range=[-2, 7]))\n","              ,height=1000,width=1000,\n","              showlegend=True\n","          )\n","\n","          ax.set_xlabel('Longitude')\n","          ax.set_ylabel('Latitude')\n","          ax.set_title('Singapore Stations of cluster '+ str(key))\n","          fig0.show()\n","          #fig.show()\n","          fig1.show()\n","\n","          print('Number of stations in cluster '+ str(key)+' : ', len(cluster_to_stations_dict[key]))\n","          print('Stations in cluster: ',cluster_to_stations_dict[key])\n","    ################## polar plot ##################\n","    else:\n","        raise ValueError('only par_coords or are polar allowed')\n","\n","    all_cluster_feat_mins = np.hstack(all_cluster_feat_mins)\n","    all_cluster_feat_maxs = np.hstack(all_cluster_feat_maxs)\n","    print(all_cluster_feat_mins)\n","    return cluster_to_stations_dict,all_cluster_feat_mins,all_cluster_feat_maxs\n","\n","#plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'outflow')\n","\n","def plot_silhouette_scores(X_train,labels,metric,linkage,clustering_type):\n","    unique_clusters = np.unique(labels)\n","    num_clusters = len(unique_clusters)\n","    sample_silhouette_values = silhouette_samples(X=X_train, labels=labels,metric=metric)\n","    silhouette_avg = silhouette_score(X=X_train, labels=labels,metric=metric)\n","    fig, ax1 = plt.subplots()\n","    y_lower = 10\n","    mean_num_zones_dev_in_clusters = []\n","\n","    mean_num_zones_in_clusters = X_train.shape[0] /num_clusters\n","\n","    for i in range(num_clusters): # exclude the\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n","        ith_cluster_silhouette_values.sort()\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","        color = cm.nipy_spectral(float(i) / num_clusters)\n","        ax1.fill_betweenx(\n","            np.arange(y_lower, y_upper),\n","            0,\n","            ith_cluster_silhouette_values,\n","            facecolor=color,\n","            edgecolor=color,\n","            alpha=0.7,\n","        )\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n","        mean_num_zones_dev_in_clusters.append(abs(mean_num_zones_in_clusters - np.sum(labels == i) ))\n","\n","    ax1.set_title(\"metric: \"+metric+\", linkage: \"+linkage+\", silhouette_avg: \"+str(silhouette_avg)+\", num_clusters: \"+str(num_clusters)+\", Cluster_type : \"+clustering_type )\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    num_neg_silhouette_scores = np.sum(sample_silhouette_values < 0 )\n","    min_silhouette_score = min(sample_silhouette_values)\n","    dev_from_mean_num_zone = np.sum(np.array(mean_num_zones_dev_in_clusters))\n","    return silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone\n","\n","\n","def plot_cluster_representatives(X_train,all_cluster_feat_mins,all_cluster_feat_maxs,plot_type,angular_labels):\n","    num_features = X_train.shape[1]\n","    low_list =  all_cluster_feat_mins[num_features:] ## remove outlier cluster\n","    high_list = all_cluster_feat_maxs[num_features:] ## remove outlier cluster\n","    num_clusters = int(len(all_cluster_feat_mins[num_features:] )/num_features) ## remove outlier cluster\n","    color = cm.rainbow(np.linspace(0, 1, num_clusters))\n","\n","    low = {}\n","    high = {}\n","    c = 0\n","    idx=0\n","    low[0] = []\n","    high[0] = []\n","    for l,h in zip(low_list,high_list):\n","        low[c].append(l)\n","        high[c].append(h)\n","        idx+=1\n","        if idx%num_features==0:\n","            c+=1\n","            low[c] = []\n","            high[c] = []\n","\n","\n","    if(plot_type == 'par_coords'): # parallel coordinates\n","        plt.close('all')\n","\n","        #fig,ax = plt.subplots(figsize=(20,7))\n","\n","        cluster_rep_arr = np.empty((num_clusters,num_features))\n","        adj=0\n","        for c in range(num_clusters):\n","            for f in range(num_features):\n","              cluster_rep_arr[c,f] = np.average([low[c][f],high[c][f]])\n","              #ax.plot([f+adj]*2,[low[c][f],high[c][f]],c=color[c],marker='*',label=str(c))\n","            adj += 0.05\n","\n","        # ax.set_xticks(range(num_features))\n","        # ax.set_xticklabels([i for i in angular_labels])\n","        # ax.set_title('Representative points from each cluster for '+str(num_clusters) + ' clusters '+ 'excluding outlier cluster')\n","        # ax.set_xlabel('TS stats features')\n","        # ax.set_ylabel('Normalized value')\n","        # ax.grid(True, linestyle='--')\n","        # ax.legend()\n","        # fig.show()\n","\n","\n","        # ####### TREND statistical Features #######\n","        # cluster_rep_arr[:,4] = 1- cluster_rep_arr[:,4] # invert\n","        # cluster_rep_arr[:,5] = 1- cluster_rep_arr[:,5] # invert\n","        # cluster_rep_arr[:,6] = 1- cluster_rep_arr[:,6] # invert\n","        # angular_labels[4] = '(1 - norm zero counts)'\n","        # angular_labels[5] = '(1 - norm kurtosis)'\n","        # angular_labels[6] = '(1 - norm skewness)'\n","        # ####### TREND statistical Features #######\n","\n","\n","        df = pd.DataFrame(cluster_rep_arr, columns=angular_labels)\n","        df['sample'] = range(len(df))\n","        fig2 = px.parallel_coordinates(df,color='sample',color_continuous_scale=\"picnic\",dimensions=df.columns[:-1],)\n","        fig2.update_layout(\n","            plot_bgcolor='black',\n","            paper_bgcolor='black',\n","            font_color='white',\n","            title='Representative points from each cluster. (excluding outlier cluster)'\n","        )\n","        fig2.show()\n","\n","\n","    elif(plot_type == 'polar'): # polar plot\n","        plt.close('all')\n","        angular_labels = [ str(l) for l in range( X_train.shape[1] )]\n","\n","        cmap = cm.get_cmap('tab20c')\n","        colors = [mcolors.rgb2hex(cmap(i)[:3]) for i in range(num_clusters)]\n","\n","        d = {}\n","        fig1 = go.Figure()\n","\n","        for c in range(num_clusters):\n","            color = colors[c % num_clusters]\n","            d['r'] = low[c]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=str(c)+'_low',\n","                line=dict(color=color),\n","            ))\n","            d['r'] = high[c]\n","            d['theta'] = angular_labels\n","            fig1.add_trace(go.Scatterpolar(\n","                r=d['r'],\n","                theta=d['theta'],\n","                fill='toself',name=str(c)+'_high',\n","                line=dict(color=color),\n","            ))\n","\n","        fig1.update_layout(title='Representative DWT points from each cluster. (excluding outlier cluster)',\n","            polar=dict(\n","                radialaxis=dict(visible=True,))#range=[-2, 7]))\n","            ,height=1000,width=1000,\n","            showlegend=True,\n","        )\n","        fig1.show()\n","    return\n","\n","def rank_clusterings(all_cluster_feat_mins,all_cluster_feat_maxs,num_clusters,num_features):\n","\n","  all_cluster_feat_mins = all_cluster_feat_mins.reshape(num_clusters,num_features)\n","  all_cluster_feat_maxs = all_cluster_feat_maxs.reshape(num_clusters,num_features)\n","\n","  mean_arr = np.empty((num_clusters,num_features))\n","  deviation_arr = np.empty((num_clusters,num_features))\n","\n","  for c in range(num_clusters):\n","    for f in range(num_features):\n","      mean = np.mean((all_cluster_feat_mins[c,f],all_cluster_feat_maxs[c,f])) #ignore outlier cluster\n","      deviation = (all_cluster_feat_maxs[c,f] - all_cluster_feat_mins[c,f])/2\n","      mean_arr[c,f] = mean\n","      deviation_arr[c,f] = deviation\n","\n","  deviation_arr = 1 - deviation_arr\n","  weights_arr = (deviation_arr.T / deviation_arr.sum(axis=1)).T\n","  mean_arr = mean_arr*weights_arr\n","\n","  cluster_magnitudes = np.linalg.norm(mean_arr,2,axis=1)\n","  ranked_clusters = np.argsort(cluster_magnitudes)\n","  ranked_clusters = { i:r for i,r in enumerate(ranked_clusters)}\n","  return ranked_clusters\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpfVHs5QyKxj"},"outputs":[],"source":["\n","def compute_mahalanobis_dist_mat(X):\n","    # Calculate the covariance matrix\n","    cov_matrix = np.cov(X.T)\n","    reg_param = 0.01  # Regularization parameter\n","    # Calculate the inverse of the covariance matrix\n","    inv_cov_matrix = np.linalg.inv(cov_matrix + reg_param * np.eye(cov_matrix.shape[0]))\n","    #inv_cov_matrix = np.linalg.inv(cov_matrix)\n","    # Calculate the Mahalanobis distance matrix\n","    mahalanobis_dist_mat = distance.cdist(X, X, metric='mahalanobis', VI=inv_cov_matrix)\n","    return mahalanobis_dist_mat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UptRw6ygsUsN"},"outputs":[],"source":["class plot_all():\n","    def __init__(self,print_msg,X_train):\n","        self.print_msg = print_msg\n","        self.X_train = X_train\n","        return\n","\n","    def plot_tSNE_2d(self,X_train,print_msg,ax,col):\n","        ## t-SNE\n","        plot_kwds = {'alpha' : 0.25, 's' : 10, 'linewidths':0}\n","        projection = TSNE(n_components=2).fit_transform(X_train)\n","        ax[col].scatter(*projection.T, **plot_kwds)\n","        ax[col].set_title('t-SNE, '+ print_msg)\n","        #plt.show()\n","        return ax\n","\n","    def plot_UMAP_2d(self,X_train,print_msg):\n","        umap_2d = UMAP(n_components=2, init='random', random_state=0)\n","        proj_2d = umap_2d.fit_transform(X_train)\n","        fig = go.Scatter(\n","            x=proj_2d[:, 0],  # X-axis values\n","            y=proj_2d[:, 1],  # Y-axis values\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ), name=print_msg\n","        )\n","        #fig.update_layout(title=print, scene=dict(xaxis_title='Dimension 1', yaxis_title='Dimension 2', zaxis_title='Dimension 3'))\n","        return fig\n","\n","\n","    def plot_UMAP_3d(self,X_train,n_neighbors,min_dist,metric):\n","        umap_3d = UMAP(n_components=3,n_neighbors=n_neighbors,min_dist=min_dist,metric=metric, init='random', random_state=0)\n","        proj_3d = umap_3d.fit_transform(X_train)\n","        fig = go.Scatter3d(\n","            x=proj_3d[:, 0],  # X-axis values from the low-dimensional representation\n","            y=proj_3d[:, 1],  # Y-axis values from the low-dimensional representation\n","            z=proj_3d[:, 2],  # Z-axis values from the low-dimensional representation\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ),name=self.print_msg\n","        )\n","        #fig.show()\n","        return fig\n","\n","    def plot_UMAP_interactive(self,n_neighbors,min_dist,metric):\n","        umap_3d = UMAP(n_components=3,n_neighbors=n_neighbors,min_dist=min_dist,metric=metric, init='random', random_state=0)\n","        proj_3d = umap_3d.fit_transform(self.X_train )\n","        x=proj_3d[:,0]\n","        y=proj_3d[:,1]\n","        z=proj_3d[:,2]\n","        # Create the figure and subplots\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n","\n","        # Plot 1 - 2D Plot\n","        ax1.scatter(x, y,c=np.arange(len(self.X_train )))\n","        ax1.set_xlabel('X')\n","        ax1.set_ylabel('Y')\n","        ax1.set_title(self.print_msg)\n","\n","        # Plot 2 - 3D Plot\n","        ax2 = fig.add_subplot(122, projection='3d')\n","        ax2.scatter(x, y, z, c=np.arange(len(self.X_train )))\n","        ax2.set_xlabel('X')\n","        ax2.set_ylabel('Y')\n","        ax2.set_zlabel('Z')\n","        ax2.set_title(self.print_msg)\n","\n","        # Adjust the spacing between subplots\n","        plt.subplots_adjust(wspace=0.5)\n","\n","        # Display the plot\n","        plt.show()\n","        return\n","\n","\n","    def plot_ISOMAP_2d(self,X_train,print_msg,n_neighbors=5):\n","        ## isomap\n","        # Create an instance of the Isomap class and specify the desired parameters\n","        n_components = 2  # Number of dimensions in the low-dimensional representation\n","        # Number of nearest neighbors used in constructing the neighborhood graph\n","        isomap = Isomap(n_components=n_components, n_neighbors=n_neighbors)\n","        # Fit the Isomap model to your data\n","        isomap_embedding = isomap.fit_transform(X_train)\n","        # Access the low-dimensional representation\n","        low_dim_data = isomap_embedding\n","        fig = go.Scatter(\n","            x=low_dim_data[:, 0],  # X-axis values\n","            y=low_dim_data[:, 1],  # Y-axis values\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ), name=print_msg\n","        )\n","        #fig.update_layout(title=print, scene=dict(xaxis_title='Dimension 1', yaxis_title='Dimension 2', zaxis_title='Dimension 3'))\n","        #fig.show()\n","        return fig\n","\n","    def plot_ISOMAP_3d(self,X_train,print_msg,n_neighbors=5):\n","        ## isomap\n","        # Create an instance of the Isomap class and specify the desired parameters\n","        n_components = 3  # Number of dimensions in the low-dimensional representation\n","        n_neighbors = 5  # Number of nearest neighbors used in constructing the neighborhood graph\n","        isomap = Isomap(n_components=n_components, n_neighbors=n_neighbors)\n","        # Fit the Isomap model to your data\n","        isomap_embedding = isomap.fit_transform(X_train)\n","        # Access the low-dimensional representation\n","        low_dim_data = isomap_embedding\n","        # Create a 3D scatter plot using Plotly\n","        fig = go.Scatter3d(\n","            x=low_dim_data[:, 0],  # X-axis values from the low-dimensional representation\n","            y=low_dim_data[:, 1],  # Y-axis values from the low-dimensional representation\n","            z=low_dim_data[:, 2],  # Z-axis values from the low-dimensional representation\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color=np.arange(len(X_train)),  # Color points based on a range of values (e.g., sample index)\n","                colorscale='Viridis',  # Choose a colorscale\n","                showscale=True  # Show the colorbar\n","            ),name=print_msg\n","        )\n","        #fig.show()\n","        return fig\n","\n","    def plot_hist_n_corr(self,X_train):\n","        X_train_df = pd.DataFrame(X_train)\n","        ######### histogram plot #########\n","        # check to see if any features are all zeros\n","        print(self.print_msg+' individual feature Histogram')\n","        X_train_df.hist(figsize=(10,10))\n","        plt.show()\n","        # ######### plot correlation #########\n","        # # perform dimensionality reduction by removing highly correlated features.\n","        X_train_df_corr = X_train_df.corr()\n","        X_train_corr = X_train_df_corr.to_numpy()\n","        fig, ax = plt.subplots(figsize=(15,15))\n","        cax = ax.matshow(X_train_corr, cmap='coolwarm')\n","        cbar = fig.colorbar(cax)\n","        # Loop over the data and add text annotations\n","        for i in range(X_train_corr.shape[0]):\n","            for j in range(X_train_corr.shape[1]):\n","                text = ax.text(j, i, f'{X_train_corr[i, j]:.3f}', ha='center', va='center')\n","        ax.set_title(self.print_msg+' features correlation matrix')\n","        plt.show()\n","        corr_threshold = 0.95 ## Warn multicollinearity\n","        if ((X_train_corr.any() >= corr_threshold) and (X_train_corr.shape[0] > 1) and (X_train_corr.any() <= 0.9999) ):\n","            warnings.warn(\"Warning: some correlation is higher than \"+str(corr_threshold))\n","        h = np.histogram(X_train_df.dropna())\n","        if  ( ( (  np.count_nonzero(h[0]) ) / h[0].shape[0]) < (1/3)):\n","            warnings.warn(\"Warning: highly skewed feature, (provies no information!) \")\n","        return"]},{"cell_type":"code","source":["h = np.histogram(np.array([2,5,4,6,4,7,1,2,3]))\n","\n"],"metadata":{"id":"5fJOdIKTU0KB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOuAuC_VAPRo"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","Feature extracted: Interpretable Time series statistical features\n","\n","1) High/Low average demand for each day of 7 days\n","\n","\n","\"\"\"\n","\n","def select_days_in_dataframe(df,days_type='all_days'):\n","    if days_type == 'all_days':\n","        return df\n","    elif days_type == 'week_days':\n","        df = df[ (df['weekday'] == 'Monday' ) | (df['weekday'] == 'Tuesday' ) | (df['weekday'] == 'Wednesday' ) | (df['weekday'] == 'Thursday' ) ]\n","        return df\n","    elif days_type == 'week_ends':\n","        df = df[ (df['weekday'] == 'Friday' ) | (df['weekday'] == 'Saturday' ) | (df['weekday'] == 'Sunday' ) ]\n","        return df\n","    else:\n","      raise ValueError('Must be one of the 3: \"\"all_days\"\" or \"\"week_days\"\" or \"\"week_ends\"\"')\n","\n","\n","#######################################################################################################################\n","\"\"\" TREND STATISTICAL FEATURES functions\"\"\"\n","def calculate_mean_crossings(list_values):\n","    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) >= np.nanmean(list_values)))[0]\n","    no_mean_crossings = len(mean_crossing_indices)\n","    return no_mean_crossings\n","\n","def calculate_max_deviation_from_n95(list_values):\n","    return np.max(list_values) - np.percentile(list_values,0.95)\n","\n","def calculate_zero_demands(list_values):\n","    return np.where(list_values==0)[0].size\n","\n","def median_of_lists(list_of_lists):\n","    combined_lists  = np.array([sublist for sublist in list_of_lists])\n","    medians = np.median(combined_lists,axis=0)\n","    return medians\n","\n","def no_mean_crossing_demand(df):\n","    return df.apply(calculate_mean_crossings,axis=0)\n","\n","def average_demand(df):\n","    #use this method instead of sample mean over all sundays. because as sample increases mean goes to zero\n","    return df.mean(axis=0)\n","\n","def peak_n95_demand(df):\n","    return df.quantile(q=0.95,axis=0)\n","\n","def max_demand(df):\n","    return df.max(axis=0)\n","\n","def outlier_demands(df):\n","    return df.apply(calculate_max_deviation_from_n95,axis=0)\n","\n","def count_zero_demands(df):\n","    return df.apply(calculate_zero_demands,axis=0)\n","\n","def kurt_of_demands(df):\n","    return df.apply(stats.kurtosis,axis=0)\n","\n","def skew_of_demands(df):\n","    return df.skew(axis=0)\n","\n","def total_demands(df):\n","    return df.sum(axis=0)\n","\n","## all are zero ##\n","# def mode_demands(df):\n","#     df = df.drop(columns=['week','hour','weekday'])\n","#     print(df.mode(axis=0))\n","#     return df.mode(axis=0)\n","\n","def range_demands(df):\n","    return df.max(axis=0) - df.min(axis=0)\n","\"\"\" TREND STATISTICAL FEATURES functions\"\"\"\n","#######################################################################################################################\n","\n","\n","\n","#######################################################################################################################\n","\"\"\" SEASONALITY STATISTICAL FEATURES functions\"\"\"\n","\n","def std_demand(df):\n","    return df.std(axis=0)\n","\n","def calculate_entropy(list_values):\n","    counter_values = Counter(list_values).most_common()\n","    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n","    entropy= stats.entropy(probabilities)\n","    return entropy\n","\n","def entropy_demands(df):\n","    return df.apply(calculate_entropy,axis=0)\n","\n","def calculate_positive_peak_autocorrelation_of_demand(list_values):\n","    view_lags = len(list_values)\n","    feat_len = 5\n","    acf1 = acf(list_values, nlags=view_lags)\n","    peak_idx_high = np.array((find_peaks(acf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(acf1)))\n","    signific_high_idx = np.where(acf1[peak_idx_high] > 1.96/np.sqrt(len(x)))\n","    positive_autocorr_lags = peak_idx_high[signific_high_idx][:feat_len]\n","    positive_autocorr_lags = np.concatenate((positive_autocorr_lags,np.zeros(feat_len-len(positive_autocorr_lags))))\n","    return list(positive_autocorr_lags)\n","\n","def calculate_negative_peak_autocorrelation_of_demand(list_values):\n","    view_lags = len(list_values)\n","    feat_len = 5\n","    acf1 = acf(list_values, nlags=view_lags)\n","    peak_idx_low = np.array((find_peaks(-1*acf1,height=(-0.7,0.7))[0]),dtype=int)\n","    x = list(range(0,len(acf1)))\n","    signific_low_idx = np.where(acf1[peak_idx_low] < (-1*1.96)/np.sqrt(len(x)))\n","    negative_autocorr_lags = peak_idx_low[signific_low_idx][:feat_len]\n","    negative_autocorr_lags = np.concatenate((negative_autocorr_lags,np.zeros(feat_len-len(negative_autocorr_lags))))\n","    return list(negative_autocorr_lags)\n","\n","# def calculate_positive_peak_partial_autocorrelation_of_demand(list_values):\n","#     view_lags = int(len(list_values)/2) - 1\n","#     feat_len = 10\n","#     pacf1 = pacf(list_values, nlags=view_lags)\n","#     peak_idx_high = np.array((find_peaks(pacf1,height=(-0.7,0.7))[0]),dtype=int)\n","#     x = list(range(0,len(pacf1)))\n","#     signific_high_idx = np.where(pacf1[peak_idx_high] > 1.96/np.sqrt(len(x)))\n","#     positive_partial_autocorr_lags = peak_idx_high[signific_high_idx][:feat_len]\n","#     positive_partial_autocorr_lags = np.concatenate((positive_partial_autocorr_lags,np.zeros(feat_len-len(positive_partial_autocorr_lags))))\n","#     return list(positive_partial_autocorr_lags)\n","\n","# def calculate_negative_peak_partial_autocorrelation_of_demand(list_values):\n","#     view_lags = int(len(list_values)/2) - 1\n","#     feat_len = 10\n","#     pacf1 = pacf(list_values, nlags=view_lags)\n","#     peak_idx_low = np.array((find_peaks(-1*pacf1,height=(-0.7,0.7))[0]),dtype=int)\n","#     x = list(range(0,len(pacf1)))\n","#     signific_low_idx = np.where(pacf1[peak_idx_low] < (-1*1.96)/np.sqrt(len(x)))\n","#     negative_partial_autocorr_lags = peak_idx_low[signific_low_idx][:feat_len]\n","#     negative_partial_autocorr_lags = np.concatenate((negative_partial_autocorr_lags,np.zeros(feat_len-len(negative_partial_autocorr_lags))))\n","#     return list(negative_partial_autocorr_lags)\n","\n","def demands_positive_autocorr_lags(df):\n","    #df['Date'] = df.index\n","    daily_pos_autocorr_lags = df.apply(calculate_positive_peak_autocorrelation_of_demand, axis=0)\n","    return daily_pos_autocorr_lags.values.T\n","\n","def demands_negative_autocorr_lags(df):\n","    #df['Date'] = df.index\n","    daily_neg_autocorr_lags = df.apply(calculate_negative_peak_autocorrelation_of_demand, axis=0)\n","    return daily_neg_autocorr_lags.values.T\n","\n","# def demands_positive_partial_autocorr_lags(df):\n","#     #df['Date'] = df.index\n","#     daily_pos_partial_autocorr_lags = df.apply(calculate_positive_peak_partial_autocorrelation_of_demand, axis=0)\n","#     return daily_pos_partial_autocorr_lags.values.T\n","\n","# def demands_negative_partial_autocorr_lags(df):\n","#     #df['Date'] = df.index\n","#     daily_neg_partial_autocorr_lags = df.apply(calculate_negative_peak_partial_autocorrelation_of_demand, axis=0)\n","#     return daily_neg_partial_autocorr_lags.values.T\n","\n","def fft_of_demand(df):\n","    stations = df.columns\n","    num_common_freq=180\n","    top_x_frequencies = 11\n","    freq_rounding_num = 6\n","    common_freq_set = set()\n","    X_train = np.empty(shape=(0,top_x_frequencies-1)) # removed dc\n","\n","    for station in stations:\n","        # Perform the FFT on the data\n","        fft_result = np.fft.fft(df[station].values)\n","        # Compute the magnitudes of the FFT result\n","        fft_magnitudes = np.abs(fft_result)\n","        # Generate the corresponding frequency axis\n","        sampling_rate = 1.0  # Assuming a sampling rate of 1.0 (change as needed)\n","        frequency_axis = np.fft.fftfreq(len(fft_result), 1 / sampling_rate)\n","        positive_freq_indices = np.where(frequency_axis >= 0)\n","\n","        \"\"\" TOP periods/frequencies in the time series\"\"\"\n","        # Sort the magnitudes and frequencies in descending order\n","        sorted_indices = np.argsort(fft_magnitudes[positive_freq_indices])[::-1]\n","        sorted_magnitudes = fft_magnitudes[positive_freq_indices][sorted_indices]\n","        sorted_frequencies = frequency_axis[positive_freq_indices][sorted_indices]\n","        # Select the top periods\n","\n","        top_freq = sorted_frequencies[2:top_x_frequencies+1] # REMOVE dc/mean component\n","        top_periods = 1/top_freq\n","        top_periods = [round(p) for p in top_periods]\n","        #print(f'Top FFT periods in demand are : {top_periods}')\n","        X_train = np.vstack((X_train,top_periods))\n","    return X_train\n","\"\"\" SEASONALITY STATISTICAL FEATURES functions\"\"\"\n","#######################################################################################################################\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjDl0_mv4P9q"},"outputs":[],"source":["\"\"\"                                                                     FEATURE Extraction\n","Feature extracted: Interpretable Time series statistical features\n","\n","1) High/Low average demand for each day of 7 days\n","\n","\n","\"\"\"\n","\n","################## Select one of \"\"all_days\"\" or \"\"week_days\" (Mon,Tue,Wed,Thur) or \"week_ends\" (Fri,Sat,Sun) to generate trend/seasonality statistical features ##################\n","\"\"\" Select DAYS of the Dataframe to use\"\"\"\n","zone_met_10m_outflow_df = select_days_in_dataframe(manhat_demand,days_type='all_days')\n","################## Select one of \"\"all_days\"\" or \"\"week_days\" (Mon,Tue,Wed,Thur) or \"week_ends\" (Fri,Sat,Sun) to generate trend/seasonality statistical features ##################\n","\n","\n","\n","######################################################################################################################\n","\"\"\" TREND STATISTICAL FEATURES calculation\"\"\"\n","\n","avg_dem = average_demand(zone_met_10m_outflow_df)\n","avg_dem_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_avg_dem = avg_dem_ts_stats_feat_transform.feature_normalization(  np.array([avg_dem.to_numpy()]).T  )\n","print_msg = 'avg_dem'\n","plot_all(print_msg,X_train_normalized_avg_dem).plot_hist_n_corr(X_train_normalized_avg_dem)\n","X_train_inv_normalized_avg_dem = avg_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_avg_dem)\n","# print_msg = 'every_days_avg_dem_non_normalized'\n","# plot_all(print_msg,X_train_inv_normalized_avg_dem).plot_hist_n_corr(X_train_inv_normalized_avg_dem)\n","\n","no_mean_cross_dem = no_mean_crossing_demand(zone_met_10m_outflow_df)\n","num_mean_cross_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_mean_cross = num_mean_cross_ts_stats_feat_transform.feature_normalization(  np.array([no_mean_cross_dem.to_numpy()]).T  )\n","print_msg = 'no_mean_crossing_dem'\n","plot_all(print_msg,X_train_normalized_mean_cross).plot_hist_n_corr(X_train_normalized_mean_cross)\n","X_train_inv_normalized_mean_cross = num_mean_cross_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_mean_cross)\n","\n","n95_dem = peak_n95_demand(zone_met_10m_outflow_df)\n","n95_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_n95 = n95_ts_stats_feat_transform.feature_normalization(  np.array([n95_dem.to_numpy()]).T   )\n","print_msg = 'n95_dem'\n","plot_all(print_msg,X_train_normalized_n95).plot_hist_n_corr(X_train_normalized_n95)\n","X_train_inv_normalized_n95 = n95_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_n95)\n","\n","max_dem = max_demand(zone_met_10m_outflow_df)\n","max_dem_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_max = max_dem_ts_stats_feat_transform.feature_normalization(    np.array([max_dem.to_numpy()]).T  )\n","print_msg = 'max_dem'\n","plot_all(print_msg,X_train_normalized_max).plot_hist_n_corr(X_train_normalized_max)\n","X_train_inv_normalized_max = max_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_max)\n","\n","max_deviation_from_n95 = outlier_demands(zone_met_10m_outflow_df)\n","max_dev_n95_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_max_dev_n95 = max_dev_n95_ts_stats_feat_transform.feature_normalization(  np.array([max_deviation_from_n95.to_numpy()]).T   )\n","print_msg = 'max_dev_from_n95_dem'\n","plot_all(print_msg,X_train_normalized_max_dev_n95).plot_hist_n_corr(X_train_normalized_max_dev_n95)\n","X_train_inv_normalized_max_dev_n95 = max_dev_n95_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_max_dev_n95)\n","\n","zero_dem = count_zero_demands(zone_met_10m_outflow_df)\n","zero_dem_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_zeros = zero_dem_ts_stats_feat_transform.feature_normalization(   np.array([zero_dem.to_numpy()]).T   )\n","print_msg = 'zero_dem'\n","plot_all(print_msg,X_train_normalized_zeros).plot_hist_n_corr(X_train_normalized_zeros)\n","X_train_inv_normalized_zeros = zero_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_zeros)\n","\n","\n","kurt = kurt_of_demands(zone_met_10m_outflow_df)\n","kurt_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_kurt = kurt_ts_stats_feat_transform.feature_normalization(   np.array([kurt.to_numpy()]).T  )\n","print_msg = 'kurt_dem'\n","plot_all(print_msg,X_train_normalized_kurt).plot_hist_n_corr(X_train_normalized_kurt)\n","X_train_inv_normalized_kurt = kurt_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_kurt)\n","\n","skew = skew_of_demands(zone_met_10m_outflow_df)\n","skew_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_skew = skew_ts_stats_feat_transform.feature_normalization(   np.array([skew.to_numpy()]).T  )\n","print_msg = 'skew_dem'\n","plot_all(print_msg,X_train_normalized_skew).plot_hist_n_corr(X_train_normalized_skew)\n","X_train_inv_normalized_skew = skew_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_skew)\n","\n","\n","tot_dem = total_demands(zone_met_10m_outflow_df)\n","tot_dem_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_tot_dem = tot_dem_ts_stats_feat_transform.feature_normalization(   np.array([tot_dem.to_numpy()]).T  )\n","print_msg = 'tot_dem'\n","plot_all(print_msg,X_train_normalized_tot_dem).plot_hist_n_corr(X_train_normalized_tot_dem)\n","X_train_inv_normalized_tot_dem = tot_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_tot_dem)\n","\n","range_dem = range_demands(zone_met_10m_outflow_df)\n","range_dem_ts_stats_feat_transform = feat_transformation()\n","X_train_normalized_range_dem = range_dem_ts_stats_feat_transform.feature_normalization(   np.array([range_dem.to_numpy()]).T  )\n","print_msg = 'range_dem'\n","plot_all(print_msg,X_train_normalized_range_dem).plot_hist_n_corr(X_train_normalized_range_dem)\n","X_train_inv_normalized_range_dem = range_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_range_dem)\n","\n","\n","# X_train_normalized_all_trend_statistic_features = np.concatenate((X_train_normalized_avg_dem,X_train_normalized_mean_cross,X_train_normalized_n95,X_train_normalized_max,\\\n","#                                                                   X_train_normalized_max_dev_n95,X_train_normalized_zeros,X_train_normalized_kurt,\\\n","#                                                                   X_train_normalized_tot_dem,X_train_normalized_range_dem,X_train_normalized_skew),axis=1)\n","#### After removing correlated features\n","X_train_normalized_all_trend_statistic_features = np.concatenate((X_train_normalized_avg_dem,X_train_normalized_mean_cross,X_train_normalized_n95,\\\n","                                                                  X_train_normalized_zeros,X_train_normalized_skew),axis=1)\n","\n","X_train_inverse_normalized_all_trend_statistic_features = np.concatenate((X_train_inv_normalized_avg_dem,X_train_inv_normalized_mean_cross,X_train_inv_normalized_n95,\\\n","                                                                  X_train_inv_normalized_zeros,X_train_inv_normalized_skew),axis=1)\n","\n","print_msg = 'all_trend_statistics features'\n","plot_all(print_msg,X_train_normalized_all_trend_statistic_features).plot_hist_n_corr(X_train_normalized_all_trend_statistic_features)\n","\n","\n","\"\"\" TREND STATISTICAL FEATURES calculation\"\"\"\n","######################################################################################################################\n","\n","\n","\n","\n","\n","\n","\n","# #######################################################################################################################\n","# \"\"\" SEASONALITY STATISTICAL FEATURES calculation\"\"\"\n","\n","# avg_dem = average_demand(select_days_station_met_60m_outflow_df)\n","# avg_dem_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_avg_dem = avg_dem_ts_stats_feat_transform.feature_normalization(  np.array([avg_dem.to_numpy()]).T  )\n","# print_msg = 'avg_dem'\n","# #plot_all(print_msg,X_train_normalized_avg_dem).plot_hist_n_corr(X_train_normalized_avg_dem)\n","# X_train_inv_normalized_avg_dem = avg_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_avg_dem)\n","# # print_msg = 'every_days_avg_dem_non_normalized'\n","# # plot_all(print_msg,X_train_inv_normalized_avg_dem).plot_hist_n_corr(X_train_inv_normalized_avg_dem)\n","\n","# no_mean_cross_dem = no_mean_crossing_demand(station_met_outflow_60min_df)\n","# num_mean_cross_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_mean_cross = num_mean_cross_ts_stats_feat_transform.feature_normalization(  np.array([no_mean_cross_dem.to_numpy()]).T  )\n","# print_msg = 'no_mean_crossing_dem'\n","# #plot_all(print_msg,X_train_normalized_mean_cross).plot_hist_n_corr(X_train_normalized_mean_cross)\n","# X_train_inv_normalized_mean_cross = num_mean_cross_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_mean_cross)\n","\n","# std_dem = std_demand(station_met_outflow_60min_df)\n","# std_dem_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_std = std_dem_ts_stats_feat_transform.feature_normalization(   np.array([std_dem.to_numpy()]).T   )\n","# print_msg = 'std_dem'\n","# #plot_all(print_msg,X_train_normalized_std).plot_hist_n_corr(X_train_normalized_std)\n","# X_train_inv_normalized_std = std_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_std)\n","\n","# entropy_dem = entropy_demands(station_met_outflow_60min_df)\n","# entropy_dem_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_entropy_dem = entropy_dem_ts_stats_feat_transform.feature_normalization(   np.array([entropy_dem.to_numpy()]).T  )\n","# print_msg = 'entropy_dem'\n","# #plot_all(print_msg,X_train_normalized_entropy_dem).plot_hist_n_corr(X_train_normalized_entropy_dem)\n","# X_train_inv_normalized_entropy_dem = entropy_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_entropy_dem)\n","\n","# pos_autocorr_lags = demands_positive_autocorr_lags(station_met_outflow_60min_df)\n","# pos_autocorr_lags_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_pos_autocorr_lags = pos_autocorr_lags_ts_stats_feat_transform.feature_normalization(pos_autocorr_lags)\n","# print_msg = 'positive_autocorr_lags'\n","# #plot_all(print_msg,X_train_normalized_pos_autocorr_lags).plot_hist_n_corr(X_train_normalized_pos_autocorr_lags)\n","# X_train_inv_normalized_pos_autocorr_lags = pos_autocorr_lags_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_pos_autocorr_lags)\n","\n","# neg_autocorr_lags = demands_negative_autocorr_lags(station_met_outflow_60min_df)\n","# neg_autocorr_lags_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_neg_autocorr_lags = neg_autocorr_lags_ts_stats_feat_transform.feature_normalization(neg_autocorr_lags)\n","# print_msg = 'negative_autocorr_lags'\n","# #plot_all(print_msg,X_train_normalized_neg_autocorr_lags).plot_hist_n_corr(X_train_normalized_neg_autocorr_lags)\n","# X_train_inv_normalized_neg_autocorr_lags = neg_autocorr_lags_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_neg_autocorr_lags)\n","\n","\n","# peak_fft_periods = fft_of_demand(station_met_outflow_60min_df)\n","# fft_ts_stats_feat_transform = feat_transformation()\n","# X_train_normalized_fft = fft_ts_stats_feat_transform.feature_normalization(peak_fft_periods)\n","# print_msg = 'fft_dem'\n","# #plot_all(print_msg,X_train_normalized_fft).plot_hist_n_corr(X_train_normalized_fft)\n","# X_train_inv_normalized_fft = fft_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_fft)\n","\n","# # X_train_inv_normalized_fft = fft_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized_fft)\n","# # print_msg = 'fft_non_normalized'\n","# # plot_all(print_msg).plot_hist_n_corr(X_train_inv_normalized_fft)\n","\n","# # ####### Takes too long to Compute ########\n","# # pos_partial_autocorr_lags = demands_positive_partial_autocorr_lags(station_met_outflow_60min_df)\n","# # pos_partial_autocorr_lags_ts_stats_feat_transform = feat_transformation()\n","# # X_train_normalized_pos_partial_autocorr_lags = pos_partial_autocorr_lags_ts_stats_feat_transform.feature_normalization(pos_partial_autocorr_lags)\n","# # print_msg = 'positive_partial_autocorr_lags'\n","# # plot_all(print_msg,X_train_normalized_neg_partial_autocorr_lags).plot_hist_n_corr(X_train_normalized_pos_partial_autocorr_lags)\n","\n","# # ####### Takes too long to Compute ########\n","# # neg_partial_autocorr_lags = demands_negative_partial_autocorr_lags(station_met_outflow_60min_df)\n","# # neg_partial_autocorr_lags_ts_stats_feat_transform = feat_transformation()\n","# # X_train_normalized_neg_partial_autocorr_lags = neg_partial_autocorr_lags_ts_stats_feat_transform.feature_normalization(neg_partial_autocorr_lags)\n","# # print_msg = 'negative_partial_autocorr_lags'\n","# # plot_all(print_msg,X_train_normalized_neg_partial_autocorr_lags).plot_hist_n_corr(X_train_normalized_neg_partial_autocorr_lags)\n","\n","\n","# # X_train_normalized_all_seasonality_statistic_features = np.concatenate((X_train_normalized_avg_dem,X_train_normalized_mean_cross,X_train_normalized_std,X_train_normalized_entropy_dem,\\\n","# #                                                                   X_train_normalized_pos_autocorr_lags,X_train_normalized_neg_autocorr_lags,X_train_normalized_fft),axis=1)\n","\n","# # # After removing correlated features\n","# # X_train_normalized_all_seasonality_statistic_features = np.concatenate((X_train_normalized_avg_dem,X_train_normalized_mean_cross,X_train_normalized_pos_autocorr_lags,\\\n","# #                                                                         X_train_normalized_fft),axis=1)\n","# # X_train_inverse_normalized_all_seasonality_statistic_features = np.concatenate((X_train_inv_normalized_avg_dem,X_train_inv_normalized_mean_cross,\\\n","# #                                                                                 X_train_inv_normalized_pos_autocorr_lags,X_train_inv_normalized_fft),axis=1)\n","\n","# X_train_normalized_all_seasonality_statistic_features = X_train_normalized_pos_autocorr_lags ## np.concatenate((X_train_normalized_fft,X_train_normalized_mean_cross,),axis=1)\n","# X_train_inverse_normalized_all_seasonality_statistic_features = X_train_inv_normalized_pos_autocorr_lags ##np.concatenate((X_train_inv_normalized_avg_dem,X_train_inv_normalized_mean_cross,),axis=1)\n","\n","# print_msg = 'all_seasonality_statistics features'\n","# plot_all(print_msg,X_train_normalized_all_seasonality_statistic_features).plot_hist_n_corr(X_train_normalized_all_seasonality_statistic_features)\n","\n","# \"\"\" SEASONALITY STATISTICAL FEATURES calculation\"\"\"\n","# ######################################################################################################################\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwSldbTnx8pn"},"outputs":[],"source":["\"\"\"                                                                     PLOT UMAP for everyday_avg_demand Time series statistical features\n","\n","\"\"\"\n","\n","plt.close('all')\n","X_train_normalized = X_train_normalized_all_trend_statistic_features\n","X_train_inv_normalized =X_train_inverse_normalized_all_trend_statistic_features\n","# X_train_normalized = X_train_normalized_all_seasonality_statistic_features\n","# X_train_inv_normalized =X_train_inverse_normalized_all_seasonality_statistic_features\n","\n","# print_msg = ''\n","# plotter_avg_dem = plot_all(print_msg,X_train_normalized)\n","\n","# interactive_UMAP_plot=interactive(plotter_avg_dem.plot_UMAP_interactive,X_train=X_train_normalized,print_msg=print_msg,n_neighbors=(10,100,20),min_dist=(0,1.0,0.1),metric=['euclidean','mahalanobis','manhattan'])\n","# display(interactive_UMAP_plot)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swgNO4Fl2_o9"},"outputs":[],"source":["\"\"\"                                                                    ### DETAILED 3d UMAP for everyday_avg_demand Time series statistical features\n","\n","use above to get correct n_neighbors ,min_dist and metric to visuzalize in detain\n","\"\"\"\n","# plt.close('all')\n","\n","# fig_umap_3d = sp.make_subplots(specs=[[{'type': 'surface'} for i in range(1)]])\n","\n","# fig_umap_3d.add_trace(plotter_avg_dem.plot_UMAP_3d(X_train_normalized,10,0.1,'euclidean'))\n","\n","# fig_umap_3d.update_layout(height=900 , width=900,title='UMAP 3d,'+print_msg)\n","# fig_umap_3d.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZRFSsSjEIKb"},"outputs":[],"source":["\"\"\"\n","Parameter search for best clustering using TS statistical Features\n","\n","\"\"\"\n","\n","best_score = 0\n","best_score_num_cluster_dict = {}\n","cov_mat = np.cov(X_train_normalized.T)\n","\n","for metric in ['euclidean','manhattan','mahalanobis']:# need to implement mahalanobis distance\n","    if metric != 'mahalanobis':\n","        for min_cluster_size in range(5,80,5):\n","            for min_samples in range(1,40,2):\n","                for cluster_selection_method in ['eom','leaf']:\n","                    # for each combination of parameters of hdbscan\n","                    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n","                                          cluster_selection_method=cluster_selection_method, metric=metric,\n","                                          gen_min_span_tree=True).fit(X_train_normalized)\n","                    # DBCV score\n","                    score = hdb.relative_validity_\n","                    outliers = (np.array(hdb.labels_)==-1).sum()\n","                    best_score_num_cluster_dict[score] = [len(np.unique(hdb.labels_)),min_cluster_size,min_samples,cluster_selection_method,metric,outliers]\n","                    #print(f'score: {score}, num of unique clusters: {len(np.unique(hdb.labels_))}' )\n","                    # if we got a better DBCV, store it and the parameters\n","                    if score > best_score:\n","                        best_score = score\n","                        best_parameters = {'min_cluster_size': min_cluster_size,\n","                                  ' min_samples':  min_samples, 'cluster_selection_method': cluster_selection_method,\n","                                  'metric': metric}\n","    else: ## mahalanobis distance\n","        for min_cluster_size in range(5,80,5):\n","            for min_samples in range(1,40,2):\n","                for cluster_selection_method in ['eom','leaf']:\n","                    # for each combination of parameters of hdbscan\n","                    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n","                                          cluster_selection_method=cluster_selection_method, metric=metric,V=cov_mat,\n","                                          gen_min_span_tree=True).fit(X_train_normalized)\n","                    # DBCV score\n","                    score = hdb.relative_validity_\n","                    outliers = (np.array(hdb.labels_)==-1).sum()\n","                    best_score_num_cluster_dict[score] = [len(np.unique(hdb.labels_)),min_cluster_size,min_samples,cluster_selection_method,metric,outliers]\n","                    #print(f'score: {score}, num of unique clusters: {len(np.unique(hdb.labels_))}' )\n","                    # if we got a better DBCV, store it and the parameters\n","                    if score > best_score:\n","                        best_score = score\n","                        best_parameters = {'min_cluster_size': min_cluster_size,\n","                                  ' min_samples':  min_samples, 'cluster_selection_method': cluster_selection_method,\n","                                  'metric': metric}\n","\n","\n","print(\"Best DBCV score: {:.3f}\".format(best_score))\n","print(\"Best parameters: {}\".format(best_parameters))"]},{"cell_type":"code","source":["sorted(best_score_num_cluster_dict.items())[-40:]"],"metadata":{"id":"s-uatiyJSRyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Further eliminate some Parameter combination for every_days_avg_dem TS statistical Features based on best DBCV score\n","\n","\"\"\"\n","\n","hdbscan_best_score_comb = []\n","dbcv_threshold = 0.05\n","min_num_clusters_threshold = 3\n","max_num_clusters_threshold = 7\n","num_outliers_margin = X_train_normalized.shape[0]/2.6 #tolerable_num_outliers\n","\n","for comb in sorted(best_score_num_cluster_dict.items())[-40:]:\n","    dbcv = comb[0]\n","    num_outliers = comb[1][5]\n","    num_clusters = comb[1][0]\n","    if ((num_outliers < num_outliers_margin) and (num_clusters < max_num_clusters_threshold) and (num_clusters > min_num_clusters_threshold) and (dbcv > dbcv_threshold) ):\n","        hdbscan_best_score_comb.append(comb)\n","hdbscan_best_score_comb\n","\n"],"metadata":{"id":"ieFzWBSLf-OH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyLF3QmMEgXp"},"outputs":[],"source":["\"\"\"\n","Plot Minimum spanning tree and dbscan cluster hierarchy from every_days_avg_dem  TS statistical features\n","\n","SHORTLISTED hdbscan clustering parameters combination for stats features\n","\n","\"\"\"\n","\n","\n","plt.close('all')\n","clustering_type = 'HDBSCAN - euclidean metric'\n","\n","if (hdbscan_best_score_comb[-1][1][4] == 'mahalanobis'):\n","    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size= hdbscan_best_score_comb[-1][1][1], min_samples= hdbscan_best_score_comb[-1][1][2],cluster_selection_method=hdbscan_best_score_comb[-1][1][3], metric=hdbscan_best_score_comb[-1][1][4], gen_min_span_tree=True,V=cov_mat,)\n","else:\n","    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size= hdbscan_best_score_comb[-1][1][1], min_samples= hdbscan_best_score_comb[-1][1][2],cluster_selection_method=hdbscan_best_score_comb[-1][1][3], metric=hdbscan_best_score_comb[-1][1][4], gen_min_span_tree=True)#,V=cov_mat,)\n","\n","\n","hdbscan_model.fit(X_train_normalized)\n","\n","fig = plt.figure(figsize=(10,5))\n","# Plotting the minimum spanning tree in the first subplot\n","hdbscan_model.minimum_spanning_tree_.plot(\n","    edge_cmap='viridis',\n","    edge_alpha=0.6,\n","    edge_linewidth=2\n",")\n","fig.show()\n","\n","fig = plt.figure(figsize=(20,10))\n","# Plotting the condensed tree in the second subplot\n","hdbscan_model.condensed_tree_.plot(\n","    select_clusters=True,\n","    selection_palette=sns.color_palette(),\n",")\n","fig.show()\n","\n","print(hdbscan_model.labels_)\n","\n","plot_silhouette_scores(X_train_normalized,hdbscan_model.labels_,'euclidean','non',clustering_type)"]},{"cell_type":"code","source":["\"\"\"\n","Visualize every_days_avg_dem Time series statistical features of all clusters using parcoords plot\n","\n","\"\"\"\n","\n","plt.close('all')\n","plot_title = 'TS statistical features'\n","mother = 'rbio2.2'\n","limit_level = 2\n","trend_angular_labels = [\"Average Demand\",'no_mean_crossing', \"n95 demand\", \"Zero demands\", \"skewness\"]\n","# seasonality_angular_labels = [\n","# 'pos_ac_lag0',\n","#  'pos_ac_lag1',\n","#  'pos_ac_lag2',\n","#  'pos_ac_lag3',\n","#  'pos_ac_lag4',]\n","#seasonality_angular_labels = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\n","\n","\n","#X_train = avg_dem_ts_stats_feat_transform.inv_feature_normalization(X_train_normalized)\n","\n","#station_met_outflow_60min_df = station_met_outflow_60min_df.drop(columns=['week','weekday','hour'])\n","clustering_result_dict,all_cluster_feat_mins,all_cluster_feat_maxs = plot_stations_of_clusters(hdbscan_model.labels_, zone_met_10m_outflow_df,'met_demand',X_train_normalized,X_train_inv_normalized,'par_coords',trend_angular_labels,plot_title,mother,limit_level)\n","\n"],"metadata":{"id":"pivcdfWMaoSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Visualize cluster representatives from every_days_avg_dem Time series statistical features using parcoords plot\n","\n","\"\"\"\n","\n","plot_cluster_representatives(X_train_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',trend_angular_labels)"],"metadata":{"id":"Sv-CNMPYavKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Rank cluster from \"highest demand\" to \"lowest demand\" vaguely without the outlier.\n","\"\"\"\n","num_clusters = len(np.unique(hdbscan_model.labels_))\n","num_features = X_train_normalized.shape[1]\n","\n","num_clusters = num_clusters - 1 #remove outlier cluster\n","all_cluster_feat_mins = all_cluster_feat_mins[num_features:].reshape(num_clusters,num_features)\n","all_cluster_feat_maxs = all_cluster_feat_maxs[num_features:].reshape(num_clusters,num_features)\n","\n","ranked_clusters = rank_clusterings(all_cluster_feat_mins,all_cluster_feat_maxs,num_clusters,num_features) # exclude outliers\n","ranked_clusters"],"metadata":{"id":"4mVsvVDGa9Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Display different ranked clusters on the same map\n","\"\"\"\n","\n","colormap = plt.cm.RdYlGn  # Diverging red to green colormap\n","norm = mcolors.Normalize(vmin=min(ranked_clusters.keys()), vmax=max(ranked_clusters.keys()))\n","\n","map_center = [40.7831, -73.9712]  # Manhattan coordinates (latitude, longitude)\n","map_zoom = 12  # Adjust the zoom level as needed\n","my_map = folium.Map(location=map_center, zoom_start=map_zoom,width='70%', height='70%')\n","\n","marker_color = [ 'black','darkred','red','lightred','lightgreen','green','darkgreen',    'lightblue',    'blue',    'darkblue',]\n","\n","for cluster in clustering_result_dict.keys(): # outliers included\n","  stations = clustering_result_dict[cluster]\n","  color = mcolors.to_hex(colormap(norm(cluster)))\n","  for station in stations:\n","    folium.Marker(location=zone_to_gps_dict[int(station)], popup=str(station)+': '+color+'-'+str(cluster), icon=folium.Icon(color=marker_color[cluster+1]),tooltip=str(cluster)).add_to(my_map)\n","display(my_map)"],"metadata":{"id":"CrR3N1ee23sR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaWyTsVv1zr9"},"outputs":[],"source":["\"\"\"\n","Identify aggolmeratic clustering parameter combination with best silhouette score for trend and seasonality stat features\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","num_clusters_list = range(4,8,1)\n","clustering_type = 'Agglomerative'\n","\n","agglomerative_result_comb = []\n","\n","methods = ['single','complete','average']#['single','complete','average','weighted',]\n","metrics = ['euclidean','cityblock']\n","for metric in metrics:\n","    for method in methods:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_normalized)\n","            labels = model.labels_\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_train_normalized,labels,metric,method,clustering_type)\n","            agglomerative_result_comb.append([silhouette_avg,num_neg_silhouette_scores, min_silhouette_score,metric,method,num_clusters,dev_from_mean_num_zone ])\n","\n","\n","methods = ['ward']#['median','centroid','ward']\n","metrics = ['euclidean']\n","for method in methods:\n","    for metric in metrics:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_normalized)\n","            labels = model.labels_\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_train_normalized,labels,metric,method,clustering_type)\n","            agglomerative_result_comb.append([silhouette_avg,num_neg_silhouette_scores, min_silhouette_score,metric,method,num_clusters,dev_from_mean_num_zone ])\n","\n","\n","methods = ['complete','average']#['single','complete','average','weighted',]\n","metrics = ['mahalanobis','manhattan']\n","for metric in metrics:\n","    for method in methods:\n","        for num_clusters in num_clusters_list:\n","            model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","            model.fit(X_train_normalized)\n","            labels = model.labels_\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_train_normalized,labels,metric,method,clustering_type)\n","            agglomerative_result_comb.append([silhouette_avg,num_neg_silhouette_scores, min_silhouette_score,metric,method,num_clusters,dev_from_mean_num_zone ])"]},{"cell_type":"code","source":["\"\"\"\n","Shortlist Agglomerative clustering\n","\n","\"\"\"\n","\n","agglomerative_result_comb_df = pd.DataFrame(agglomerative_result_comb,columns=[\"silhouette_avg\",\"num_neg_silhouette_scores\", \"min_silhouette_score\",\"metric\",\"method\",\"num_clusters\",\"dev_from_mean_num_zone\"])\n","\n","sorted_best_agglomerative_result_comb = agglomerative_result_comb_df.sort_values(by=['num_neg_silhouette_scores','dev_from_mean_num_zone','min_silhouette_score'],ascending=[True,True,False])\n","\n","sorted_best_agglomerative_result_comb"],"metadata":{"id":"t5mV-wFe-rqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","SHORTLISTED aggolmeratic clustering parameters combination for stats features\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","num_clusters_list = [sorted_best_agglomerative_result_comb.iloc[0]['num_clusters']]\n","clustering_type = 'Agglomerative'\n","\n","\n","plot_title = 'TS statistical features'\n","mother = 'rbio2.2'\n","limit_level = 2\n","trend_angular_labels = [\"Average Demand\",'no_mean_crossing', \"n95 demand\", \"Zero demands\", \"skewness\"]\n","\n","methods =  [sorted_best_agglomerative_result_comb.iloc[0]['method']] #['complete']#['single','complete','average','weighted',]\n","metrics = [sorted_best_agglomerative_result_comb.iloc[0]['metric']]\n","for metric,method,num_clusters in zip(metrics,methods,num_clusters_list):\n","    print('\\n',metric,method,num_clusters)\n","    plot_title = 'TS stats features, '+ ', metric: '+metric+', method: '+', num_clusters: '+str(num_clusters)+'. '+', Clustering_type : '+clustering_type\n","    agglomerative_model = AgglomerativeClustering(n_clusters=num_clusters, metric=metric, linkage=method)\n","    agglomerative_model.fit(X_train_normalized)\n","    labels = agglomerative_model.labels_\n","    plot_silhouette_scores(X_train_normalized,agglomerative_model.labels_,metric,method,clustering_type)\n","    clustering_result_dict,all_cluster_feat_mins,all_cluster_feat_maxs = plot_stations_of_clusters(agglomerative_model.labels_, zone_met_10m_outflow_df,'met_demand',X_train_normalized,X_train_inv_normalized,'par_coords',trend_angular_labels,plot_title,mother,limit_level)\n","\n","\n","\n"],"metadata":{"id":"VvVTuYgrdwnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Visualize cluster representatives from every_days_avg_dem Time series statistical features using parcoords plot\n","\n","\"\"\"\n","\n","# #Add dummy outliers\n","# all_cluster_feat_mins = np.append( np.random.randint(0,1,(5,)) , all_cluster_feat_mins)\n","# all_cluster_feat_maxs = np.append( np.random.randint(0,1,(5,)) , all_cluster_feat_maxs)\n","\n","plot_cluster_representatives(X_train_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',trend_angular_labels)"],"metadata":{"id":"lY9LRF07qkVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Rank cluster from \"highest demand\" to \"lowest demand\" vaguely without the outlier.\n","\"\"\"\n","num_clusters = len(np.unique(agglomerative_model.labels_))\n","num_features = X_train_normalized.shape[1]\n","\n","ranked_clusters = rank_clusterings(all_cluster_feat_mins,all_cluster_feat_maxs,num_clusters,num_features)\n","ranked_clusters"],"metadata":{"id":"wtyP4zemr5g6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(all_cluster_feat_mins)"],"metadata":{"id":"GtQWJmlFseEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Display different ranked clusters on the same map\n","\"\"\"\n","\n","colormap = plt.cm.RdYlGn  # Diverging red to green colormap\n","norm = mcolors.Normalize(vmin=min(ranked_clusters.keys()), vmax=max(ranked_clusters.keys()))\n","\n","map_center = [40.7831, -73.9712]  # Manhattan coordinates (latitude, longitude)\n","map_zoom = 12  # Adjust the zoom level as needed\n","my_map = folium.Map(location=map_center, zoom_start=map_zoom,width='70%', height='70%')\n","\n","marker_color = [ 'black','darkred','red','lightred','lightgreen','green','darkgreen',    'lightblue',    'blue',    'darkblue',]\n","\n","for cluster in clustering_result_dict.keys(): # ourliers included\n","  stations = clustering_result_dict[cluster]\n","  color = mcolors.to_hex(colormap(norm(cluster)))\n","  for station in stations:\n","    folium.Marker(location=zone_to_gps_dict[int(station)], popup=str(station)+': '+color+'-'+str(cluster), icon=folium.Icon(color=marker_color[cluster+1]),tooltip=str(cluster)).add_to(my_map)\n","display(my_map)"],"metadata":{"id":"TYvq9BpKr5eS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Identify kmeans clustering parameter combination with best silhouette score for trend and seasonality stat features\n","\n","\"\"\"\n","\n","metrics = ['euclidean']\n","methods = ['single','complete','average','ward']\n","clustering_type = 'K-Means'\n","\n","kmeans_result_comb = []\n","\n","for num_cluster in range(4,8,1):\n","    for metric in metrics:\n","        for method in methods:\n","            km = TimeSeriesKMeans(n_clusters=num_cluster, metric=metric)\n","            km.fit(X_train_normalized)\n","            #plot_stations_of_clusters(km.labels_, Dim_Red_unmet_60min_outflow_week_df,'unmet_demand')\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_train_normalized,km.labels_,metric,method,clustering_type)\n","            kmeans_result_comb.append([silhouette_avg,num_neg_silhouette_scores, min_silhouette_score,metric,method,num_cluster,dev_from_mean_num_zone ])\n"],"metadata":{"id":"ysIs5F3Xr5b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Shortlist kmeans clustering for TS trend stats features\n","\n","\"\"\"\n","\n","kmeans_result_comb_df = pd.DataFrame(kmeans_result_comb,columns=[\"silhouette_avg\",\"num_neg_silhouette_scores\", \"min_silhouette_score\",\"metric\",\"method\",\"num_clusters\",\"dev_from_mean_num_zone\"])\n","\n","sorted_best_kmeans_result_comb = kmeans_result_comb_df.sort_values(by=['num_neg_silhouette_scores','dev_from_mean_num_zone','min_silhouette_score'],ascending=[True,True,False])\n","\n","sorted_best_kmeans_result_comb"],"metadata":{"id":"5eBaZmNwN8Vv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","SHORTLISTED K-means clustering parameters combination for stats features\n","\n","\"\"\"\n","\n","plt.close('all')\n","\n","plot_title = 'TS statistical features'\n","mother = 'rbio2.2'\n","limit_level = 2\n","trend_angular_labels = [\"Average Demand\",'no_mean_crossing', \"n95 demand\", \"Zero demands\", \"skewness\"]\n","\n","metrics = [sorted_best_kmeans_result_comb.iloc[0]['metric']]\n","methods = [sorted_best_kmeans_result_comb.iloc[0]['method']]\n","clustering_type = 'K-Means'\n","num_clusters_list = [sorted_best_kmeans_result_comb.iloc[0]['num_clusters']]\n","\n","for num_cluster in num_clusters_list:\n","    for metric in metrics:\n","        for method in methods:\n","            kmeans_model = TimeSeriesKMeans(n_clusters=num_cluster, metric=metric)\n","            kmeans_model.fit(X_train_normalized)\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_train_normalized,kmeans_model.labels_,metric,method,clustering_type)\n","            clustering_result_dict,all_cluster_feat_mins,all_cluster_feat_maxs = plot_stations_of_clusters(kmeans_model.labels_, zone_met_10m_outflow_df,'met_demand',X_train_normalized,X_train_inv_normalized,'par_coords',trend_angular_labels,plot_title,mother,limit_level)\n","\n","\n"],"metadata":{"id":"hPk-wJnUtz0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Visualize cluster representatives from every_days_avg_dem Time series statistical features using parcoords plot\n","\n","\"\"\"\n","\n","#Add dummy outliers\n","# all_cluster_feat_mins = np.append( np.random.randint(0,1,(5,)) , all_cluster_feat_mins)\n","# all_cluster_feat_maxs = np.append( np.random.randint(0,1,(5,)) , all_cluster_feat_maxs)\n","\n","plot_cluster_representatives(X_train_normalized, all_cluster_feat_mins, all_cluster_feat_maxs,'par_coords',trend_angular_labels)"],"metadata":{"id":"Vj8TxAvKtzw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Rank cluster from \"highest demand\" to \"lowest demand\" vaguely without the outlier.\n","\"\"\"\n","num_clusters = len(np.unique(kmeans_model.labels_))\n","num_features = X_train_normalized.shape[1]\n","\n","ranked_clusters = rank_clusterings(all_cluster_feat_mins,all_cluster_feat_maxs,num_clusters,num_features) # exclude outliers\n","ranked_clusters"],"metadata":{"id":"VEerEUpPtzq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Display different ranked clusters on the same map\n","\"\"\"\n","\n","colormap = plt.cm.RdYlGn  # Diverging red to green colormap\n","norm = mcolors.Normalize(vmin=min(ranked_clusters.keys()), vmax=max(ranked_clusters.keys()))\n","\n","map_center = [40.7831, -73.9712]  # Manhattan coordinates (latitude, longitude)\n","map_zoom = 12  # Adjust the zoom level as needed\n","my_map = folium.Map(location=map_center, zoom_start=map_zoom,width='70%', height='70%')\n","\n","marker_color = [ 'black','darkred','red','lightred','lightgreen','green','darkgreen',    'lightblue',    'blue',    'darkblue',]\n","\n","for cluster in clustering_result_dict.keys(): # ourliers included\n","  stations = clustering_result_dict[cluster]\n","  color = mcolors.to_hex(colormap(norm(cluster)))\n","  for station in stations:\n","    folium.Marker(location=zone_to_gps_dict[int(station)], popup=str(station)+': '+color+'-'+str(cluster), icon=folium.Icon(color=marker_color[cluster+1]),tooltip=str(cluster)).add_to(my_map)\n","display(my_map)"],"metadata":{"id":"MA-z_W20tzoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Decide which is the best clustering option/method out of the 3 clustering methods\n","\n","\"\"\"\n","\n","## Best results from each clustering\n","\n","# hdbscan_best_score_comb_df = pd.DataFrame( {'dbcv':hdbscan_best_score_comb[-1][0] ,'num_outliers':hdbscan_best_score_comb[-1][1][5],'num_clusters': hdbscan_best_score_comb[-1][1][0], \\\n","#                                             'min_cluster_size': hdbscan_best_score_comb[-1][1][1], 'min_samples': hdbscan_best_score_comb[-1][1][2], \\\n","#                                             'cluster_selection_method': hdbscan_best_score_comb[-1][1][3], 'metric': hdbscan_best_score_comb[-1][1][4],},index=range(1)).transpose()\n","\n","# sorted_best_agglomerative_result_comb_df = pd.DataFrame(sorted_best_agglomerative_result_comb.iloc[0])\n","# sorted_best_kmeans_result_comb_df = pd.DataFrame(sorted_best_kmeans_result_comb.iloc[0])\n","\n","best_clustering_comb = pd.DataFrame( {'dbcv/silh_avg':[hdbscan_best_score_comb[-1][0],sorted_best_agglomerative_result_comb.iloc[0]['silhouette_avg'],sorted_best_kmeans_result_comb.iloc[0]['silhouette_avg'] ], \\\n","                                      'num_outliers/neg_silh_score':[hdbscan_best_score_comb[-1][1][5],sorted_best_agglomerative_result_comb.iloc[0]['num_neg_silhouette_scores'],sorted_best_kmeans_result_comb.iloc[0]['num_neg_silhouette_scores'] ], \\\n","                                      'min_silhouette_score':[np.NaN,sorted_best_agglomerative_result_comb.iloc[0]['min_silhouette_score'],sorted_best_kmeans_result_comb.iloc[0]['min_silhouette_score']],\n","                                      'num_clusters': [hdbscan_best_score_comb[-1][1][0],sorted_best_agglomerative_result_comb.iloc[0]['num_clusters'],sorted_best_kmeans_result_comb.iloc[0]['num_clusters'] ], \\\n","                                      'metric': [hdbscan_best_score_comb[-1][1][4],sorted_best_agglomerative_result_comb.iloc[0]['metric'], sorted_best_kmeans_result_comb.iloc[0]['metric']], \\\n","                                      },index=['hdbscan','agglomerative','kmeans'])\n","\n","#TODO: shortlist clustering results based on hyperparamter conditions\n","\n","model_labels = {'hdbscan':hdbscan_model.labels_,'agglomerative':agglomerative_model.labels_,'kmeans':kmeans_model.labels_}\n","\n","sorted_best_clustering_comb = best_clustering_comb.sort_values(['num_outliers/neg_silh_score'],ascending=[True])\n","chosen_model = sorted_best_clustering_comb.index[0] # select the clustering method with minimum number of outliers.\n","\n","sorted_best_clustering_comb\n"],"metadata":{"id":"CSxa8MnGtzl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","save final cluster labels results\n","\"\"\"\n","final_cluster_labels_df = pd.DataFrame(data={manhat_demand.index[-1]:model_labels[chosen_model]},index=range(0,len(model_labels[chosen_model]),1))\n","final_cluster_labels_df.to_csv('Manhattan_final_cluster_result_'+ str(manhat_demand.index[-1])+'.csv')\n"],"metadata":{"id":"ooSf2-wgGtAd"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}