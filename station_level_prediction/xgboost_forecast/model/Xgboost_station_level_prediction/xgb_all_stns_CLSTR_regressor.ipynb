{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tweedie\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import pygraphviz\n",
    "#warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from xgboost import plot_tree\n",
    "from sklearn.utils import class_weight\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "class train_validate_n_test(object):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        os.chdir('/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/data/xgboost_data')    \n",
    "        dataset_filename = \"xgboost_feat_train_ds_all_stns.csv\"\n",
    "        full_set = pd.read_csv(dataset_filename,parse_dates=['dt_ts'])\n",
    "        full_set = full_set.drop(['Unnamed: 0'],axis=1)\n",
    "        os.chdir('/home/optimusprime/Desktop/peeterson/github/DeepAR_demand_prediction/2_freq_nbinom_LSTM/1_cluster_demand_prediction/model/Xgboost_station_level_prediction')\n",
    "\n",
    "        self.train_stop_time_fold_1 = '2021/11/30 00:00'\n",
    "        self.val_stop_time_fold_1 = '2021/12/05 00:00'\n",
    "        self.train_stop_time_fold_2 = self.val_stop_time_fold_1\n",
    "        self.val_stop_time_fold_2 = '2021/12/10 00:00'\n",
    "        self.train_stop_time_fold_3 = self.val_stop_time_fold_2\n",
    "        self.val_stop_time_fold_3 = '2021/12/16 00:00'\n",
    "\n",
    "        #self.classes_weights = self.analyze_target(full_set)\n",
    "        self.full_set = self.onehotencode_cat_var(full_set)\n",
    "\n",
    "\n",
    "    def analyze_target(self,full_set):\n",
    "        print('###################################### TARGET data EXPLORATION ###########################################')\n",
    "        print('Target class frequency:')\n",
    "        print(full_set['target'].value_counts())\n",
    "\n",
    "        full_set['target'].hist()\n",
    "        plt.title('Target histogram')\n",
    "        plt.xlabel('demand value (class)')\n",
    "        plt.ylabel('count')\n",
    "        plt.show()\n",
    "\n",
    "        classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=full_set['target'])\n",
    "        print('\\nTarget class weights:')\n",
    "        print(np.unique(classes_weights))\n",
    "        print('##########################################################################################################')\n",
    "        return classes_weights\n",
    "\n",
    "\n",
    "    def onehotencode_cat_var(self,full_set):\n",
    "        full_set = full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n",
    "        full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =[\"stn_id\",\"block_id\",\"ts_of_day\",\"hr_of_day\",\"day_of_wk\",\"day_of_mn\",\"wk_of_mon\"],drop_first=True)\n",
    "        #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n",
    "        return full_set\n",
    "\n",
    "\n",
    "    def walk_forward_val_n_train_set(self,fold):\n",
    "        # split validation set into 3 parts\n",
    "        if (fold==1):\n",
    "            train1 = self.full_set.loc[self.full_set['dt_ts'] < self.train_stop_time_fold_1]\n",
    "            tr1 = train1.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_train1 = pd.DataFrame(train1[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_train1)\n",
    "            dtrain1 = xgb.DMatrix(tr1, label=label_train1,weight=classes_weights)\n",
    "\n",
    "            print('\\n',train1.isnull().values.any(),'\\n')\n",
    "\n",
    "            val1 = self.full_set.loc[(self.full_set['dt_ts'] >= self.train_stop_time_fold_1) & (self.full_set['dt_ts'] < self.val_stop_time_fold_1)]\n",
    "            v1 = val1.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_val1 = pd.DataFrame(val1[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_val1)\n",
    "            dval1 = xgb.DMatrix(v1, label=label_val1,weight=classes_weights)\n",
    "            print('\\n',val1.isnull().values.any(),'\\n')\n",
    "\n",
    "            return dtrain1, dval1\n",
    "        \n",
    "        elif (fold==2):\n",
    "            train2 = self.full_set.loc[self.full_set['dt_ts'] < self.train_stop_time_fold_2]\n",
    "            tr2 = train2.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_train2 = pd.DataFrame(train2[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_train2)\n",
    "            dtrain2 = xgb.DMatrix(tr2, label=label_train2,weight=classes_weights)\n",
    "\n",
    "            print('\\n',train2.isnull().values.any(),'\\n')\n",
    "\n",
    "            val2 = self.full_set.loc[(self.full_set['dt_ts'] >= self.train_stop_time_fold_2) & (self.full_set['dt_ts'] < self.val_stop_time_fold_2)]\n",
    "            v2 = val2.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_val2 = pd.DataFrame(val2[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_val2)\n",
    "            dval2 = xgb.DMatrix(v2, label=label_val2,weight=classes_weights)\n",
    "\n",
    "            print('\\n',val2.isnull().values.any(),'\\n')\n",
    "\n",
    "            return dtrain2,dval2\n",
    "        \n",
    "        else:\n",
    "            train3 = self.full_set.loc[self.full_set['dt_ts'] < self.train_stop_time_fold_3]\n",
    "            tr3 = train3.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_train3 = pd.DataFrame(train3[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_train3)\n",
    "            dtrain3 = xgb.DMatrix(tr3, label=label_train3,weight=classes_weights)\n",
    "\n",
    "            print('\\n',train3.isnull().values.any(),'\\n')\n",
    "\n",
    "            val3 = self.full_set.loc[(self.full_set['dt_ts'] >= self.train_stop_time_fold_3) & (self.full_set['dt_ts'] < self.val_stop_time_fold_3)]\n",
    "            v3 = val3.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "            label_val3 = pd.DataFrame(val3[\"target\"])\n",
    "            classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_val3)\n",
    "            dval3 = xgb.DMatrix(v3, label=label_val3,weight=classes_weights)\n",
    "\n",
    "            print('\\n',val3.isnull().values.any(),'\\n')\n",
    "\n",
    "            return dtrain3,dval3\n",
    "\n",
    "\n",
    "\n",
    "    def xgb_train_validate(self,params,num_round, e_s_r,t_v_p):\n",
    "        dtrain1, dval1 = self.walk_forward_val_n_train_set(fold=1)\n",
    "        watchlist1  = [(dtrain1,'train1_tweedie_loss'), (dval1, 'val1_tweedie_loss')]\n",
    "        evals_result1 = {}\n",
    "        model1 = xgb.train(params=params, dtrain=dtrain1, num_boost_round=num_round, evals=watchlist1, evals_result=evals_result1,  early_stopping_rounds=e_s_r,verbose_eval=False)\n",
    "        val1_error = evals_result1['val1_tweedie_loss']['tweedie-nloglik@'+str(t_v_p)]\n",
    "\n",
    "        dtrain2, dval2 = self.walk_forward_val_n_train_set(fold=2)\n",
    "        watchlist2  = [(dtrain2,'train2_tweedie_loss'), (dval2, 'val2_tweedie_loss')]\n",
    "        evals_result2 = {}\n",
    "        model2 = xgb.train(params=params, dtrain=dtrain2, num_boost_round=num_round, evals=watchlist2, evals_result=evals_result2, early_stopping_rounds=e_s_r,xgb_model=model1,verbose_eval=False )\n",
    "        val2_error = evals_result2['val2_tweedie_loss']['tweedie-nloglik@'+str(t_v_p)]\n",
    "\n",
    "        dtrain3, dval3 = self.walk_forward_val_n_train_set(fold=3)\n",
    "        watchlist3  = [(dtrain3,'train3_tweedie_loss'), (dval3, 'val3_tweedie_loss')]\n",
    "        evals_result3 = {}\n",
    "        self.model3 = xgb.train(params=params, dtrain=dtrain3, num_boost_round=num_round, evals=watchlist3, evals_result=evals_result3, early_stopping_rounds=e_s_r,xgb_model=model2,verbose_eval=False )\n",
    "        val3_error = evals_result3['val3_tweedie_loss']['tweedie-nloglik@'+str(t_v_p)]\n",
    "\n",
    "        print('\\n')\n",
    "        print('###################################### VALIDATION Loss ###########################################')\n",
    "        print('Last estimator val1_error',val1_error[-1]) # Result is a list of validation loss from each of boosters/estimators, last one is final val loss.\n",
    "        print('Last estimator val2_error',val2_error[-1])\n",
    "        print('Last estimator val3_error',val3_error[-1])\n",
    "\n",
    "        val_avg_error = np.mean(np.array([val1_error[-1],val2_error[-1],val3_error[-1]]))\n",
    "        print('val_avg_error',val_avg_error)\n",
    "        print('##################################################################################################')\n",
    "        return val_avg_error\n",
    "\n",
    "\n",
    "    def display_tweedie_plot(self,):\n",
    "        tvs = tweedie.tweedie(mu=1, p=1.1, phi=1.5).rvs(6000)\n",
    "        sn.set(rc={'figure.figsize':(10,5)})\n",
    "        # plt.figure(figsize=(25,10))\n",
    "        sn.displot(tvs)\n",
    "        plt.title('Parametric tweedie distribution')\n",
    "        plt.xlabel('demand value (class)')\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    def evaluate_predictions(self,model):\n",
    "        test = self.full_set.loc[self.full_set['dt_ts'] >= self.val_stop_time_fold_3]\n",
    "        X_test = test.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "        label_test = pd.DataFrame(test[\"target\"])\n",
    "        classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_test)\n",
    "        dtest = xgb.DMatrix(X_test, label=label_test,weight=classes_weights)\n",
    "\n",
    "        print('\\n',test.isnull().values.any(),'\\n')\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        preds = np.rint(preds)\n",
    "        label_test = label_test['target'].to_numpy()\n",
    "\n",
    "        print('####################################### PREDICTION #################################################')    \n",
    "        plt.plot(preds,'*')\n",
    "        plt.ylabel('Demand value (prediction)')\n",
    "        plt.xlabel('Time')     \n",
    "        plt.show()\n",
    "        plt.plot(label_test,'*')\n",
    "        plt.ylabel('Demand value (Ground-truth)')\n",
    "        plt.xlabel('Time')     \n",
    "        plt.show()\n",
    "        # plt.plot(label_test[40:60],'*')\n",
    "        # plt.title('zoomed true label test')\n",
    "        # plt.ylabel('Demand value')\n",
    "        # plt.xlabel('Time')     \n",
    "        # plt.show()\n",
    "        # plt.plot(preds[40:60],'*')\n",
    "        # plt.title('zoomed pred label')\n",
    "        # plt.ylabel('Demand value')\n",
    "        # plt.xlabel('Time')     \n",
    "        # plt.show()\n",
    "\n",
    "        #\"use confusion matrix, ROC, F1 scores to evaluate\"\n",
    "        cm = confusion_matrix(label_test,preds)\n",
    "        max_classes = max(len(np.unique(label_test)),len(np.unique(preds)))\n",
    "        df_cm = pd.DataFrame(cm, index = [i for i in range(max_classes)],\n",
    "                        columns = [i for i in range(max_classes)])\n",
    "        plt.figure(figsize = (10,7))      \n",
    "        s = sn.heatmap(df_cm, annot=True, )\n",
    "        s.set(xlabel='Predicted-Label', ylabel='True-Label')\n",
    "        print('##################################################################################################')\n",
    "\n",
    "        return X_test,label_test, preds\n",
    "\n",
    "\n",
    "    def identify_tweedie_variance_param():\n",
    "        #TODO:#\"fit on data\"\n",
    "        # follow this tutorial https://notebook.community/thequackdaddy/tweedie/example/tweedie_demo\n",
    "        # GLM.estimate_tweedie_power()\n",
    "        # #Training model\n",
    "        # tweedie_model = sm.GLM(y_train, X_train, exposure = df_train.exposure, family=sm.families.Tweedie(link=None,var_power=1.5,eql=True))\n",
    "        # tweedie_result = tweedie_model.fit()\n",
    "        # #Using the initial model output to decide the optimum index parameter \"p\"\n",
    "        # GLM.estimate_tweedie_power(training_result, method='brentq', low=1.01, high=5.0)\n",
    "        # tweedie_model.estimate_tweedie_power(tweedie_result.mu, method='brentq', low=1.01, high=5.0)\n",
    "        return\n",
    "\n",
    "\n",
    "    def make_predictions(self,best_params,num_round, e_s_r):\n",
    "        #TODO:#for new predictions after tuning model\n",
    "        train4 = self.full_set.loc[self.full_set['dt_ts'] < self.val_stop_time_fold_3]\n",
    "        tr4 = train4.drop([\"target\",\"dt_ts\"], axis=1)\n",
    "        label_train4 = pd.DataFrame(train4[\"target\"])\n",
    "        classes_weights = class_weight.compute_sample_weight(class_weight='balanced',y=label_train4)\n",
    "        dtrain4 = xgb.DMatrix(tr4, label=label_train4,weight=classes_weights)\n",
    "\n",
    "        watchlist4  = [(dtrain4,'train3_tweedie_loss')]\n",
    "        evals_result4 = {}\n",
    "        self.model4 = xgb.train(params=best_params, dtrain=dtrain4, num_boost_round=num_round, evals=watchlist4, evals_result=evals_result4, early_stopping_rounds=e_s_r,verbose_eval=False )\n",
    "        return self.model4 # best_model\n",
    "    \n",
    "\n",
    "    def visualize_tree(self,):\n",
    "        #TODO:#\"draw out a few trees to interpret results\"        \n",
    "        #xgb.plot_tree(self.train3)\n",
    "        plot_tree(self.model3)\n",
    "\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    " \n",
    "    #os.chdir(\"c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\BLUESG_Demand_data\\Data-preprocessing_data_generation\")\n",
    "    t_v_t = train_validate_n_test()\n",
    "\n",
    "    tweedie_variance_power = round(trial.suggest_float(name='tweedie_variance_power',low=1.1,high=1.9,step=0.1,log=False),1)\n",
    "\n",
    "    ######  SET Hyperparameter's range for tuning ######\n",
    "    early_stopping_rounds = 30\n",
    "    eval_metric = 'tweedie-nloglik@'+str(tweedie_variance_power)\n",
    "    num_round= 1000\n",
    "    # Hyperparameters and algorithm parameters are described here\n",
    "    params = {\"max_depth\": trial.suggest_int('max_depth', 3, 10),\n",
    "            \"eta\": trial.suggest_float(name='learning_rate', low=0.0001, high=0.1,log=True),\n",
    "            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.1, high=1.0,step=0.1),1),\n",
    "            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.1, high=1.0,step=0.1),1),\n",
    "            'eval_metric':eval_metric, ## try using AUC as well.. \n",
    "            'tweedie_variance_power': tweedie_variance_power,\n",
    "            'gamma': trial.suggest_int('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_int('reg_alpha', 0, 5), \n",
    "            'reg_lambda': trial.suggest_int('reg_lambda', 0, 5),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            \"objective\": 'reg:tweedie',\n",
    "            'gpu_id': 0,\n",
    "            \"tree_method\": 'gpu_hist',\n",
    "            }\n",
    "    ######  SET Hyperparameter's range for tuning ######\n",
    "\n",
    "    val_avg_error = t_v_t.xgb_train_validate(params,num_round, early_stopping_rounds,tweedie_variance_power)\n",
    "    return val_avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=12000, n_trials=500)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    #print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "    fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = optuna.visualization.plot_slice(study)\n",
    "    fig.show()\n",
    "\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.show()\n",
    "\n",
    "    best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "    best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "            \"eta\": study.best_params[\"eta\"],\n",
    "            \"subsample\" : study.best_params[\"subsample\"],\n",
    "            \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "            'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "            'tweedie_variance_power': best_tweedie_variance_power,\n",
    "            'gamma': study.best_params[\"gamma\"],\n",
    "            'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "            'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "            'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "            \"objective\": 'reg:tweedie',\n",
    "            }\n",
    "    early_stopping_rounds = 30\n",
    "    eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "    num_round= 1000\n",
    "\n",
    "    t_v_t = train_validate_n_test()\n",
    "    best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "    t_v_t.evaluate_predictions(best_model)\n",
    "\n",
    "    # t_v_t.display_tweedie_plot() # compare the graphs from target histogram and this function to choose tweedie_variance_power \n",
    "    # X_test,label_test, preds = t_v_t.evaluate_predictions()\n",
    "    # t_v_t.visualize_tree()\n",
    "    # # print('\\n',\"tweedie_variance_power \",tweedie_variance_power,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\" : study.best_params[\"subsample\"],\n",
    "        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "        'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "        'tweedie_variance_power': best_tweedie_variance_power,\n",
    "        'gamma': study.best_params[\"gamma\"],\n",
    "        'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "        'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "        'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "        \"objective\": 'reg:tweedie',\n",
    "        }\n",
    "early_stopping_rounds = 30\n",
    "eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "num_round= 1000\n",
    "\n",
    "t_v_t = train_validate_n_test()\n",
    "best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "t_v_t.evaluate_predictions(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "        \"eta\": 0.01,\n",
    "        \"subsample\" : study.best_params[\"subsample\"],\n",
    "        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "        'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "        'tweedie_variance_power': best_tweedie_variance_power,\n",
    "        'gamma': study.best_params[\"gamma\"],\n",
    "        'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "        'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "        'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "        \"objective\": 'reg:tweedie',\n",
    "        }\n",
    "early_stopping_rounds = 30\n",
    "eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "num_round= 1000\n",
    "\n",
    "t_v_t = train_validate_n_test()\n",
    "best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "t_v_t.evaluate_predictions(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "        \"eta\": 0.001,\n",
    "        \"subsample\" : study.best_params[\"subsample\"],\n",
    "        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "        'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "        'tweedie_variance_power': best_tweedie_variance_power,\n",
    "        'gamma': study.best_params[\"gamma\"],\n",
    "        'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "        'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "        'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "        \"objective\": 'reg:tweedie',\n",
    "        }\n",
    "early_stopping_rounds = 30\n",
    "eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "num_round= 1000\n",
    "\n",
    "t_v_t = train_validate_n_test()\n",
    "best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "t_v_t.evaluate_predictions(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "        \"eta\": 0.0001,\n",
    "        \"subsample\" : study.best_params[\"subsample\"],\n",
    "        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "        'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "        'tweedie_variance_power': best_tweedie_variance_power,\n",
    "        'gamma': study.best_params[\"gamma\"],\n",
    "        'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "        'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "        'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "        \"objective\": 'reg:tweedie',\n",
    "        }\n",
    "early_stopping_rounds = 30\n",
    "eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "num_round= 1000\n",
    "\n",
    "t_v_t = train_validate_n_test()\n",
    "best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "t_v_t.evaluate_predictions(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'xgb_stn_pred_gpu' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n xgb_stn_pred_gpu ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "best_tweedie_variance_power = study.best_params[\"tweedie_variance_power\"]\n",
    "best_params = {\"max_depth\": study.best_params[\"max_depth\"],\n",
    "        \"eta\": 0.00001,\n",
    "        \"subsample\" : study.best_params[\"subsample\"],\n",
    "        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n",
    "        'eval_metric':'tweedie-nloglik@'+str(best_tweedie_variance_power), ## try using AUC as well.. \n",
    "        'tweedie_variance_power': best_tweedie_variance_power,\n",
    "        'gamma': study.best_params[\"gamma\"],\n",
    "        'reg_alpha': study.best_params[\"reg_alpha\"], \n",
    "        'reg_lambda': study.best_params[\"reg_lambda\"],\n",
    "        'min_child_weight': study.best_params[\"min_child_weight\"],\n",
    "        \"objective\": 'reg:tweedie',\n",
    "        }\n",
    "early_stopping_rounds = 30\n",
    "eval_metric = 'tweedie-nloglik@'+str(best_tweedie_variance_power)\n",
    "num_round= 1000\n",
    "\n",
    "t_v_t = train_validate_n_test()\n",
    "best_model = t_v_t.make_predictions(best_params,num_round, early_stopping_rounds)\n",
    "t_v_t.evaluate_predictions(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
