{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "334e549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from haversine import haversine_vector, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf4fbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_clustering(H,demand):\n",
    "    \n",
    "    def str_to_dt(dt_str):\n",
    "        return datetime.datetime.strptime(dt_str, '%H:%M:%S').time()\n",
    "\n",
    "    \n",
    "    # temporal clustering\n",
    "    demand.insert(3,'start_timeslot', demand['start_time'].apply(lambda x:int(np.floor((60*str_to_dt(x).hour + str_to_dt(x).minute)/H))))\n",
    "    demand.insert(6,'end_timeslot', demand['end_time'].apply(lambda x:int(np.floor((60*str_to_dt(x).hour + str_to_dt(x).minute)/H))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71abd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_clustering(d,demand):\n",
    "    stations = pd.read_csv('stations.csv',index_col=0)\n",
    "    \n",
    "    stn_coords = list(zip(stations.y, stations.x))\n",
    "\n",
    "    D = haversine_vector(stn_coords, stn_coords, Unit.KILOMETERS, comb=True)\n",
    "    R = 1*(D < d) # 1 km threshold\n",
    "    \n",
    "    # ilp to select cluster centers\n",
    "    x = cp.Variable((len(stations),),boolean=True)\n",
    "    prob = cp.Problem(cp.Minimize(cp.sum(x)), [x.T@R>=1])\n",
    "    prob.solve()\n",
    "    \n",
    "    # map each station to a cluster\n",
    "    stations['cc'] = x.value\n",
    "    cluster_centers = stations.loc[stations.cc==1]\n",
    "    cc_coords = list(zip(cluster_centers.x, cluster_centers.y))\n",
    "    stations['cc_id'] = np.argmin(haversine_vector(cc_coords, stn_coords, Unit.KILOMETERS, comb=True), axis=1)\n",
    "    \n",
    "    station_to_cluster_map = dict(zip(stations.index, stations.cc_id))\n",
    "    \n",
    "    # spatial clustering\n",
    "    demand['start_cluster'] = demand['start_station'].apply(lambda x: station_to_cluster_map[x])\n",
    "    demand['end_cluster'] = demand['end_station'].apply(lambda x: station_to_cluster_map[x])\n",
    "    \n",
    "    return station_to_cluster_map, stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db67aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'C:\\\\Work\\\\WORK_PACKAGE\\\\Demand_forecasting\\\\github\\\\blue-sg/Demand_forecasting/BLUESG_Demand_data/Data-preprocessing_data_generation'\n",
      "c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\BLUESG_Demand_data\\Data-preprocessing_data_generation\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Work\\WORK_PACKAGE\\Demand_forecasting\\github\\blue-sg/Demand_forecasting/BLUESG_Demand_data/Data-preprocessing_data_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcb9304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows \n",
      " Volume Serial Number is A2F9-40D5\n",
      "\n",
      " Directory of c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\BLUESG_Demand_data\\Data-preprocessing_data_generation\n",
      "\n",
      "08/03/2023  11:05 am    <DIR>          .\n",
      "08/03/2023  11:05 am    <DIR>          ..\n",
      "31/01/2023  01:26 pm            15,968 Clstr_inflow_outflow_data_generator_new.ipynb\n",
      "08/04/2022  02:31 pm        51,077,895 demand.csv\n",
      "08/03/2023  11:04 am         1,781,831 inflow_clstr_dem.csv\n",
      "08/03/2023  11:04 am         1,784,302 outflow_clstr_dem.csv\n",
      "08/03/2023  11:04 am           123,686 station_cc_id.csv\n",
      "08/03/2023  11:28 am            19,478 station_inflow_outflow_data_generator_new.ipynb\n",
      "17/04/2022  08:54 pm           120,634 stations.csv\n",
      "               7 File(s)     54,923,794 bytes\n",
      "               2 Dir(s)  21,308,481,536 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43159661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_station              4    5    6    7    8    9    10   11   12   13   \\\n",
      "start_date start_timeslot                                                     \n",
      "2021-09-23 130             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           135             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           137             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           138             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           139             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...                        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2021-12-23 110             0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           111             0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
      "           112             0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           113             0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           114             0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "start_station              ...  380  381  382  383  384  385  386  387  388  \\\n",
      "start_date start_timeslot  ...                                                \n",
      "2021-09-23 130             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           135             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           137             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           138             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           139             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...                        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2021-12-23 110             ...  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           111             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           112             ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           113             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "           114             ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "start_station              389  \n",
      "start_date start_timeslot       \n",
      "2021-09-23 130             0.0  \n",
      "           135             0.0  \n",
      "           137             0.0  \n",
      "           138             0.0  \n",
      "           139             0.0  \n",
      "...                        ...  \n",
      "2021-12-23 110             0.0  \n",
      "           111             0.0  \n",
      "           112             0.0  \n",
      "           113             0.0  \n",
      "           114             0.0  \n",
      "\n",
      "[13032 rows x 382 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demand = pd.read_csv('demand.csv',index_col=0)\n",
    "\n",
    "## start\n",
    "# DATAFRAME T cluster level demand\n",
    "\n",
    "    H,d = 10,0.01 # H:Timeslot duration in minutes, d = radius for clustering in km\n",
    "    #temporal clustering\n",
    "    temporal_clustering(H,demand)\n",
    "    \n",
    "    # #spatial clustering\n",
    "    # station_to_cluster_map, stations = spatial_clustering(d,demand)\n",
    "    \n",
    "    # print(station_to_cluster_map)\n",
    "    # #print(stations['region'])\n",
    "    # stations.to_csv('station_cc_id_new.csv', index=False)\n",
    "\n",
    "    #aggregated demand table\n",
    "    T = pd.pivot_table(demand, values = 'car', index=[\"start_date\",\"start_timeslot\"], columns=[\"start_station\"], aggfunc=lambda x: len(x.unique()))\n",
    "    IN = pd.pivot_table(demand, values = 'car', index=[\"end_date\",\"end_timeslot\"], columns=[\"end_station\"], aggfunc=lambda x: len(x.unique()))\n",
    "    IN.fillna(0, inplace=True)\n",
    "    T.fillna(0, inplace=True)\n",
    "    \n",
    "    # missing_clusters = [x for x in range(T.columns[-1]) if x not in T.columns]\n",
    "    # missing_clusters.sort()\n",
    "\n",
    "    # for ms in missing_clusters:\n",
    "    #     T.insert(ms, ms, 0)\n",
    "\n",
    "    print(T)\n",
    "\n",
    "    T.to_csv('outflow_station_dem.csv', index=False)\n",
    "\n",
    "    # missing_clusters = [x for x in range(IN.columns[-1]) if x not in IN.columns]\n",
    "    # missing_clusters.sort()\n",
    "    \n",
    "    # for ms in missing_clusters:\n",
    "    #     IN.insert(ms, ms, 0)\n",
    "\n",
    "    # print(IN)\n",
    "    IN.to_csv('inflow_station_dem.csv', index=False)\n",
    "    \n",
    "## stop\n",
    "\n",
    "    #raw_data = T.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68343c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3492f4d3e3c55a6ecd296255a1842ef4032fa21edc5071abaeda59e7bc7b3161"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 ('dem_forcast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
